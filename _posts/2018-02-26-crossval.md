---
layout: post
title: How to cross-validate PCA, clustering, and matrix decomposition models
comments: True
author: alex_williams
completed: True
topic: Dimensionality Reduction
post_description: "Cross-validation is a fundamental paradigm in modern data analysis. However, it is largely applied to supervised settings, such as regression and classification. Here, the procedure is simple: fit your model on, say, 90% of the data (the training set), and evaluate its performance on the remaining 10% (the test set). However, this idea does not easily extend to other unsupervised methods, such as dimensionality reduction methods or clustering. <br><br><b>TL;DR</b> I cover how cross-validation is a somewhat tricky problem for matrix factorization models (including PCA & clustering as special cases) and provide some Python code snippets for fitting these models with held out data."
---

**TL;DR** I cover how cross-validation is a somewhat tricky problem for matrix factorization models (including PCA & clustering as special cases) and provide some Python code snippets for fitting these models with held out data.

<hr>

## Cross-validation in Linear Regression

Cross-validation is a fundamental paradigm in modern data analysis. However, it is largely applied to supervised settings, such as regression and classification. Here, the procedure is simple: fit your model on, say, 90% of the data (the training set), and evaluate its performance on the remaining 10% (the test set). However, this idea does not easily extend to other unsupervised methods, such as dimensionality reduction methods or clustering.

It is easiest to see why visually. Take a simple linear regression problem:

$$
\begin{equation}
\underset{\mathbf{x}}{\text{minimize}} \quad \left \lVert \mathbf{A} \mathbf{x} - \mathbf{b} \right \lVert^2
\end{equation}
$$

Here, $$\mathbf{A}$$ is a $$m \times n$$ matrix, $\mathbf{x}$ is vector with $n$ elements, $$\mathbf{b}$$ is a vector containing $m$ elements. This model has $n$ parameters (the elements of $$\mathbf{b}$$) and $$m$$ datapoints. Rows of $\mathbf{A}$ correspond to independent/predictor variables, and elements of $$\mathbf{b}$$ correspond to dependent variables.

The basic idea behind cross-validation (and related techniques, like bootstrapping) is to leave out datapoints and quantify the effect on the model. We can leave out rows of $\mathbf{A}$ and corresponding elements of $\mathbf{b}$. Critically, this leaves the length of $\mathbf{x}$ unchanged --- there are still $n$ variables to predict, but the number of datapoints $m$ is smaller.

{%include image.html url="/itsneuronalblog/img/pca-crossval/lstsq.png" width="600px" title="Procedure to hold out data for linear regression." description="Note that $\mathbf{x}$ does not change in length." %}

To be explicit: let $\mathbf{A}\_\text{tr}$ and $\mathbf{b}\_\text{tr}$ respectively denote the *training set* of independent and dependent variables. Then fitting our model amounts to:

$$
\hat{\mathbf{x}} = \underset{\mathbf{x}}{\text{argmin}} \quad \left \lVert \mathbf{A}_\text{tr} \mathbf{x} - \mathbf{b}_\text{tr} \right \lVert^2
$$

which is essentially the problem we started with (equation 1), and we know how to solve this problem (least-squares) very efficiently. After fitting the model, we can evaluate its performance on the held-out datapoints (i.e. the *test set*), which we denote $\mathbf{A}\_\text{te}$ and $\mathbf{b}\_\text{te}$. In the end we obtain an estimate of the generalization error of our model as $\lVert \mathbf{A}\_\text{te} \hat{\mathbf{x}} - \mathbf{b}\_\text{te} \lVert^2$.

This procedure is easily adapted to nonlinear regression models, of course.

## Cross-validation in PCA

Ok so what's the problem with cross-validating PCA? I like to think of PCA as the following optimization problem:

$$
\begin{equation}
\underset{\mathbf{U}, \mathbf{V}}{\text{minimize}} \quad \left \lVert \mathbf{U} \mathbf{V}^T - \mathbf{Y} \right \lVert^2
\end{equation}
$$

This is much like linear regression, except we are optimizing over both $\mathbf{A}$ and $\mathbf{X}$ (which have been renamed to $\mathbf{U}$ and $\mathbf{V}^T$ to avoid confusion between the two models). Here, $\mathbf{Y}$ is a $m \times n$ matrix of data. In large-scale studies both $m$ and $n$ can be very high-dimensional and we may seek a simple low-dimensional linear model. This model is captured by $\mathbf{U}$ which is a tall-skinny matrix and $\mathbf{V}^T$ which is a short-fat matrix. Concretely, lets define $\mathbf{U}$ to me an $m \times r$ matrix and define $\mathbf{V}^T$ to be a $r \times n$ matrix, and choose $r$ to be less than $m$ and $n$. Then our model estimate $\hat{\mathbf{Y}} = \mathbf{U} \mathbf{V}^T$ is a rank-$r$ matrix

Some of you are probably grumpy that the above problem is not "really PCA" because PCA places an additional orthogonality constraint on the columns of $\mathbf{U}$ and $\mathbf{V}$. For a more careful discussion about the connection between PCA and equation 2 see [my other post on PCA](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/), [this talk/tutorial I gave](https://cbmm.mit.edu/video/dimensionality-reduction-matrix-and-tensor-coded-data-part-1), and Madeleine Udell's [thesis work](https://people.orie.cornell.edu/mru8/doc/udell16_glrm.pdf). Those resources also explain how nonnegative matrix factorization, clustering, and many other unsupervised models are also closely related to equation 2. Thus, our discussion of cross-validation will quickly generalize to these other very interesting models.

Once you are on board with the matrix factorization framework, you'll see why cross-validation is tricky. Ultimately, our problem now looks like this:

{%include image.html url="/itsneuronalblog/img/pca-crossval/matrixfac.png" width="600px" title="Matrix Factorization Model." description="In keeping with figure 1, I've colored the model parameters (here, $\mathbf{U}$ and $\mathbf{V}$) in orange, while the data (here, $\mathbf{Y}$) is in blue. We optimize over $\mathbf{U}$ and $\mathbf{V}$ to minimize reconstruction error with respect to $\mathbf{Y}$" %}

Ok, so how exactly should we hold out data in this setting? You might think we could leave out rows of $\mathbf{Y}$, however this would mean that you would have to leave out the corresponding row of $\mathbf{U}$. Thus, you couldn't fit all of your model parameters! Likewise, leaving out a column of $\mathbf{A}$ would mean that you'd have to leave out a column of $\mathbf{V}^T$.

{%include image.html url="/itsneuronalblog/img/pca-crossval/holdout_naive.png" width="700px" title="Not so great ideas for cross-validating matrix factorization." description="" %}

Maybe you only care about evaluating held-out/test error on our estimate of $\mathbf{V}$. That is, you don't care about cross-validating $\mathbf{U}$ --- you *only* care about cross-validating $\mathbf{V}$. Thus, you might suggest a two-step procedure in which we leave out rows of $\mathbf{Y}$ and fit $\mathbf{V}$ along with a subset of the rows of $\mathbf{U}$ on this training set. Then, to evaluate the performance of $\mathbf{V}$, we bend the rules of cross-validation ever so slightly and use the test set (held out rows) to fill out our estimate of $\mathbf{U}$. 

Even this is not a good idea! It violates a core tenet of cross-validation that you don't get to touch the the held out data. Your estimate of $\mathbf{V}$ could be horribly overfit, but by fitting $\mathbf{U}$ to on the test data, you could set these rows to zero and essentially blunt the effects of overfitting. Note that we *have to* fit this held out row of $\mathbf{U}$ in order to evaluate model performance along the corresponding row in the data matrix $\mathbf{Y}$ (which is the whole point of cross-validation).

In a moment, I'll describe a very simple cross-validation procedure that draws a connection between matrix factorization models and the well-studied [matrix completion](https://en.wikipedia.org/wiki/Matrix_completion) problem. But I should mention at the outset that there is much more detailed published work on this topic, which I touch on briefly in the Appendix. Some of these articles discuss ways of leaving out entire rows and columns of the data matrix, but those procedures require a bit more care than what we will focus on.

## Why cross-validate PCA (and related methods)?

Before jumping into a solution, let's remind ourselves why cross-validation is great and why it would be great to apply it to PCA. In the case of regression and other supervised learning techniques, the goal of cross-validation is to monitor overfitting and calibrate hyperparameters. If we have a large number of regression parameters (i.e., $\mathbf{x}$ in equation 1 is a very long vector) then we'd commonly add regularization to the model. To take a very simple example, consider [LASSO regression](https://en.wikipedia.org/wiki/Lasso_(statistics)):

$$
\underset{\mathbf{x}}{\text{minimize}} \quad \left \lVert \mathbf{A} \mathbf{x} - \mathbf{b} \right \lVert^2 + \lambda \lVert \mathbf{x} \lVert_1
$$

which will tend to produce an parameter estimate, $\hat{\mathbf{x}}$, that is sparse (containing many zeros). The scalar hyperparameter $\lambda > 0$ tunes how sparse our parameter estimate will be. If $\lambda$ is too large, then $\hat{\mathbf{x}}$ will very sparse and do a poor job of predicting $\mathbf{y}$. If $\lambda$ is too small, then $\hat{\mathbf{x}}$ will be not sparse at all, and our model could be overfit.

We can use cross-validation to estimate a good value for $\lambda$, by finding the value which minimizes the error of our model on the held-out test data. This procedure for model selection is machine learning 101 (see Chap. 7 in [*ESL*](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)). The basic picture looks like this:

{%include image.html url="/itsneuronalblog/img/pca-crossval/crossval-lasso.png" width="500px" title="Cross-validation schematic for LASSO" description="dashed vertical line denotes the best value for $\lambda$, which achieves lowest error on the held-out test set." %}

PCA also has an important hyperparameter that people worry about --- the number of components in the model. People have published a lot of papers on this (e.g. [here](https://arxiv.org/abs/1305.5870), [here](http://papers.nips.cc/paper/1853-automatic-choice-of-dimensionality-for-pca.pdf), & [here](https://arxiv.org/abs/math/0609042)). Similarly, many clustering models require the user to choose the number of clusters prior to fitting the model. Choosing the number of clusters has also received [a lot of attention](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set). Viewed from the perspective of matrix factorization, these are the *exact same problem* -- i.e. how to choose $r$, the width of $\mathbf{U}$ and the height of $\mathbf{V}^T$.

## A cross-validation procedure for matrix decomposition

Without further ado, here are some plots that demonstrate how cross-validation can help you choose the number of components in PCA, NMF, and K-means clustering. In each example, I generated data from a ground truth model with $r=4$ and then added noise. That is, for PCA, the correct number of PCs was 4. For k-means clustering, the correct number of clusters was 4. From the test error, you can see that all models begin to overfit when $r>4$. From the training data alone, the cutoff is maybe not so clear.

{%include image.html url="/itsneuronalblog/img/pca-crossval/cv_curves.png" width="900px" title="Cross-validation plots for some matrix decomposition models." description="Note that the k-means implementation is a bit noisy so I averaged across multiple optimization runs. The code to generate these plots is <a href='https://gist.github.com/ahwillia/65d8f87fcd4bded3676d67b55c1a3954' target='_blank'>posted here</a>." %}

To make these plots I used a "speckled" holdout pattern ([Wold, 1978](http://dx.doi.org/10.2307/1267639)). For simplicity and demonstration, I left out a small number of elements of $\mathbf{Y}$ at random:

{%include image.html url="/itsneuronalblog/img/pca-crossval/holdout.png" width="700px" title="A good solution is to hold out data at random." description="Importantly, we can still fit all parameters in $\mathbf{U}$ and $\mathbf{V}$ as long as no column or row of $\mathbf{Y}$ is fully removed. Intuitively, we should keep at least $r$ observations in each row or column." %}

Formally, we define a binary matrix $\mathbf{M}$, which acts as a mask over our data. That is every element in $\mathbf{M}$ is either $m_{ij} = 0$, indicating a left out datapoint, or $m_{ij}=1$, indicating a datapoint in the training set. We are left with the following optimization problem:

$$
\begin{equation}
\underset{\mathbf{U}, \mathbf{V}}{\text{minimize}} \quad \left \lVert \mathbf{M} \circ \left ( \mathbf{U} \mathbf{V}^T - \mathbf{Y} \right ) \right \lVert_F^2 
\end{equation}
$$

Where $ \circ $ denotes the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)). After solving the above optimization problem, we can then evaluate the error of our model on the held-out datapoints as: $\left \lVert (1-\mathbf{M}) \circ \left ( \mathbf{U} \mathbf{V}^T - \mathbf{Y} \right ) \right \lVert_F^2$.

Note that we need not choose $\mathbf{M}$ to be random, we can select whatever holdout pattern we like. Indeed to ensure that all columns and rows are left out at equal rates we can leave out pseudo-diagonals of the matrix. In the interest of brevity and simplicity we will sweep these choices under the rug, but see [Fig.1 in Wold (1978)](http://dx.doi.org/10.2307/1267639) for more discussion.

So how do we solve this optimization problem? As I mentioned before, equation 3 amounts to the well-studied low-rank [matrix completion](https://en.wikipedia.org/wiki/Matrix_completion) problem. However many papers that I've looked at do not provide algorithms for solving the problem in the particular form that we care about. In particular, they tend to optimize over a single matrix, call it $\hat{\mathbf{Y}}$, rather than jointly optimize over a factorized representation where $\hat{\mathbf{Y}} = \mathbf{U} \mathbf{V}^T$ (which is what we'd like to do). For example, [Candes and Recht (2008)](https://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf) consider:

$$
\begin{aligned}
& \underset{\hat{\mathbf{Y}}}{\text{minimize}} & & \lVert \hat{\mathbf{Y}} \lVert_* \\
& \text{subject to} & & \mathbf{M} \circ \hat{\mathbf{Y}} = \mathbf{M} \circ \mathbf{Y}
\end{aligned}
$$

where $\lVert \cdot \lVert_*$ denotes the [nuclear norm](https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms) of a matrix. Minimizing the nuclear norm is a good surrogate for minimizing the rank of $\hat{\mathbf{Y}}$ directly, which is more computationally challenging. The advantage of this approach is that the optimization problem is convex and therefore comes with really nice guaruntees and mathematical analysis.

The problem with this is that it solves the matrix completion problem without giving us $\mathbf{U}$ and $\mathbf{V}$, which are of direct interest to us in the context of PCA and clustering. Furthermore, I don't think this approach scales particularly well to very large matrices or tensor datasets since you are optimizing over a very large number of variables $mn$, as opposed to $mr + nr$ variables in the factorized representation.

A very simple and effective procedure for fitting matrix decomposition is the alternating minimization algorithm, which I've [blogged](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/) and [talked](https://cbmm.mit.edu/video/dimensionality-reduction-matrix-and-tensor-coded-data-part-1) about in the past. For the case of PCA, this amounts to:

> **Algorithm:** *Alternating minimization:*<br>
> 1 &nbsp;&nbsp;&nbsp;&nbsp; initialize $\mathbf{V}$ randomly<br>
> 2 &nbsp;&nbsp;&nbsp;&nbsp; **while** not converged<br>
> 3 &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $\mathbf{U}  \leftarrow \underset{\tilde{\mathbf{U}}}{\text{argmin}} \quad \left \lVert \mathbf{M} \circ \left ( \tilde{\mathbf{U}} \mathbf{V}^T - \mathbf{Y} \right ) \right \lVert_F^2$<br>
> 4 &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $\mathbf{V}  \leftarrow \underset{\tilde{\mathbf{V}}}{\text{argmin}} \quad \left \lVert \mathbf{M} \circ \left ( \mathbf{U} \tilde{\mathbf{V}}^T - \mathbf{Y} \right ) \right \lVert_F^2$<br>
> 5 &nbsp;&nbsp;&nbsp;&nbsp; **end while**

While other papers (e.g. [Hardt 2013](https://arxiv.org/abs/1312.0925)) use alternating minimization to solve matrix completion problems, I haven't come across many good sources that explain how to implement this idea in practice (please email me if you find good ones!). So I'll give some guidance below.

#### Implementation Notes: PCA

Each parameter update (lines 3 and 4) of the alternating minimization algorithm boils down to a least-squares problem *with missing data*. In this interest of brevity, I derived the solution to least squares under missing data [in a separate post](/itsneuronalblog/2017/02/20/censored-lstsq/). The answer is kinda cool and involves tensors! The basic solution we arrive at is this:

```python
def censored_lstsq(A, B, M):
    """Least squares of M * (AX - B) over X, where M is a masking matrix.
    """
    rhs = np.dot(A.T, M * B).T[:,:,None] # n x r x 1 tensor
    T = np.matmul(A.T[None,:,:], M.T[:,:,None] * A[None,:,:])
    return np.linalg.solve(T, rhs).reshape(A.shape[1], B.shape[1])
```

Wow --- only three lines thanks to the magic of numpy broadcasting! And we can use this sub-routine to cross-validate PCA as follows:

```python
def cv_pca(data, rank, p_holdout=.1):
    """Fit PCA while holding out a fraction of the dataset.
    """
    # create masking matrix
    M = np.random.rand(data.shape) < p_holdout

    # fit pca
    U = np.random.randn(data.shape[0], rank)
    for itr in range(20):
        Vt = censored_lstsq(U, data, M)
        U = censored_lstsq(Vt.T, data.T, M.T).T

    # We could orthogonalize U and Vt and then rotate to align
    # with directions of maximal variance, but we won't bother.

    # return result and test/train error
    resid = np.dot(U, Vt) - data
    train_err = np.mean(resid[M]**2)
    test_err = np.mean(resid[~M]**2)
    return train_err, test_err
```

A few disclaimers about the above code, which is only meant to give the gist of a solution:

* In the spirit of brevity, I don't check for convergence. Obviously, in real code, you should either monitor the reconstruction error or the change in `U` and `Vt` and break the loop when these converge.
* The `censored_lstsq` function is also not optimized. See discussion in my other post.
* If there are too many missing values (e.g. an entire row or column is left out) then the `censored_lstsq` function will fail due to a singular/non-invertible matrix.

#### Implementation Notes: NMF and K-means

NMF is very similar to PCA when viewed from the perspective of matrix factorization. Each subproblem becomes a *nonnegative least-squares problem* with missing data in the dependent variable. Nonnegative least-squares is a very well-studied problem, and the methods discussed for classic least squares can be generalized without that much effort. Jingu Kim has a really nice Python library called [nonnegfac-python](https://github.com/kimjingu/nonnegfac-python) that handles these kind of things.

Another possibility is to modify the [classic multiplicative update rules for NMF](http://www.almoststochastic.com/2013/06/nonnegative-matrix-factorization.html). These rules are originally:

$$
U_{ij} \leftarrow U_{ij} \frac{(\mathbf{Y} \mathbf{V})_{ij}}{(\mathbf{U} \mathbf{V}^T \mathbf{V})_{ij}}
$$

$$
V_{ij} \leftarrow V_{ij} \frac{(\mathbf{U}^T \mathbf{Y})_{ij}}{(\mathbf{U}^T \mathbf{U} \mathbf{V}^T)_{ij}}
$$

Under missing data, these rules become:

$$
U_{ij} \leftarrow U_{ij} \frac{((\mathbf{M} \circ \mathbf{Y}) \mathbf{V})_{ij}}{((\mathbf{M} \circ \mathbf{U} \mathbf{V}^T) \mathbf{V})_{ij}}
$$

$$
V_{ij} \leftarrow V_{ij} \frac{(\mathbf{U}^T (\mathbf{M} \circ \mathbf{Y}))_{ij}}{(\mathbf{U}^T (\mathbf{M} \circ \mathbf{U} \mathbf{V}^T))_{ij}}
$$

K-means clustering was a bit more finicky (as you can see I averaged over many random initializations in the figure above). But the basic cross-validation principles are the same. See [Chi et al. (2016)](https://arxiv.org/abs/1411.7013) for fitting K-means clustering with missing data. There is also a really short and simple implementation of this idea in a [stack-overflow answer](https://stackoverflow.com/questions/35611465/python-scikit-learn-clustering-with-missing-data).

### Conclusions and References

Everything here is a brief overview of a topic that has been studied more deeply in the literature. The take-home message is that cross-validation is a bit tricky for unsupervised learning, but there is a simple idea that generalizes to many methods. I just showed you three (PCA, NMF, and $k$-means clustering), but the basic idea likely applies to other models.

How can the ideas I covered here be improved upon? From a computational standpoint, leaving out data at random made our lives difficult. A nicer choice may have been to hold out some of the data from a subset of rows and columns. This means that our holdout pattern partitions the data matrix into four blocks, and without loss of generality we can rearrange the rows and columns so that the upper left block is held out as shown below:

{%include image.html url="/itsneuronalblog/img/pca-crossval/bcv_holdout.png" width="900px" title="Bi-cross-validation holdout pattern." description="." %}

This holdout pattern was considered by [Owen & Perry (2009)](https://arxiv.org/abs/0908.2062) who attribute the basic idea to Gabriel (although he only proposed holding out a single entry of the matrix, rather than a block). A neat thing about this approach is that one can fit a PCA or NMF model to the (fully observed) bottom right block, $\mathbf{Y}\_{(2,2)}$, and then use that model to predict the (held-out test) block $\mathbf{Y}\_{(1,1)}$ based on the off-diagonal blocks. Understanding how this works takes a bit of effort, but the take-home message is that their approach offers significant speed ups to what I outlined in this post.

[Fu & Perry (2017)](https://arxiv.org/abs/1702.02658) recently extended the above idea to K-means clustering. And I'm still digging through it but [Perry's PhD thesis](https://arxiv.org/abs/0909.3052) seems like a good reference as well.

Predating the above work, there is a nice review of these topics by [Bro et al. (2008)](https://doi.org/10.1007/s00216-007-1790-1). They show that many previous algorithms for cross-validating PCA don't work that well in practice and aren't very well-motivated. So be careful if you are reading older papers on this subject!
