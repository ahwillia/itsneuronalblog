<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      On the Uniqueness of PCA and Tensor Decompositions &middot; Its 
 Neuronal
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/misc.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">


  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link href="/itsneuronalblog/feed.xml" rel='alternate' type='application/atom+xml'>

</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>

    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/otherblogs/">Other Blogs</a>
          
        
      

      <a href='http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml'  target='blank'><img id='feedlyFollow' src='http://s3.feedly.com/img/follows/feedly-follow-rectangle-volume-medium_2x.png' style="float:left;margin-top:12px" alt='follow us in feedly' width='71' height='28' ></a>

      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
        <img alt="Creative Commons License" style="float:right;margin-top:12px" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
      </a>

    </nav>

    
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">On the Uniqueness of PCA and Tensor Decompositions</h1>
  <span class="post-date">21 Apr 2015</span>
  <!-- Look the author details up from the site config. -->
  

  <!-- Output author details if some exist. -->
  
      <div style="height:50px">
          <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

          <small style="vertical-align:middle;">
          Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
          </small>
      </div>
      <br>
  

  <h4 class="no_toc" id="there-is-no-mathematical-reason-why-principal-components-must-be-orthogonal">There is no mathematical reason why principal components must be orthogonal</h4>

<p>There are two potential justifications for imposing orthogonal components (i.e. $C^T C = I$).</p>

<ul>
  <li>We tend to plot and think about data in orthogonal coordinate systems. Specifically, imposing orthogonality means the principal components are uncorrelated, so we can talk about the contribution of individual components without referring to the others.</li>
  <li>Imposing this means that there is a unique solution to the optimization problem so that there is a standard protocol for the field.</li>
</ul>

<p>If the goal was to minimize $\lVert A - W C^T \lVert_F^2$ without constraints, then there are an infinite number of solutions! To see this let $R$ be any invertible $r \times r$ matrix. Then for any candidate solution $\{W, C\}$ there is an equally viable solution $\{W^\prime, C^\prime\}$ with $W^\prime = W R^{-1}$ and $C^\prime = C R^T$:</p>

<script type="math/tex; mode=display">\lVert A - W C^T \lVert_F^2 = \lVert A - W R^{-1} R C^T \lVert_F^2 = \lVert A - W^\prime C^{\prime T} \lVert_F^2</script>

<p>Thus, there is a very general set of linear transformations we can apply to components and loadings without affecting the reconstruction error. This fundamental non-uniqueness is important to understand because it limits our ability to interpret and assign meaning to the components. In other cases, we can understand outcome of the model in this way. For example, suppose we are trying to develop a model of some black-box system that has multiple inputs and multiple outputs. We observe a sequence of inputs $\mathbf{x}$ and their corresponding outputs $\mathbf{y}$:</p>

<p>PICTURE</p>

<p>If the black-box implements a nearly linear transformation, then we can determine this transformation by doing linear regression. The resulting model is a matrix, $B$, which can be used to predict the outcome for any input: $\mathbf{y} = B \mathbf{x}$. Each element in the matrix has a nice interpretation: $B_{ij}$ tells us the effect of input $j$ on output $i$. If $B_{ij}$ is nearly zero then changing input $x_j$ barely effects output $y_i$. If $B_{ij}$ is positive (resp. negative) then increasing $x_j$ increases (resp. decreases) $y_i$. We can even attach units to each $B_{ij}$ as the units of $y_i$ divided by the units of $x_j$.</p>

<p>All of this is very appealing, but falls apart when we move to PCA. In this case, we observe some output of a black-box system, but we don’t know the underlying inputs that produced this dataset. For example, we might measure the expression of many genes across many different samples. We may suspect that the input of the system is low-dimensional and more simple that then output we observed (e.g. we might think activity of a handful of transcription factors determines the expression of many genes in our experiment). However, in this hypothetical case, we weren’t able to identify or measure these inputs. Nevertheless, there is some real set of inputs $\mathbf{w}^*_i$ and a real linear transformation $C^*$ for each observation $\mathbf{a}_i$. Relabeling the input-output diagram we get:</p>

<p>PICTURE</p>

<p>In this case we can consolidate our observations into a data matrix $A$, and do PCA to get a loadings $W$ and components $C$. But it is somewhat intuitive that we can’t recover both the inputs and the actual transformation of the system. This is sort of like asking someone to tell you which two numbers you multiplied together to get the number 16 (4 and 4? or 2 and 8?).</p>

<p>However, PCA <strong>does</strong> capture the correct linear subspace.</p>

<p>Perhaps the best known example is <a href="https://en.wikipedia.org/wiki/Independent_component_analysis">independent components analysis (ICA)</a></p>

</div>

<!-- <div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/04/30/tensor-decomp/">
            Tensor Decomp
            <small>30 Apr 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/04/20/pca-appendix/">
            Probabilistic PCA and Factor Analysis
            <small>20 Apr 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/03/27/pca/">
            Everything you did and didn't know about PCA
            <small>27 Mar 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div> -->


      
      
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'quantneuro';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>


    </div>
   
  </body>
</html>
