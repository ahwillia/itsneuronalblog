<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Is clustering mathematically impossible? &middot; Its 
 Neuronal
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/misc.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">


  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link href="/itsneuronalblog/feed.xml" rel='alternate' type='application/atom+xml'>

</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>

    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/otherblogs/">Other Blogs</a>
          
        
      

      <a href='http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml'  target='blank'><img id='feedlyFollow' src='http://s3.feedly.com/img/follows/feedly-follow-rectangle-volume-medium_2x.png' style="float:left;margin-top:12px" alt='follow us in feedly' width='71' height='28' ></a>

      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
        <img alt="Creative Commons License" style="float:right;margin-top:12px" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
      </a>

    </nav>

    
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Is clustering mathematically impossible?</h1>
  <span class="post-date">01 Oct 2015</span>
  <!-- Look the author details up from the site config. -->
  

  <!-- Output author details if some exist. -->
  
      <div style="height:50px">
          <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

          <small style="vertical-align:middle;">
          Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
          </small>
      </div>
      <br>
  

  <p>In the <a href="http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/">previous post</a>, we saw intuitive reasons why clustering is a hard,<a href="#f1b" id="f1t"><sup>[1]</sup></a> and maybe even <em>ill-defined</em>, problem. In practice, we are often stuck using heuristics that can sometimes perform quite badly when their assumptions are violated (see <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem"><em>No free lunch theorem</em></a>). Is there a mathematical way of expressing all of these difficulties? This post will cover some theoretical results of <a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg (2002)</a> related to this question.</p>

<p><strong><em>Notation.</em></strong> Suppose we have a set of $N$ datapoints $x^{(1)}, x^{(2)}, …, x^{(N)}$. A <em>clustering function</em> produces a <a href="https://en.wikipedia.org/wiki/Partition_of_a_set"><em>partition</em></a> (i.e. a set of clusters), based on the pairwise distances between datapoints. The distance between two points $x^{(i)}$ and $x^{(j)}$ is given by $d(x^{(i)},x^{(j)})$, where $d$ is the <em>distance function</em>. We could choose different ways to measure distance,<a href="#f2b" id="f2t"><sup>[2]</sup></a> for simplicity you can imagine we are using <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>, $\sqrt{ (x^{(i)}-x^{(j)}) \cdot (x^{(i)}-x^{(j)})}$.</p>

<h3 id="an-axiomatic-approach-to-clustering">An axiomatic approach to clustering</h3>

<p>There are many possible clustering functions we could come up with. Some are stupid — randomly split the data into two groups — and others are useful in practice. We would like to precisely define what it means for a clustering function to be “useful in practice.”</p>

<p><a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg (2002)</a> proposed that the ideal clustering function would achieve three properties: <a href="/itsneuronalblog/2015/10/01/clustering2/#scale-invariance"><em>scale-invariance</em></a>, <a href="/itsneuronalblog/2015/10/01/clustering2/#consistency"><em>consistency</em></a>, <a href="/itsneuronalblog/2015/10/01/clustering2/#richness"><em>richness</em></a>. The idea is that these principles should align with your intuitive notion of what a “good clustering function” is:</p>

<!--more-->

<p><strong>1. Scale-invariance:</strong> An ideal clustering function does not change its result when the data are scaled equally in all directions.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Scale-invariance.. </b>For any scalar $\alpha > 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-scale-invariance.png" alt="For any scalar $\alpha &gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$." width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Scale-invariance.</b>
	For any scalar $\alpha &gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.
</p></td></tr>
</table>

<p><strong>2. Consistency:</strong> If we stretch the data so that the distances between clusters increases and/or the distances within clusters decreases, then the clustering shouldn’t change.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Consistency.. </b>Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the <i>same</i> cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to <i>different</i> clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn't change: $f(d) = f(d^\prime)$</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-consistency.png" alt="Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the &lt;i&gt;same&lt;/i&gt; cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to &lt;i&gt;different&lt;/i&gt; clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn't change: $f(d) = f(d^\prime)$" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Consistency.</b>
	Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the <i>same</i> cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to <i>different</i> clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn't change: $f(d) = f(d^\prime)$
</p></td></tr>
</table>

<p><strong>3. Richness:</strong> Suppose a dataset contains $N$ points, but we are not told anything about the distances between points. An ideal clustering function would be flexible enough to produce all possible partition/clusterings of this set. This means that the it automatically determines both the number and proportions of clusters in the dataset. This is shown schemetically below for a set of six datapoints:</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Richness.. </b>For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-richness.png" alt="For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$." width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Richness.</b>
	For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.
</p></td></tr>
</table>

<h3 id="kleinbergs-impossibility-theorem">Kleinberg’s <em>Impossibility Theorem</em></h3>

<p><a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg’s paper</a> is a bait-and-switch though. <strong><em>It turns out that no clustering function can satisfy all three axioms!</em></strong> <a href="#f3b" id="f3t"><sup>[3]</sup></a> The proof in Kleinberg’s paper is a little terse — A simpler proof is given in <a href="http://www.cs.fsu.edu/~ackerman/thesisPhD.pdf">Margareta Ackerman’s thesis</a>, specifically Theorem 21. The intuition provided there is diagrammed below.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Intuition behind impossibility.. </b>A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/impossibility-intuition.png" alt="A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation." width="350px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Intuition behind impossibility.</b>
	A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.
</p></td></tr>
</table>

<h3 id="clustering-functions-that-satisfy-two-of-the-three-axioms">Clustering functions that satisfy two of the three axioms</h3>

<p>The above explanation may still be a bit difficult to digest. Another perspective for understanding the impossibility theory is to examine clustering functions that come close to satisfying the three axioms.</p>

<p>Kleinberg mentions three variants of <a href="https://en.wikipedia.org/wiki/Single-linkage_clustering">single-linkage clustering</a> as an illustration. Single-linkage clustering starts by assigning each point to its own cluster, and then repeatedly fusing together the nearest clusters (where <em>nearest</em> is measured by our specified distance function). To complete the clustering function we need a <em>stopping condition</em> — something that tells us when to terminate and return the current set of clusters as our solution. Kleinberg outlines three different stopping conditions, each of which violates one of his three axioms, while satisfying the other two.</p>

<p><strong>1. $k$-cluster stopping condition:</strong> Stop fusing clusters once we have $k$ clusters (where $k$ is some number provided beforehand, similar to the <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means algorithm</a>).</p>

<p>This clearly violates the <em>richness</em> axiom. For example, if we choose $k=3$, then we could never return a result with 2 clusters, 4 clusters, etc. However, it satisfies <em>scale-invariance</em> and <em>consistency</em>. To check this, notice that the transformations in the above diagrams above do not change which $k$ clusters are nearest to each other. It is only once we start merging and dividing clusters that we get into trouble.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>$k$-cluster stopping does not satisfy richness. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/k-stopping-violation.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>$k$-cluster stopping does not satisfy richness</b>
	
</p></td></tr>
</table>

<p><strong>2. Distance-$r$ stopping condition:</strong> Stop when the nearest two clusters are farther than a pre-defined distance $r$.</p>

<p>This satisfies <em>richness</em> — we can place $N$ points to end up in $N$ clusters by having the minimum distance between any two points to be greater than $r$, we can place $N$ points to end up in one cluster by having the maximum distance be less than $r$, and we can generate all partitions between these extremes.</p>

<p>It also satisfies <em>consistency</em>. Shrinking the distances between points in a cluster keeps the maximum distance less than $r$ (our criterion for defining a cluster in the first place). Expanding the distances between points in different clusters keeps the minimum distance greater than $r$. Thus, the clusters remain the same.</p>

<p>However, <em>scale-invariance</em> is violated. If we multiply the data by a large enough number, then the $N$ points will be assigned $N$ different clusters (all points are more than distance $r$ from each other). If we multiply the data by a number close to zero, everything ends up in the same cluster.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>distance-$r$ stopping does not satisfy scale-invariance. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/distance-r-violation.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>distance-$r$ stopping does not satisfy scale-invariance</b>
	
</p></td></tr>
</table>

<p><strong>3. Scale-$\epsilon$ stopping condition:</strong> Stop when the nearest two clusters are farther than a fraction of the maximum distance between two points. This is like the distance-$r$ stopping condition, except we choose $r = \epsilon \cdot \Delta$, where $\Delta$ is the maximum distance between any two data points and $\epsilon$ is a number between 0 and 1.</p>

<p>By adapting $r$ to the scale of the data, this procedure now satisfies <em>scale-invariance</em> in addition to <em>richness</em>. However, it <strong>does not</strong> satisfy <em>consistency</em>. To see this, consider the following transformation of data, in which one cluster (the green one) is pulled much further away from the other two clusters. This increases the maximum distance between data points, leading us to merge the blue and red clusters into one:</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>scale-$\epsilon$ stopping does not satisfy consistency. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-consistency-violation.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>scale-$\epsilon$ stopping does not satisfy consistency</b>
	
</p></td></tr>
</table>

<h3 id="sidestepping-impossibility-and-subsequent-work">Sidestepping impossibility and subsequent work</h3>

<p>Kleinberg’s analysis outlines what we <strong>should not expect</strong> clustering algorithms to do for us. It is good not to have unrealistic expectations. But can we circumvent his impossibility theorem, and are his axioms even really desirable?</p>

<p>The consistency axiom is particularly suspect as illustrated below:</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Is consistency a desirable axiom?. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-consistency-problem.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Is consistency a desirable axiom?</b>
	
</p></td></tr>
</table>

<p>The problem is that our intuitive sense of clustering would probably lead us to merge the two clusters in the lower left corner. This criticism is taken up in <a href="http://www.cs.fsu.edu/~ackerman/thesisPhD.pdf">Margareta Ackerman’s thesis</a>, which I hope to summarize in a future blog post.</p>

<p>Many clustering algorithms also ignore the <em>richness</em> axiom by specifying the number of clusters beforehand. For example, we can run $k$-means multiple times with different choices of $k$, allowing us to re-interpret the same dataset at different levels of granularity. <a href="http://stanford.edu/~rezab/papers/slunique.pdf">Zadeh &amp; Ben-David (2009)</a> study a relaxation of the richness axiom, which they call $k$-richness — a desirable clustering function should produce all possible $k$-partitions of a datset (rather than <strong>all</strong> partitions).</p>

<p>Overall, Kleinberg’s axiomatic approach provides an interesting perspective on clustering, but his analysis serves more as a starting point, rather than a definitive theoretical characterization of clustering.</p>

<hr />

<div class="share-page">

    <strong>
    Share:
    </strong>
    
    <a href="https://twitter.com/intent/tweet?text=Is clustering mathematically impossible?&amp;url=http://localhost:4000/itsneuronalblog/2015/10/01/clustering2/&amp;via=ItsNeuronal&amp;related=ItsNeuronal" rel="nofollow" target="_blank" title="Share on Twitter" class="btn-social btn-twitter">
        <i class="fa fa-twitter"></i>
    </a>

    <a href="http://www.reddit.com/submit?url=http://localhost:4000/itsneuronalblog/2015/10/01/clustering2/&amp;title=Is clustering mathematically impossible?" rel="nofollow" target="_blank" title="Share on Reddit" class="btn-social btn-reddit">
        <i class="fa fa-reddit"></i>
    </a>

    <a href="https://facebook.com/sharer.php?u=http://localhost:4000/itsneuronalblog/2015/10/01/clustering2/&amp;t=Is clustering mathematically impossible?" rel="nofollow" target="_blank" title="Share on Facebook" class="btn-social btn-facebook">
        <i class="fa fa-facebook-official"></i>
    </a>

    <a href="https://plus.google.com/share?url=http://localhost:4000/itsneuronalblog/2015/10/01/clustering2/" rel="nofollow" target="_blank" title="Share on Google+" class="btn-social btn-google">
        <i class="fa fa-google-plus"></i>
    </a>

    <strong>
    &nbsp;&nbsp;&nbsp;&nbsp;Follow:
    </strong>

    <a href="http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml" rel="nofollow" target="_blank" title="Follow on Feedly" class="btn-social btn-rss">
        <i class="fa fa-rss"></i>
    </a>

</div>

<h4 id="footnotes">Footnotes</h4>

<p class="footnotes">
<a href="#f1t" id="f1b"><b>[1]</b></a> I am using loose language when I say clustering is a “hard problem.” Similar to the <a href="http://localhost:4000/itsneuronalblog/2015/09/11/clustering1/">previous post</a>, we will be concerned with why clustering is hard on a conceptual/theoretical level. But it is also worth pointing out that clustering is hard on a computational level — it takes a long time to compute a provably optimal solution. For example, <em>k</em>-means is provably NP-hard for even k=2 clusters <a href="https://dx.doi.org/10.1007%2Fs10994-009-5103-0">(Aloise et al., 2009)</a>. This is because cluster assignment is a discrete variable (a point <em>either</em> belongs to a cluster or does not); in many cases, discrete optimization problems are more difficult to solve than continuous problems because we can compute the derivatives of the objective function and thus take advantage of gradient-based methods. (However this <a href="http://cstheory.stackexchange.com/questions/31054/is-it-a-rule-that-discrete-problems-are-np-hard-and-continuous-problems-are-not">doesn’t entirely account for</a> the hardness.)
</p>
<p class="footnotes">
<a href="#f2t" id="f2b"><b>[2]</b></a> Kleinberg (2002) only requires that the distance be nonnegative and symmetric, $d(x_i,x_j) = d(x_j,x_i)$, and not necessarily satisfy the <a href="https://en.wikipedia.org/wiki/Triangle_inequality">triangle inequality</a>. According to Wikipedia these are called <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)#Semimetrics"><em>semimetrics</em></a>. There are many other exotic distance functions that fit within this space. For example, we can choose other <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">vector norms</a> $d(x,y) = ||x -y||$ or information theoretic quantities like <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence"><em>Jensen-Shannon divergence</em></a>.
</p>
<p class="footnotes">
<a href="#f3t" id="f3b"><b>[3]</b></a> Interesting side note: the title of Kleinberg’s paper — <em>An Impossibility Theorem for Clustering</em> — is an homage to <a href="https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem"><em>Kenneth Arrow’s impossibility theorem</em></a>, which roughly states that there is no “fair” voting system in which voters rank three or more choices. As in Kleinberg’s approach, “fairness” is defined by three axioms, which cannot be simultaneously satisfied.
</p>

</div>

<!-- <div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2018/02/26/crossval/">
            How to cross-validate PCA, clustering, and matrix decomposition models
            <small>26 Feb 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2018/02/26/censored-lstsq/">
            Solving Least-Squares Regression with Missing Data
            <small>26 Feb 2018</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/12/01/uniqueness/">
            On the identifiability of PCA and related methods.
            <small>01 Dec 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div> -->


      
      
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'quantneuro';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>


    </div>
   
  </body>
</html>
