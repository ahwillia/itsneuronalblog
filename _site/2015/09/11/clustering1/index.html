<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      What is clustering and why is it hard? &middot; Its 
 Neuronal
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>
    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">
      <a class="sidebar-nav-item" href="/itsneuronalblog/">Home</a>

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
          
        
      
        
          
        
      

      <h2 style="color:rgba(255,255,255,0.9)">Online Texts</h2>
      <hr style="margin:0.5rem 0">

      <a class="sidebar-nav-item" href="/theory_book" target="_blank">Intro to Comp Neuro</a>
    </nav>

    <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
      <img alt="Creative Commons License" style="margin:0 auto;display:block" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
    </a>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">What is clustering and why is it hard?</h1>
  <span class="post-date">11 Sep 2015</span>
  <!-- Look the author details up from the site config. -->
  

  <!-- Output author details if some exist. -->
  
      <div style="height:50px">
          <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

          <small style="vertical-align:middle;">
          Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
          </small>
      </div>
      <br>
  
  <p>I’ve been working on some clustering techniques to <a href="http://alexhwilliams.info/pubs/DoE_2015_DNA_meth.compressed.pdf">identify cell types from DNA methylation data</a>. When you dive into the literature on clustering, two things becomes immediately apparent: first, clustering is fundamental to many scientific questions, and second, there is <a href="http://stanford.edu/~rezab/papers/slunique.pdf">“distressingly little general theory”</a> on how it works or how to apply it to your particular data.</p>

<p>This was surprising to me. I imagine that most biologists and neuroscientists come across <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering</a>, <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">hierarchical clustering</a>, and similar techniques all the time in papers related to their work. Given how commonplace these techniques are, one would think that we have a solid handle on how they work and what can go wrong.</p>

<p>This will be the first post in a short series on clustering techniques. I will try to explain why clustering is hard from a high-level, intuitive perspective. The next post will cover some more technical theoretical results. I’ll focus on <a href="http://web.stanford.edu/~rezab/classes/cme305/W15/Notes/Kleinberg%20-%20impossibility%20theorem.pdf">Jon Kleinberg’s paper</a> which precisely defines an ideal clustering function, but then proves that <strong><em>no such function exists</em></strong> and that there are inevitable tradeoffs that must be made. The final few posts will cover other theoretical work and some current projects of mine.</p>

<!--more-->

<h3 id="what-is-clustering">What is clustering?</h3>

<p><a href="https://en.wikipedia.org/wiki/Clustering"><em>Clustering</em></a> is difficult because it is an <em>unsupervised learning</em> problem: we are given a dataset and are asked to infer structure within it (in this case, the latent clusters/categories in the data). The problem is that there isn’t necessarily a “correct” or ground truth solution that we can refer to if we want to check our answers. This is in contrast to <a href="https://en.wikipedia.org/wiki/Statistical_classification"><em>classification problems</em></a>, where we do know the ground truth. Deep artificial neural networks are very good at classification (<a href="http://bits.blogs.nytimes.com/2014/08/18/computer-eyesight-gets-a-lot-more-accurate/"><em>NYT article</em></a>; <a href="http://www.image-net.org/papers/imagenet_cvpr09.pdf">Deng et al. 2009</a>), but clustering is still a very open problem.</p>

<p>For example, it is a classification problem to predict whether or not a patient has a common disease based on a list of symptoms. In this case, we can draw upon past clinical records to make this judgment, and we can gather further data (e.g. a blood test) to confirm our prediction. In other words, we assume there is a self-evident ground truth (the patient either has or does not have disease X) that can be observed.</p>

<p>For clustering, we lack this critical information. For example, suppose you are given a large number of beetles and told to group them into clusters based on their appearance. Assuming that you aren’t an entomologist, this will involve some judgment calls and guesswork.<sup>[1]</sup> If you and a friend sort the same 100 beetles into 5 groups, you will likely come up with slightly different answers. And — here’s the important part — there isn’t really a way to determine which one of you is “right”.</p>

<h3 id="approaches-for-clustering">Approaches for clustering</h3>

<p>There is a lot of material written on this already, so rather than re-hash what’s out there I will just point you to the best resources.</p>

<ul>
  <li>
    <p><strong>K-means clustering</strong>  (<a href="https://en.wikipedia.org/wiki/K-means_clustering"><em>Wikipedia</em></a>, <a href="http://www.bytemuse.com/post/k-means-clustering-visualization/">Visualization by @ChrisPolis</a>, <a href="http://tech.nitoyon.com/en/blog/2013/11/07/k-means/">Visualization by TECH-NI blog</a>).</p>
  </li>
  <li>
    <p><strong>Hierarchical clustering</strong> works by starting with each datapoint in its own cluster and fusing the nearest clusters together repeatedly (<a href="https://en.wikipedia.org/wiki/Hierarchical_clustering"><em>Wikipedia</em></a>, <a href="https://youtu.be/XJ3194AmH40"><em>Youtube #1</em></a>, <a href="https://youtu.be/VMyXc3SiEqs"><em>Youtube #2</em></a>).</p>

    <ul>
      <li><a href="https://en.wikipedia.org/wiki/Single-linkage_clustering"><strong>Single-linkage clustering</strong></a> is a particularly popular and well-characterized form of hierarchical clustering. Briefly, single-linkage begins by initializing each point as its own cluster, and then repeatedly combining the two closest clusters (as measured by their closest points of approach) until the desired number of clusters is achieved.</li>
    </ul>
  </li>
  <li>
    <p><strong>Bayesian methods</strong> include <a href="http://ifas.jku.at/gruen/BayesMix/bayesmix-intro.pdf">finite mixture models</a> and <a href="http://www.kyb.tue.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2299.pdf">infinite mixture models</a>.<sup>[2]</sup></p>
  </li>
</ul>

<p>The important thing to realize is that all of these approaches are <a href="http://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf">very computationally difficult</a> to solve exactly for large datasets (more on this in my next post). As a result, we often resort to optimization heuristics that may or may not produce reasonable results. And, as we will soon see, even if the results are “reasonable” from the algorithm’s perspective, they might not align with our intuition, prior knowledge, or desired outcome.</p>

<h3 id="it-is-difficult-to-determine-the-number-of-clusters-in-a-dataset">It is difficult to determine the number of clusters in a dataset</h3>

<p>This has to be the most widely understood problem with clustering. In fact, there is an <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set"><em>entire Wikipedia article</em></a> devoted to it. If you think about the problem for long enough, you will come to the inescapable conclusion is that there is no “true” number of clusters (though some numbers <strong><em>feel</em></strong> better than others), and that the same dataset is appropriately viewed at various levels of granularity depending on analysis goals.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering_ambiguity.png" alt="" width="400px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b></b>
	
</p></td></tr>
</table>

<p>While this problem cannot be “solved” definitively, there are some nice ways of dealing with it. Hierarchical clustering approaches provide cluster assignments for all possible number of clusters, allowing the analyst or reader to view the data across different levels of granularity. There are also Bayesian approaches such as <a href="http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/">Dirichlet Process Mixture Models</a> that adaptively estimate the number of clusters based on a hyperparameter which tunes dispersion. A number of recent papers have focused on <em>convex clustering</em> techniques that fuse cluster centroids together in a continuous manner along a regularization path; this exposes a hierarchical structure for a clustering approach (roughly) similar to k-means.<sup>[3]</sup> Of course, there are many other papers out there on this subject.<sup>[4]</sup></p>

<h3 id="it-is-difficult-to-cluster-outliers-even-if-they-form-a-common-group">It is difficult to cluster outliers (even if they form a common group)</h3>

<p>I recommend you read David Robinson’s excellent post on <a href="http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means">the shortcomings of k-means clustering</a>. The following example he provides is particularly compelling:</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Raw Data.. </b>Three spherical clusters with variable numbers of elements/points.</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering_01.png" alt="Three spherical clusters with variable numbers of elements/points." width="400px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Raw Data.</b>
	Three spherical clusters with variable numbers of elements/points.
</p></td></tr>
</table>

<p>The human eye can pretty easily separate these data into three groups, but the k-means algorithm fails pretty hard:</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering_02.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b></b>
	
</p></td></tr>
</table>

<p>Rather than assigning the points in the upper left corner to their own cluster, the algorithm breaks the largest cluster (in the upper right) into two clusters. In other words it tolerates a few large errors (upper left) in order to decrease the errors where data is particularly dense (upper right). This likely doesn’t align with our analysis, but it is <em>completely reasonable</em> from the perspective of the algorithm. And again, <strong><em>there isn’t a ground truth to show that the algorithm is “wrong” per se.</em></strong></p>

<h3 id="it-is-difficult-to-cluster-non-spherical-overlapping-data">It is difficult to cluster non-spherical, overlapping data</h3>

<p>A final, related problem arises from the shape of the data clusters. Every clustering algorithm makes structural assumptions about the dataset that need to be considered. For example, k-means works by minimizing the total sum-of-squared distance to the cluster centroids. This can produce undesirable results when the clusters are elongated in certain directions — particularly when the between-cluster distance is smaller than the maximum within-cluster distance. Single-linkage clustering, in contrast, can perform well in these cases, since points are clustered together based on their nearest neighbor, which facilitates clustering along ‘paths’ in the dataset.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Datasets where single-linkage outperforms k-means.. </b>If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C).</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/kmeans_fail.png" alt="If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C)." width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Datasets where single-linkage outperforms k-means.</b>
	If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C).
</p></td></tr>
</table>

<p>However, there is <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem">no free lunch</a>.<sup>[5]</sup> Single-linkage clustering is more sensitive to noise, because each clustering assignment is based on a single pair of datapoints (the pair with minimal distance). This can cause paths to form between overlapping clouds of points. In contrast, k-means uses a more global calculation — minimizing the distance to the nearest centroid summed over all points. As a result, k-means typically does a better job of identifying partially overlapping clusters.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>A dataset where k-means outperforms single-linkage.. </b>Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances.</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/linkage_fail.png" alt="Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances." width="550px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>A dataset where k-means outperforms single-linkage.</b>
	Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances.
</p></td></tr>
</table>

<p>The above figures were schematically reproduced from <a href="http://www.stat.cmu.edu/~cshalizi/350/lectures/08/lecture-08.pdf">these lecture notes</a> from a statistics course at Carnegie Mellon.</p>

<h3 id="are-there-solutions-these-problems">Are there solutions these problems?</h3>

<p>This post was meant to highlight the inherent difficulty of clustering rather than propose solutions to these issues. It may therefore come off as a bit pessimistic. There are many heuristics that can help overcome the above issues, but I think it is important to emphasize that these are <strong><em>only heuristics</em></strong>, not guarantees. While many of biologists treat k-means as an “off the shelf” clustering algorithm, we need to be at least a little careful when we do this.</p>

<p>One of the more interesting heuristics worth reading up on is called <a href="http://dx.doi.org/10.1109/ICPR.2002.1047450">ensemble clustering</a>. The basic idea is to average the outcomes of several clustering techniques or from the same technique fit from different random initializations. Each clustering fit may suffer from instability, but the average behavior of the ensemble of models will tend to be more desirable. This general trick is called <a href="https://en.wikipedia.org/wiki/Ensemble_averaging"><em>ensemble averaging</em></a> and has been successfully applied to a number of machine learning problems.<sup>[6]</sup></p>

<p>This post only provides a quick outline of the typical issues that arise for clustering problems. The details of the algorithms have been purposefully omitted, although a deep understanding of these issues likely requires a closer look at these specifics. <a href="http://www.cse.msu.edu/biometrics/Publications/Clustering/JainClustering_PRL10.pdf">Jain (2010)</a> provides a more comprehensive review that is worth reading.</p>

<hr />

<div class="share-page">
    <strong>
    Share this on &rarr; 
    &nbsp;&nbsp;
    <a href="https://twitter.com/intent/tweet?text=What is clustering and why is it hard?&amp;url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;via=ItsNeuronal&amp;related=ItsNeuronal" rel="nofollow" target="_blank" title="Share on Twitter">Twitter</a>
    &nbsp;&nbsp;
    <a href="https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;t=What is clustering and why is it hard?" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a>
    &nbsp;&nbsp;
    <a href="https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/" rel="nofollow" target="_blank" title="Share on Google+">Google+</a>
	&nbsp;&nbsp;
	<a href="http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;title=What is clustering and why is it hard?" rel="nofollow" target="_blank" title="Share on Google+">Reddit</a>
    </strong>
</div>

<h4 id="footnotes">Footnotes</h4>

<p><span class="footnotes">
<strong>[1]</strong> It will probably involve guesswork even if you are.<br />
<strong>[2]</strong> Highlighting the difficulty of clustering, <a href="https://normaldeviate.wordpress.com/2012/08/04/mixture-models-the-twilight-zone-of-statistics/">Larry Wasserman</a> has joked that “mixtures, like tequila, are inherently evil and should be avoided at all costs.” <a href="http://andrewgelman.com/2012/08/15/how-to-think-about-mixture-models/">Andrew Gelman</a> is slightly less pessimistic.<br />
<strong>[3]</strong> Convex clustering performs continuous clustering, similar to how <a href="http://statweb.stanford.edu/~tibs/lasso/lasso.pdf">LASSO</a> performs continuous variable selection.<br />
<strong>[4]</strong> See <a href="http://dx.doi.org/10.1111/1467-9868.00293">Tibshirani et al. (2001)</a>, <a href="http://dx.doi.org/10.1186/gb-2002-3-7-research0036">Dudoit &amp; Fridlyand (2002)</a>, <a href="http://dx.doi.org/10.1109/34.990138">Figueiredo &amp; Jain (2002)</a>, <a href="http://dx.doi.org/10.1111/j.1541-0420.2007.00784.x">Yan &amp; Ye (2007)</a>, <a href="http://www.cs.berkeley.edu/~jordan/papers/kulis-jordan-icml12.pdf">Kulis &amp; Jordan (2012)</a><br />
<strong>[5]</strong> The <a href="http://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf">“no free lunch” theorem</a> roughly states that whenever an algorithm performs well on a certain class of problems it is because it makes <em>good assumptions</em> about those problems; however, you can always construct new problems that violate these assumptions, leading to worse performance. Interestingly, this basic idea pops up in other contexts. For example, certain feedback control systems can be engineered so that they are robust to particular perturbations, but such engineering renders them <strong><em>more sensitive</em></strong> to other forms of perturbations (see <a href="https://en.wikipedia.org/wiki/Bode%27s_sensitivity_integral">“waterbed effect.”</a>)<br />
<strong>[6]</strong> See <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating"><em>bootstrap aggregation</em></a>, <a href="https://en.wikipedia.org/wiki/Random_forest"><em>random forests</em></a>, and <a href="https://en.wikipedia.org/wiki/Ensemble_learning"><em>ensemble learning</em></a>. Seminal work on this topic was done by <a href="https://en.wikipedia.org/wiki/Leo_Breiman">Leo Breiman </a> — his papers are lucid, fascinating, and accessible, and I particularly recommend his 1996 article <a href="http://dx.doi.org/10.1007/BF00058655">“Bagging Predictors”</a>. This is also covered in any modern textbook, such as <em>The Elements of Statistical Learning</em>.
</span></p>

</div>

<!-- <div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/12/30/feedback-alignment/">
            Is backprop necessary to train neural networks?
            <small>30 Dec 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/01/10/nonidentifiable-neural-codes/">
            What is a neuron's firing rate? Is it even well-defined?
            <small>10 Jan 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/12/15/nips/">
            Highlights of NIPS2015
            <small>15 Dec 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div> -->


      
      
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'quantneuro';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>


    </div>
   
  </body>
</html>
