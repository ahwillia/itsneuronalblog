<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Clustering is hard, except when it's not &middot; Its 
 Neuronal
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>
    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">
      <a class="sidebar-nav-item" href="/itsneuronalblog/">Home</a>

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
          
        
      
        
          
        
      

      <h2 style="color:rgba(255,255,255,0.9)">Online Texts</h2>
      <hr style="margin:0.5rem 0">

      <a class="sidebar-nav-item" href="/theory_book" target="_blank">Intro to Comp Neuro</a>
    </nav>

    <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
      <img alt="Creative Commons License" style="margin:0 auto;display:block" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
    </a>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Clustering is hard, except when it's not</h1>
  <span class="post-date">18 Nov 2015</span>
  <!-- Look the author details up from the site config. -->
  

  <!-- Output author details if some exist. -->
  
      <div style="height:50px">
          <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

          <small style="vertical-align:middle;">
          Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
          </small>
      </div>
      <br>
  
  <p>The previous two posts (<a href="/itsneuronalblog/2015/09/11/clustering1/">part 1</a>, <a href="/itsneuronalblog/2015/09/11/clustering2/">part 2</a>) on clustering have been somewhat depressing and pessimistic. However, the reality is that scientists use simple clustering heuristics <em>all the time</em>, and often find interpretable results. What gives? Is the theoretical hardness of clustering flawed? Or have we just been deluding ourselves? Have we been fooled into believing results that are in some sense fundamentally flawed?</p>

<p>This post will explore a more optimistic possibility, which has been referred to as the <em>“Clustering is Only Difficult When It Does Not Matter” hypothesis</em>. Proponents argue that, while we can construct worst-case scenarios that cause algorithms to fail, clustering techniques work very well in practice because real-world datasets often have characteristic structure that more-or-less guarantees the success of these algorithms. Put differently, <a href="http://arxiv.org/abs/1205.4891">Daniely et al. (2012)</a> say that “clustering is easy, otherwise it is pointless” — whenever clustering fails, it is probably because the data in question were not amenable to clustering in the first place.</p>

<!--more-->

<h3 id="notation">Notation</h3>

<p>In this post, we are going to view clustering as an optimization problem.</p>

<ul>
  <li>
    <p>Let $\mathcal{C}$ denote a clustering (or <a href="https://en.wikipedia.org/wiki/Partition_of_a_set">partition</a>) of a dataset into $k$ clusters.<a href="#f1b" id="f1t"><sup>[1]</sup></a></p>
  </li>
  <li>
    <p>Let $F(\mathcal{C})$ be the loss function (a.k.a objective function) that computes a “cost” or “badness” for any clustering.</p>
  </li>
  <li>
    <p>Our goal is to find the <em>best</em> or <em>optimal</em> clustering (i.e. the one with the lowest value of $F$). We call the optimal clustering $\mathcal C_{\text{opt}}$ , and the lowest/best value of the objective function $F_{\text{opt}}$.</p>
  </li>
</ul>

<script type="math/tex; mode=display">\mathcal{C}_{\text{opt}} = \arg \min_{\mathcal{C}_i} F(\mathcal{C}_i)</script>

<script type="math/tex; mode=display">F_{\text{opt}} = \min_{\mathcal{C}_i} F(\mathcal{C}_i)</script>

<p><strong>Example:</strong> $k$-means clustering results from choosing $F$ to be the sum-of-squared residuals between each datapoint $\mathbf{x}_j$ and the <a href="https://en.wikipedia.org/wiki/Centroid">centroid</a> ($\bar{\mathbf{x}}$) of the cluster it belongs to:</p>

<script type="math/tex; mode=display">F(\mathcal{C}) = \sum_{i=1}^k \sum_{\mathbf{x}_j \in \mathcal{K}_i}  \big\Vert \bar{\mathbf{x}}_i - \mathbf{x}_j \big\Vert^2_2</script>

<p><strong>Example:</strong> $k$-medians clustering results from choosing $F$ to be the sum of the absolute residuals between each datapoint $\mathbf{x}_j$ and the <a href="https://en.wikipedia.org/wiki/Medoid">mediod</a> ($\tilde{\mathbf{x}}$) of the cluster it belongs to:</p>

<script type="math/tex; mode=display">F(\mathcal{C}) = \sum_{i=1}^k \sum_{\mathbf{x}_j \in \mathcal{K}_i} \big \vert \tilde{\mathbf{x}}_i - \mathbf{x}_j \big \vert</script>

<p>For the purposes of this post, you can assume we’re using either of the above objective functions.<a href="#f2b" id="f2t"><sup>[2]</sup></a> Throughout this post, we assume that the number of clusters, $k$, is known <em>a priori</em> — analysis becomes very difficult otherwise.</p>

<h3 id="intuition-behind-easy-vs-hard-clustering">Intuition behind easy vs. hard clustering</h3>

<p>It is easy to construct datasets where it takes a <em>very long time</em> to find $\mathcal{C}_{\text{opt}}$. Consider the (schematic) dataset below. The data form an amorphous blob of points that are not easily separated into two clusters.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>A dataset not amenable to clustering.. </b>Datapoints are shown as open circles, with color representing cluster assignment and <b>x</b>'s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (<i>left</i>) and $\mathcal{C}_2$ (<i>middle</i>) are shown that have a similar loss. There are many local minima in the objective function (<i>right</i>). <b><i>Disclaimer: schematic, not real data.</i></b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/c_e_condition_1.png" alt="Datapoints are shown as open circles, with color representing cluster assignment and &lt;b&gt;x&lt;/b&gt;'s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (&lt;i&gt;left&lt;/i&gt;) and $\mathcal{C}_2$ (&lt;i&gt;middle&lt;/i&gt;) are shown that have a similar loss. There are many local minima in the objective function (&lt;i&gt;right&lt;/i&gt;). &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;" width="1000px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>A dataset not amenable to clustering.</b>
	Datapoints are shown as open circles, with color representing cluster assignment and <b>x</b>'s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (<i>left</i>) and $\mathcal{C}_2$ (<i>middle</i>) are shown that have a similar loss. There are many local minima in the objective function (<i>right</i>). <b><i>Disclaimer: schematic, not real data.</i></b>
</p></td></tr>
</table>

<p>In the above dataset we can find many clusterings that are nearly equivalent in terms of the loss function (e.g. $\mathcal{C}_1$ and $\mathcal{C}_2$ in the figure). Thus, if we want to be sure to find the <em>very best</em> clustering, we need to essentially do a brute force search.<a href="#f3b" id="f3t"><sup>[3]</sup></a> The <em>“Clustering is Only Difficult When It Does Not Matter” hypothesis</em> (and common sense) would tell us that it is stupid to do a cluster analysis on this dataset — there simply aren’t any clusters to be found!</p>

<p>Now compare this to a case where there are, in fact, two clearly separated clusters. In this case, there is really only one clustering that passes the “common sense” test. Assuming we pick a reasonable loss function, there should also be a very obvious global solution (unlike the first example):</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>A dataset that is easily clustered.. </b>As before, two clusterings are shown. The clustering on the <i>left</i> is much better in terms of the loss function than the clustering shown in the <i>middle</i>. <b><i>Disclaimer: schematic, not real data.</i></b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/c_e_condition_good.png" alt="As before, two clusterings are shown. The clustering on the &lt;i&gt;left&lt;/i&gt; is much better in terms of the loss function than the clustering shown in the &lt;i&gt;middle&lt;/i&gt;. &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;" width="1000px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>A dataset that is easily clustered.</b>
	As before, two clusterings are shown. The clustering on the <i>left</i> is much better in terms of the loss function than the clustering shown in the <i>middle</i>. <b><i>Disclaimer: schematic, not real data.</i></b>
</p></td></tr>
</table>

<p>Intuitively, it should be easier to find the solution for this second dataset: there is <em>clear winner</em> for the clustering, so there is an obvious global minimum, with few local minima. Remember, in the first dataset, there were multiple local minima that were <em>nearly as good</em> as the global minimum.</p>

<h3 id="provably-easy-clustering-situations">Provably “easy” clustering situations</h3>

<p>We would like to formalize the intuition outlined in the previous section to develop efficient and accurate clustering algorithms. To do this we introduce the concept of <strong><em>approximation stability</em></strong>, which characterizes how “nice” the error landscape of the optimization problem is. In the schematic figures, the first difficult-to-cluster example is unstable, while the second easy-to-cluster example is stable. <em>The ultimate punchline is that sufficiently stable clustering problems are provably easy to solve.</em></p>

<blockquote>
  <p><strong>Definition:</strong> $(c,\epsilon)$-approximation-stability.</p>

  <p>A clustering problem is said to be $(c,\epsilon)$-stable when all clusterings, $\mathcal{C^\prime}$, that satisfy $ F(\mathcal{C}^\prime) \leq c F_{\text{opt}}$ also satisfy $d(\mathcal{C}^\prime,\mathcal{C}_{\text{opt}}) &lt; \epsilon$. Here, $0 &lt; \epsilon \ll 1$, and $c &gt; 1$, and $d(\cdot,\cdot)$ measures the fraction of differently assigned datapoints between two clusterings.</p>
</blockquote>

<p>The more stable the clustering problem is, the larger $c$ and the smaller $\epsilon$ are allowed to be. For example, if $c = 1.1$ and $\epsilon = 0.02$, then a problem is $(c,\epsilon)$-stable if all clusterings within 10% of the optimal objective value, are no more than 2% different from the optimal clustering.</p>

<p>As cluster stability increases, two things happen:</p>

<ol>
  <li>
    <p><strong>The problem becomes easier to solve.</strong> <a href="http://dx.doi.org/10.1145/2450142.2450144">Balcan et al. (2013)</a> provide several algorithms that are guaranteed to find <em>near-optimal</em> clusterings if the clusters are large enough and the problem is stable enough.<a href="#f4b" id="f4t"><sup>[4]</sup></a> These algorithms are very efficient,<a href="#f5b" id="f5t"><sup>[5]</sup></a> easy to implement, and similar to classic clustering algorithms.</p>
  </li>
  <li>
    <p><strong>Cluster analysis becomes more sensible and interpretable.</strong> While not immediately obvious, it turns out that approximation stability (as well as similar concepts, like <a href="http://arxiv.org/abs/1112.0826"><em>perturbation stability</em></a>) correlates with our intuitive sense of clusterability: when the data contain well-separated and compact clusters, then the clustering optimization problem is likely stable. This is outlined in Lemma 3.1 by <a href="http://dx.doi.org/10.1145/2450142.2450144">Balcan et al. (2013)</a>.</p>
  </li>
</ol>

<p>In short, research along these lines is <a href="#caveats"><em>beginning</em></a> to provide rigorous support for the <em>“Clustering is Only Difficult When It Does Not Matter” hypothesis</em>. To see the proofs associated with this work in detail check out the course materials for <a href="http://theory.stanford.edu/~tim/f14/f14.html">this class on “Beyond Worst-Case Analysis”</a>. A particularly relevant lecture is embedded below (the others are also online):</p>

<iframe width="373" height="210" style="margin:20px auto; display:block" src="https://www.youtube.com/embed/n0T0fyRt0Xo" frameborder="0" allowfullscreen=""></iframe>

<h3 id="caveats">Caveats</h3>

<p><a href="https://cs.uwaterloo.ca/~shai/">Shai Ben-David</a> recently published <a href="http://arxiv.org/abs/1510.05336">a brief commentary</a> on the <em>“Clustering is Only Difficult When It Does Not Matter” hypothesis</em> alongside <a href="http://arxiv.org/abs/1501.00437">a more detailed paper</a>.<a href="#f6b" id="f6t"><sup>[6]</sup></a> He argues that, while the above results (and others) are encouraging, <strong><em>current theory has only shown clustering to be easy when clusters are very, very obvious</em></strong> in the dataset. For example, Ben-David digs into the specific results of <a href="http://dx.doi.org/10.1145/2450142.2450144">Balcan et al. (2013)</a> and concludes that their (simple, efficient) algorithm indeed produces the correct solution as clustering becomes <em>stable enough</em>. However, “stable enough” in this case more or less means that <strong><em>the majority of points sit more than 20 times closer to their true cluster than to any other cluster.</em></strong> This seems like a very strong assumption, which won’t hold for many practical applications.</p>

<p>There are other caveats to briefly mention.</p>

<ul>
  <li>
    <p>We have restricted our discussion to center-based clustering frameworks (e.g. $k$-means and $k$-medians). This excludes the possibility of clustering more complicated manifolds. However, I’m not sure how much this matters. It is easy to dream up toy, nonlinear datasets (e.g. <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">the Swiss Roll</a>) that cause center-based clustering to fail. Are real-world datasets this pathological? <a href="https://en.wikipedia.org/wiki/Consensus_clustering">Ensemble clustering</a> provides a nice way to cluster non-linear manifolds with center-based techniques. Thus, to address this concern, it would be interesting to extend the theoretical results covered in this post to ensemble-based algorithms.</p>
  </li>
  <li>
    <p>Throughout this post we have assumed that the number of clusters is known beforehand. Estimating the number of clusters ($k$) is a well-known and generally unsolved problem.<a href="#f7b" id="f7t"><sup>[7]</sup></a> In practice, we typically run clustering algorithms for various choices of $k$, and compare results in a somewhat <em>ad hoc</em> manner. For clustering to truly be “easy”, we need simple, consistent, and accurate methods for estimating $k$. While there is some work on this issue (e.g., <a href="http://statweb.stanford.edu/~gwalther/gap">Tibshirani et al., 2001</a>), most of it is constrained to the case of “well-separated” clusters.</p>
  </li>
</ul>

<h3 id="conclusions-and-related-work">Conclusions and related work</h3>

<p>A theoretical understanding of clustering algorithms is desperately needed, and despite substantial caveats, it seems that we are beginning to make progress. I find the theoretical analysis in this area to be quite interesting and worthy of further work. However, it may be overly optimistic to conclude the <em>“Clustering is only difficult when it does not matter”</em>. Given current results, it is probably safer to conclude that <em>“Clustering is difficult, except when it isn’t”</em>.</p>

<p>The essential flavor of this work is part of a <a href="http://sunju.org/research/nonconvex/">growing literature</a> on finding provably accurate and efficient algorithms to solve problems that were traditionally thought to be difficult (often NP-Hard) to solve. A well-known example in the machine learning community is <a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization">nonnegative matrix factorization (NMF)</a> under the “separability condition.” While NMF is NP-hard in general, work by <a href="http://arxiv.org/abs/1111.0952">Arora et al. (2012)</a> showed that it could be solved in polynomial time under certain assumptions (which were typically satisfied or nearly satisfied, in practice). <a href="http://arxiv.org/abs/1310.7529">Further</a> <a href="http://arxiv.org/abs/1208.1237">work</a> by <a href="https://scholar.google.be/citations?user=pVIJV7wAAAAJ">Nicolas Gilles</a> on this problem is worthy of special mention.</p>

<p>While all of this may seem a bit tangential to the topic of clustering, it really isn’t. One of the reasons NMF is useful is that it produces a sparse representation of a dataset, which can be thought of as an approximate clustering, or <em>soft clustering</em> of a dataset <a href="https://smartech.gatech.edu/bitstream/handle/1853/20058/GT-CSE-08-01.pdf?sequence=1">(Park &amp; Kim, 2008)</a>. In other words, the very recent and very exciting work on provable NMF algorithms raises the tantalizing possibility that these ideas will soon provide deep insight into clustering.</p>

<hr />

<div class="share-page">
    <strong>
    Share this on &rarr; 
    &nbsp;&nbsp;
    <a href="https://twitter.com/intent/tweet?text=Clustering is hard, except when it's not&amp;url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;via=ItsNeuronal&amp;related=ItsNeuronal" rel="nofollow" target="_blank" title="Share on Twitter">Twitter</a>
    &nbsp;&nbsp;
    <a href="https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;t=Clustering is hard, except when it's not" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a>
    &nbsp;&nbsp;
    <a href="https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/" rel="nofollow" target="_blank" title="Share on Google+">Google+</a>
	&nbsp;&nbsp;
	<a href="http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;title=Clustering is hard, except when it's not" rel="nofollow" target="_blank" title="Share on Google+">Reddit</a>
    </strong>
</div>

<h4 id="footnotes">Footnotes</h4>

<p class="footnotes">
<a href="#f1t" id="f1b"><b>[1]</b></a> Explicitly, $\mathcal{C} = \{ \mathcal{K}_1,\mathcal{K}_2,…,\mathcal{K}_k\}$, where each $\mathcal{K}_i$ is a set of datapoints, $\mathcal{K}_i = \{\mathbf{x}_1^{(i)},\mathbf{x}_2^{(i)},…\}$, where $\mathbf{x}_j^{(i)}$, is a datapoint (a vector) in cluster $i$, indexed by $j$.
</p>
<p class="footnotes">
<a href="#f2t" id="f2b"><b>[2]</b></a> It is instructive to prove that the arithmetic mean (i.e. centroid) of a set of points <a href="http://math.stackexchange.com/questions/967138/formal-proof-that-mean-minimize-squared-error-function">minimizes the sum-of-squared residuals</a>. Similarly, the median <a href="http://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations">minimizes the sum-of-absolute residuals</a>.
</p>
<p class="footnotes">
<a href="#f3t" id="f3b"><b>[3]</b></a> Speaking loosely, the necessity of this brute-force search makes finding the solution to the k-means optimization problem NP-hard <a href="https://dx.doi.org/10.1007%2Fs10994-009-5103-0">(Aloise et al., 2009)</a>. Note that there are simple and efficient algorithms that find local minima, given an initial guess. However, solving the problem (i.e. finding and certifying that you’ve found the global minimum) is NP-hard in the worst-case.
</p>
<p class="footnotes">
<a href="#f4t" id="f4b"><b>[4]</b></a> What, exactly, does it mean for a clustering problem to be “stable enough”? This is a very critical question, that is revisited in the <a href="#caveats">caveats section</a>.
</p>
<p class="footnotes">
<a href="#f5t" id="f5b"><b>[5]</b></a> They run in <a href="https://en.wikipedia.org/wiki/P_(complexity)">polynomial time</a>. Again, clustering problems are NP-hard in the worst case; they (probably) take exponential time to solve (assuming P $\neq$ NP).
</p>
<p class="footnotes">
<a href="#f6t" id="f6b"><b>[6]</b></a> I borrowed his phrase — the <em>“Clustering is Only Difficult When It Does Not Matter”</em> hypothesis — for this post, although it is also a title of a <a href="http://arxiv.org/abs/1205.4891">older paper</a> from a different group.
</p>
<p class="footnotes">
<a href="#f7t" id="f7b"><b>[7]</b></a> I discussed this point to some extent in <a href="/itsneuronalblog/2015/09/11/clustering1/">a previous post</a>
</p>

</div>

<!-- <div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2015/12/10/nonidentifiable-neural-codes/">
            What is a neuron's firing rate? Is it even well-defined?
            <small>10 Dec 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/11/01/clustering3/">
            Improved clustering by using prior knowledge
            <small>01 Nov 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/10/01/clustering2/">
            Is clustering mathematically impossible?
            <small>01 Oct 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div> -->


      
      
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'quantneuro';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>


    </div>
   
  </body>
</html>
