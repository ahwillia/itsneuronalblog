<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>A blog on math and computation in neuroscience.</description>
    <link>http://alexhwilliams.info</link>
    <atom:link href="http://alexhwilliams.info/feed.xml" rel="self" type="application/rss+xml" />
    
    
    
    
    
    
    
    
    
    
      <item>
        <title>Highlights of NIPS2015</title>
        <description>&lt;p&gt;I was warned that NIPS is an overwhelming conference, but I didn’t listen because I’ve gotten used to SfN, which is several times larger. But for what NIPS lacks in size (nearly 4,000 attendees, still no joke) it more than makes up for in it’s energy. It feels like I haven’t talked about anything other than statistics and machine learning for the last 7 days, and I don’t even remember what a good night’s sleep feels like anymore. I’m writing this up on the bus home, physically and emotionally defeated. But my boss told me to consolidate some brief notes from the conference, so here is my attempt.&lt;/p&gt;

&lt;p&gt;&lt;i&gt;&lt;strong&gt;Note:&lt;/strong&gt; Watch for updates: I’d like to fill in more links and details about the talks/posters)&lt;/i&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;high-level-probably-misinformed-thoughts-on-deep-learning&quot;&gt;High-level (probably misinformed) thoughts on deep learning&lt;/h3&gt;

&lt;p&gt;It is pretty much impossible for me to write this post without weighing in on the deep learning craze. From my perspective, it seems like we’ve pretty much solved pattern recognition &lt;a href=&quot;http://rocknrollnerd.github.io/ml/2015/05/27/leopard-sofa.html&quot;&gt;modulo some edge cases&lt;/a&gt;. In fact, we’ve solved it insanely well in certain domains. Take &lt;a href=&quot;http://dx.doi.org/10.1038/nature14236&quot;&gt;DeepMind’s Atari-playing deep net&lt;/a&gt;. Without a doubt, it is an incredible feat of engineering. But does it really “learn” to play the games on a conceptual level?&lt;/p&gt;

&lt;p&gt;Consider what would happen if you flipped a game like space invaders upside down, or even simply switched the red and blue color channels on the pixels.&lt;a href=&quot;#f1b&quot; id=&quot;f1t&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; A human could probably adjust to these changes nearly instantaneously. The deep net would be completely confused and unable to play the game. Sure, you could retrain it, and you might even argue that relearning just requires changes to the early layers of the network. However, a naïve retraining procedure could potentially modify the weights in deeper layers, destroying useful high-level abstractions that the network had already learned.&lt;a href=&quot;#f2b&quot; id=&quot;f2t&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.mit.edu/~mnick/brains-minds-and-machines-2015/&quot;&gt;Brains, Minds, and Machines Symposium&lt;/a&gt; shed great light on these issues. For me, the talks by Josh Tenenbaum and Gary Marcus were particularly insightful. The basic idea I left with (from these talks and other conversations) was that deep learning is very well-suited for pattern recognition and dealing with high-dimensional inputs. But we need to combine this with other frameworks — for example probabilistic inference and simulation — to solve many problems that humans do with ease. Prior to NIPS I had the misconception that the Bayesian/probabilistic viewpoint couldn’t scale to hard problems. What I failed to realize was that &lt;strong&gt;&lt;em&gt;many hard problems aren’t high-dimensional problems&lt;/em&gt;&lt;/strong&gt;. Josh gave the example of inferring the position of a person’s occluded limb in a crowded photo (something deep nets can’t do… yet). Put simply, we don’t have that many limbs and that many possible configurations for them to be in, so having a cognitive engine that simulates all the possibilities is feasible.&lt;/p&gt;

&lt;p&gt;The take-home message was that cognitive science is actually pretty awesome and (potentially) has a lot to offer. Coming from a molecular/cellular neurobiology research background, this was an incredibly refreshing perspective.&lt;/p&gt;

&lt;h3 id=&quot;neuroscience-at-nips&quot;&gt;Neuroscience at NIPS&lt;/h3&gt;

&lt;h4 id=&quot;posters&quot;&gt;Posters&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5767-high-dimensional-neural-spike-train-analysis-with-generalized-count-linear-dynamical-systems&quot;&gt;&lt;strong&gt;High-dimensional neural spike train analysis with generalized count linear dynamical systems&lt;/strong&gt;&lt;/a&gt;.&lt;br /&gt;&lt;em&gt;Yuanjun Gao, Lars Büsing, Krishna V. Shenoy, John P. Cunningham&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Extends previous work (&lt;a href=&quot;http://dx.doi.org/10.1080/09548980701625173&quot;&gt;Kulkarni &amp;amp; Paninski, 2007&lt;/a&gt;; &lt;a href=&quot;http://papers.nips.cc/paper/4995-robust-learning-of-low-dimensional-dynamics-from-large-neural-ensembles&quot;&gt;Pfau et al., 2013&lt;/a&gt;) on extracting low-dimensional dynamics from network recordings. Previous work has assumed Poisson output, but this is constrained by (mean = variance). Real spike counts are often &lt;a href=&quot;https://en.wikipedia.org/wiki/Overdispersion&quot;&gt;over-dispersed&lt;/a&gt;, so something like a &lt;a href=&quot;https://en.wikipedia.org/wiki/Negative_binomial_distribution&quot;&gt;negative binomial distribution&lt;/a&gt; may be more accurate. The authors show how to extend traditional GLMs to accommodate this class of models (and all others in the exponential family). They also apply their method to some interesting data to demonstrate the advantages of this approach.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5962-convolutional-spike-triggered-covariance-analysis-for-neural-subunit-models.pdf&quot;&gt;&lt;strong&gt;Convolutional spike-triggered covariance analysis for neural subunit models&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Anqi Wu, Il Memming Park, Jonathan W. Pillow&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The latest in the quest to fit good phenomenological models of early sensory neurons. The authors examine how to efficiently fit parameters of subunit models — in which a output neuron is activated by a layer of “subunits” with shifted linear filters and individual nonlinearities. Fitting these models is generally hard — see earlier work by &lt;a href=&quot;http://papers.nips.cc/paper/4742-efficient-and-direct-estimation-of-a-neural-subunit-model-for-sensory-coding&quot;&gt;Vintch et al. (2012)&lt;/a&gt;. However, the authors show that it can be easy under certain assumptions.&lt;a href=&quot;#f3b&quot; id=&quot;f3t&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt; They derive an estimator based on the spike-triggered average and covariance; it seems to work well even when the assumptions are violated.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5952-synaptic-sampling-a-bayesian-approach-to-neural-network-plasticity-and-rewiring&quot;&gt;&lt;strong&gt;Synaptic Sampling: A Bayesian Approach to Neural Network Plasticity and Rewiring&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;em&gt;David Kappel, Stefan Habenschuss, Robert Legenstein, Wolfgang Maass&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I really like the motivation of this poster: synapses are highly dynamic biological units that grow, retract, and change in size and strength. Despite this indisputable fact, almost all modeling work considers synaptic weights that are stable, noiseless, and deterministically updated by learning rules. The authors construct and examine a framework where the synaptic weights stochastically explore a probability distribution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5948-enforcing-balance-allows-local-supervised-learning-in-spiking-recurrent-networks&quot;&gt;&lt;strong&gt;Enforcing balance allows local supervised learning in spiking recurrent networks&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Ralph Bourdoukan, Sophie Denève&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In line with &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pcbi.1003258&quot;&gt;previous work&lt;/a&gt; from Denève’s group, the paper describes how to train spiking neural networks to implement a linear dynamical system. It seems like the key advance is that the rule described here is purely local (and therefore perhaps more biologically plausible).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;shameless-plug&quot;&gt;Shameless Plug&lt;/h4&gt;

&lt;p&gt;Some talented friends from the Columbia NeuroTheory group have started a new company called &lt;a href=&quot;https://cognescent.com/&quot;&gt;&lt;strong&gt;&lt;em&gt;Cognescent&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;. They’re just getting started, but keep an eye on them! They want to expand their team, so get in touch if you are looking for a job.&lt;/p&gt;

&lt;h3 id=&quot;nonconvex-optimization&quot;&gt;Nonconvex Optimization&lt;/h3&gt;

&lt;h4 id=&quot;posters-1&quot;&gt;Posters&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5728-accelerated-proximal-gradient-methods-for-nonconvex-programming&quot;&gt;&lt;strong&gt;Accelerated Proximal Gradient Methods for Nonconvex Programming&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Huan Li, Zhouchen Lin&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Seminal work by Nesterov produced simple methods for smooth, convex problems that achieved fast (quadratic) convergence using only information the gradient of the objective function (see &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf&quot;&gt;Sutskever et al., 2013&lt;/a&gt; for a review). This work was extended to nonsmooth, convex problems, producing “accelerated” proximal methods (&lt;a href=&quot;http://dx.doi.org/10.1109/TIP.2009.2028250&quot;&gt;Beck &amp;amp; Teboulle, 2009&lt;/a&gt;; &lt;a href=&quot;https://web.stanford.edu/~boyd/papers/prox_algs.html&quot;&gt;Parikh &amp;amp; Boyd, 2014&lt;/a&gt;). This paper by Li and Lin takes it a step further to nonsmooth, nonconvex functions. Like previous work, their algorithm produces iterative updates based on the gradient (subgradient for nonsmooth cases) and an extrapolation term (which is similar, not the same, as momentum). The critical insight is that these extrapolations are potentially quite bad for nonconvex problems, so they extend &lt;a href=&quot;http://dx.doi.org/10.1109/TIP.2009.2028250&quot;&gt;Beck &amp;amp; Teboulle’s&lt;/a&gt; method by monitoring each step and correcting it when it goes off in a bad direction.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;workshop-websitehttpssitesgooglecomsitenips2015nonconvexoptimizationhome&quot;&gt;Workshop &lt;a href=&quot;https://sites.google.com/site/nips2015nonconvexoptimization/home&quot;&gt;&lt;em&gt;(website)&lt;/em&gt;&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://people.csail.mit.edu/hmobahi/&quot;&gt;&lt;strong&gt;Tackling Nonconvex Optimization by Complexity Progression&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
&lt;em&gt;Hossein Mobahi&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The convex envelope of a nonconvex function can be optimized efficiently, and has a unique solution that is equal to the global minimum of the original function. The problem is that it is generally intractable to compute. &lt;a href=&quot;http://people.csail.mit.edu/hmobahi/pubs/gaussian_convenv_2015.pdf&quot;&gt;Mobahi and Fisher (2015)&lt;/a&gt; show that, for certain problems,&lt;a href=&quot;#f4b&quot; id=&quot;f4t&quot;&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/a&gt; a Gaussian smoothing of the nonconvex function can be found in closed form and is the best affine approximation of the convex envelope. You can first solve a &lt;em&gt;very smoothed&lt;/em&gt; version of the nonconvex problem, and then solve progressively less smoothed versions of the problem (i.e. use continuation methods). The basic idea is that solving each smoothed problem gives you a very good warm start on the next, more difficult optimization problem, so you arrive at a good solution. Unfortunately, when the Gaussian smoothing can’t be computed in closed form, it is expensive to compute numerically.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Check out &lt;a href=&quot;#comment-2411704388&quot;&gt;the discussion&lt;/a&gt; below. Closed form smoothing is possible for common nonlinearities in deep networks! Thanks to Hossein for his comments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There were also a couple of cool posters at this workshop:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cheng Tang &amp;amp; Claire Monteleoni presented theoretical analysis and guarantees of the $k$-means clustering algorithm. &lt;a href=&quot;/itsneuronalblog/papers/clustering/poster_ncvx_f.pdf&quot;&gt;The poster&lt;/a&gt; (posted with permission) is similar in spirit to my &lt;a href=&quot;/itsneuronalblog/2015/11/18/clustering-is-easy/&quot;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jacob Abernethy, Alex Kulesza, &amp;amp; Matus Telgarsky presented some simple intution/insights into why deep networks are typically more powerful than wide, shallow networks. The case they examine is very simple, but it nevertheless provides nice insight. There is a &lt;a href=&quot;http://arxiv.org/abs/1509.08101&quot;&gt;paper on arxiv&lt;/a&gt; that covers the material on the poster.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;miscellaneous-things-i-thought-were-cool&quot;&gt;Miscellaneous things I thought were cool&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Andrew Gelman gave an excellent, and thoroughly entertaining, talk on how experiments can suffer from the well-known problem of multiple-comparisons, &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf&quot;&gt;&lt;strong&gt;&lt;em&gt;even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One of the best paper awards, &lt;a href=&quot;http://papers.nips.cc/paper/5762-competitive-distribution-estimation-why-is-good-turing-good&quot;&gt;&lt;em&gt;Competitive Distribution Estimation: Why is Good-Turing Good&lt;/em&gt;&lt;/a&gt;, was pretty interesting to me.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sanjeev Arora announced a &lt;a href=&quot;http://www.offconvex.org/&quot;&gt;new blog on nonconvex optimization&lt;/a&gt; that he will write with a few colleagues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://memming.wordpress.com/2015/12/09/nips-2015-part-2/&quot;&gt;Il Memming Park’s blog&lt;/a&gt; has some nice notes on the conference.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;women-in-machine-learning&quot;&gt;Women in Machine Learning&lt;/h3&gt;

&lt;p&gt;I’ll end with some very short comments about gender balance. First, as I already noted on Twitter, the &lt;a href=&quot;http://www.wimlworkshop.org/&quot;&gt;Women in Machine Learning&lt;/a&gt; (WiML) poster session was fantastic. I thought it was unfortunate that it wasn’t better advertised — I only ended up there by pure accident, since I found the tutorials kind of dull. Unlike the main poster session, which was swamped with a frustrating number of people&lt;a href=&quot;#f5b&quot; id=&quot;f5t&quot;&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/a&gt;, I got to have a couple of really nice in depth conversations about things very relevant to my interests. (I even found out about &lt;a href=&quot;http://www.genomebiology.com/2015/16/1/241&quot;&gt;a cool way of doing factor analysis on RNA expression datasets&lt;/a&gt;!) It would be nice if the WiML meeting was integrated into the main program — perhaps still as a parallel track, but with more general participation.&lt;/p&gt;

&lt;p&gt;In terms of women in the general meeting, the numbers are pretty abysmal:&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet tw-align-center&quot; lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;There&amp;#39;s a lot of room for improvement at &lt;a href=&quot;https://twitter.com/NipsConference&quot;&gt;@NipsConference&lt;/a&gt;. Cutoff: lots of 0s for women in the &lt;a href=&quot;https://twitter.com/hashtag/NIPS2015?src=hash&quot;&gt;#NIPS2015&lt;/a&gt; Symposia &lt;a href=&quot;https://t.co/5MqP8OomEf&quot;&gt;pic.twitter.com/5MqP8OomEf&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sarah Brown (@BrownSarahM) &lt;a href=&quot;https://twitter.com/BrownSarahM/status/673864059859558401&quot;&gt;December 7, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;I’m optimistic that the insane growth in NIPS attendance will bring greater attention and pressure for the organizers to address this issue. I found &lt;a href=&quot;http://dx.doi.org/10.1371%2Fjournal.pcbi.1003903&quot;&gt;&lt;strong&gt;&lt;em&gt;Ten Simple Rules to Achieve Conference Speaker Gender Balance&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; a thought-provoking read, both about why this is an important topic and what we can do about it.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share this on:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=Highlights of NIPS2015&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/&amp;amp;title=Highlights of NIPS2015&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/&amp;amp;t=Highlights of NIPS2015&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f1t&quot; id=&quot;f1b&quot;&gt;&lt;b&gt;[1]&lt;/b&gt;&lt;/a&gt; I didn’t come up with this thought experiment. I stole it from &lt;a href=&quot;https://twitter.com/jhamrick&quot;&gt;@jhamrick&lt;/a&gt; — all credit goes to her.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f2t&quot; id=&quot;f2b&quot;&gt;&lt;b&gt;[2]&lt;/b&gt;&lt;/a&gt; This brings up an interesting research question: is there a general way of identifying layers that should be retrained? Identifying layers that are working fine and should not be retrained? 
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Namely, they assume that (a) the stimulus is Gaussian, (b) that the subunit nonlinearity is a second-order polynomial, and (c) the final nonlinearity is exponential.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f4t&quot; id=&quot;f4b&quot;&gt;&lt;b&gt;[4]&lt;/b&gt;&lt;/a&gt; When the error landscape can be represented by polynomials or Gaussian radial basis functions, then the convolution/smoothing can be solved in closed form.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f5t&quot; id=&quot;f5b&quot;&gt;&lt;b&gt;[5]&lt;/b&gt;&lt;/a&gt; This is another minor criticism I have of the conference. The proportion of posters to people attending is much too small. It became impossible to reach the front of the line and talk with the presenter. Having concurrent sessions/talks is seemingly inevitable given the rapid growth of the conference. The workshops were probably my favorite part for this reason.
&lt;/p&gt;
</description>
        <pubDate>Mon, 14 Dec 2015 00:00:00 -0800</pubDate>
        <link>http://alexhwilliams.info//2015/12/14/nips/</link>
        <guid isPermaLink="true">http://alexhwilliams.info//2015/12/14/nips/</guid>
      </item>
    
    
    
      <item>
        <title>Clustering is hard, except when it&#39;s not</title>
        <description>&lt;p&gt;The previous two posts (&lt;a href=&quot;/itsneuronalblog/2015/09/11/clustering1/&quot;&gt;part 1&lt;/a&gt;, &lt;a href=&quot;/itsneuronalblog/2015/09/11/clustering2/&quot;&gt;part 2&lt;/a&gt;) on clustering have been somewhat depressing and pessimistic. However, the reality is that scientists use simple clustering heuristics &lt;em&gt;all the time&lt;/em&gt;, and often find interpretable results. What gives? Is the theoretical hardness of clustering flawed? Or have we just been deluding ourselves? Have we been fooled into believing results that are in some sense fundamentally flawed?&lt;/p&gt;

&lt;p&gt;This post will explore a more optimistic possibility, which has been referred to as the &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter” hypothesis&lt;/em&gt;. Proponents argue that, while we can construct worst-case scenarios that cause algorithms to fail, clustering techniques work very well in practice because real-world datasets often have characteristic structure that more-or-less guarantees the success of these algorithms. Put differently, &lt;a href=&quot;http://arxiv.org/abs/1205.4891&quot;&gt;Daniely et al. (2012)&lt;/a&gt; say that “clustering is easy, otherwise it is pointless” — whenever clustering fails, it is probably because the data in question were not amenable to clustering in the first place.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;notation&quot;&gt;Notation&lt;/h3&gt;

&lt;p&gt;In this post, we are going to view clustering as an optimization problem.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let $\mathcal{C}$ denote a clustering (or &lt;a href=&quot;https://en.wikipedia.org/wiki/Partition_of_a_set&quot;&gt;partition&lt;/a&gt;) of a dataset into $k$ clusters.&lt;a href=&quot;#f1b&quot; id=&quot;f1t&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let $F(\mathcal{C})$ be the loss function (a.k.a objective function) that computes a “cost” or “badness” for any clustering.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Our goal is to find the &lt;em&gt;best&lt;/em&gt; or &lt;em&gt;optimal&lt;/em&gt; clustering (i.e. the one with the lowest value of $F$). We call the optimal clustering $C_{opt}$ , and the lowest/best value of the objective function $F_{opt}$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{C}_{\text{opt}} = \arg \min_{\mathcal{C}_i} F(\mathcal{C}_i)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{\text{opt}} = \min_{\mathcal{C}_i} F(\mathcal{C}_i)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; $k$-means clustering results from choosing $F$ to be the sum-of-squared residuals between each datapoint $\mathbf{x}_j$ and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Centroid&quot;&gt;centroid&lt;/a&gt; ($\bar{\mathbf{x}}$) of the cluster it belongs to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(\mathcal{C}) = \sum_{i=1}^k \sum_{\mathbf{x}_j \in \mathcal{K}_i}  \big\Vert \bar{\mathbf{x}}_i - \mathbf{x}_j \big\Vert^2_2&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; $k$-medians clustering results from choosing $F$ to be the sum of the absolute residuals between each datapoint $\mathbf{x}_j$ and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Medoid&quot;&gt;mediod&lt;/a&gt; ($\tilde{\mathbf{x}}$) of the cluster it belongs to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(\mathcal{C}) = \sum_{i=1}^k \sum_{\mathbf{x}_j \in \mathcal{K}_i} \big \vert \tilde{\mathbf{x}}_i - \mathbf{x}_j \big \vert&lt;/script&gt;

&lt;p&gt;For the purposes of this post, you can assume we’re using either of the above objective functions.&lt;a href=&quot;#f2b&quot; id=&quot;f2t&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt; Throughout this post, we assume that the number of clusters, $k$, is known &lt;em&gt;a priori&lt;/em&gt; — analysis becomes very difficult otherwise.&lt;/p&gt;

&lt;h3 id=&quot;intuition-behind-easy-vs-hard-clustering&quot;&gt;Intuition behind easy vs. hard clustering&lt;/h3&gt;

&lt;p&gt;It is easy to construct datasets where it takes a &lt;em&gt;very long time&lt;/em&gt; to find $\mathcal{C}_{\text{opt}}$. Consider the (schematic) dataset below. The data form an amorphous blob of points that are not easily separated into two clusters.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;A dataset not amenable to clustering.. &lt;/b&gt;Datapoints are shown as open circles, with color representing cluster assignment and &lt;b&gt;x&lt;/b&gt;&#39;s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (&lt;i&gt;left&lt;/i&gt;) and $\mathcal{C}_2$ (&lt;i&gt;middle&lt;/i&gt;) are shown that have a similar loss. There are many local minima in the objective function (&lt;i&gt;right&lt;/i&gt;). &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/c_e_condition_1.png&quot; alt=&quot;Datapoints are shown as open circles, with color representing cluster assignment and &amp;lt;b&amp;gt;x&amp;lt;/b&amp;gt;&#39;s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (&amp;lt;i&amp;gt;left&amp;lt;/i&amp;gt;) and $\mathcal{C}_2$ (&amp;lt;i&amp;gt;middle&amp;lt;/i&amp;gt;) are shown that have a similar loss. There are many local minima in the objective function (&amp;lt;i&amp;gt;right&amp;lt;/i&amp;gt;). &amp;lt;b&amp;gt;&amp;lt;i&amp;gt;Disclaimer: schematic, not real data.&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;&quot; width=&quot;1000px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;A dataset not amenable to clustering.&lt;/b&gt;
	Datapoints are shown as open circles, with color representing cluster assignment and &lt;b&gt;x&lt;/b&gt;&#39;s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (&lt;i&gt;left&lt;/i&gt;) and $\mathcal{C}_2$ (&lt;i&gt;middle&lt;/i&gt;) are shown that have a similar loss. There are many local minima in the objective function (&lt;i&gt;right&lt;/i&gt;). &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In the above dataset we can find many clusterings that are nearly equivalent in terms of the loss function (e.g. $\mathcal{C}_1$ and $\mathcal{C}_2$ in the figure). Thus, if we want to be sure to find the &lt;em&gt;very best&lt;/em&gt; clustering, we need to essentially do a brute force search.&lt;a href=&quot;#f3b&quot; id=&quot;f3t&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt; The &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter” hypothesis&lt;/em&gt; (and common sense) would tell us that it is stupid to do a cluster analysis on this dataset — there simply aren’t any clusters to be found!&lt;/p&gt;

&lt;p&gt;Now compare this to a case where there are, in fact, two clearly separated clusters. In this case, there is really only one clustering that passes the “common sense” test. Assuming we pick a reasonable loss function, there should also be a very obvious global solution (unlike the first example):&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;A dataset that is easily clustered.. &lt;/b&gt;As before, two clusterings are shown. The clustering on the &lt;i&gt;left&lt;/i&gt; is much better in terms of the loss function than the clustering shown in the &lt;i&gt;middle&lt;/i&gt;. &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/c_e_condition_good.png&quot; alt=&quot;As before, two clusterings are shown. The clustering on the &amp;lt;i&amp;gt;left&amp;lt;/i&amp;gt; is much better in terms of the loss function than the clustering shown in the &amp;lt;i&amp;gt;middle&amp;lt;/i&amp;gt;. &amp;lt;b&amp;gt;&amp;lt;i&amp;gt;Disclaimer: schematic, not real data.&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;&quot; width=&quot;1000px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;A dataset that is easily clustered.&lt;/b&gt;
	As before, two clusterings are shown. The clustering on the &lt;i&gt;left&lt;/i&gt; is much better in terms of the loss function than the clustering shown in the &lt;i&gt;middle&lt;/i&gt;. &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Intuitively, it should be easier to find the solution for this second dataset: there is &lt;em&gt;clear winner&lt;/em&gt; for the clustering, so there is an obvious global minimum, with few local minima. Remember, in the first dataset, there were multiple local minima that were &lt;em&gt;nearly as good&lt;/em&gt; as the global minimum.&lt;/p&gt;

&lt;h3 id=&quot;provably-easy-clustering-situations&quot;&gt;Provably “easy” clustering situations&lt;/h3&gt;

&lt;p&gt;We would like to formalize the intuition outlined in the previous section to develop efficient and accurate clustering algorithms. To do this we introduce the concept of &lt;strong&gt;&lt;em&gt;approximation stability&lt;/em&gt;&lt;/strong&gt;, which characterizes how “nice” the error landscape of the optimization problem is. In the schematic figures, the first difficult-to-cluster example is unstable, while the second easy-to-cluster example is stable. &lt;em&gt;The ultimate punchline is that sufficiently stable clustering problems are provably easy to solve.&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Definition:&lt;/strong&gt; $(c,\epsilon)$-approximation-stability.&lt;/p&gt;

  &lt;p&gt;A clustering problem is said to be $(c,\epsilon)$-stable when all clusterings, $\mathcal{C^\prime}$, that satisfy $F(\mathcal{C}^\prime) \leq c F_{opt}$ also satisfy $d(\mathcal{C}^\prime,\mathcal{C}_{\text{opt}}) &amp;lt; \epsilon$. Here, $0 &amp;lt; \epsilon \ll 1$, and $c &amp;gt; 1$, and $d(\cdot,\cdot)$ measures the fraction of differently assigned datapoints between two clusterings.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The more stable the clustering problem is, the larger $c$ and the smaller $\epsilon$ are allowed to be. For example, if $c = 1.1$ and $\epsilon = 0.02$, then a problem is $(c,\epsilon)$-stable if all clusterings within 10% of the optimal objective value, are no more than 2% different from the optimal clustering.&lt;/p&gt;

&lt;p&gt;As cluster stability increases, two things happen:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The problem becomes easier to solve.&lt;/strong&gt; &lt;a href=&quot;http://dx.doi.org/10.1145/2450142.2450144&quot;&gt;Balcan et al. (2013)&lt;/a&gt; provide several algorithms that are guaranteed to find &lt;em&gt;near-optimal&lt;/em&gt; clusterings if the clusters are large enough and the problem is stable enough.&lt;a href=&quot;#f4b&quot; id=&quot;f4t&quot;&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/a&gt; These algorithms are very efficient,&lt;a href=&quot;#f5b&quot; id=&quot;f5t&quot;&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/a&gt; easy to implement, and similar to classic clustering algorithms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cluster analysis becomes more sensible and interpretable.&lt;/strong&gt; While not immediately obvious, it turns out that approximation stability (as well as similar concepts, like &lt;a href=&quot;http://arxiv.org/abs/1112.0826&quot;&gt;&lt;em&gt;perturbation stability&lt;/em&gt;&lt;/a&gt;) correlates with our intuitive sense of clusterability: when the data contain well-separated and compact clusters, then the clustering optimization problem is likely stable. This is outlined in Lemma 3.1 by &lt;a href=&quot;http://dx.doi.org/10.1145/2450142.2450144&quot;&gt;Balcan et al. (2013)&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In short, research along these lines is &lt;a href=&quot;#caveats&quot;&gt;&lt;em&gt;beginning&lt;/em&gt;&lt;/a&gt; to provide rigorous support for the &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter” hypothesis&lt;/em&gt;. To see the proofs associated with this work in detail check out the course materials for &lt;a href=&quot;http://theory.stanford.edu/~tim/f14/f14.html&quot;&gt;this class on “Beyond Worst-Case Analysis”&lt;/a&gt;. A particularly relevant lecture is embedded below (the others are also online):&lt;/p&gt;

&lt;iframe width=&quot;373&quot; height=&quot;210&quot; style=&quot;margin:20px auto; display:block&quot; src=&quot;https://www.youtube.com/embed/n0T0fyRt0Xo&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://cs.uwaterloo.ca/~shai/&quot;&gt;Shai Ben-David&lt;/a&gt; recently published &lt;a href=&quot;http://arxiv.org/abs/1510.05336&quot;&gt;a brief commentary&lt;/a&gt; on the &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter” hypothesis&lt;/em&gt; alongside &lt;a href=&quot;http://arxiv.org/abs/1501.00437&quot;&gt;a more detailed paper&lt;/a&gt;.&lt;a href=&quot;#f6b&quot; id=&quot;f6t&quot;&gt;&lt;sup&gt;[6]&lt;/sup&gt;&lt;/a&gt; He argues that, while the above results (and others) are encouraging, &lt;strong&gt;&lt;em&gt;current theory has only shown clustering to be easy when clusters are very, very obvious&lt;/em&gt;&lt;/strong&gt; in the dataset. For example, Ben-David digs into the specific results of &lt;a href=&quot;http://dx.doi.org/10.1145/2450142.2450144&quot;&gt;Balcan et al. (2013)&lt;/a&gt; and concludes that their (simple, efficient) algorithm indeed produces the correct solution as clustering becomes &lt;em&gt;stable enough&lt;/em&gt;. However, “stable enough” in this case more or less means that &lt;strong&gt;&lt;em&gt;the majority of points sit more than 20 times closer to their true cluster than to any other cluster.&lt;/em&gt;&lt;/strong&gt; This seems like a very strong assumption, which won’t hold for many practical applications.&lt;/p&gt;

&lt;p&gt;There are other caveats to briefly mention.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We have restricted our discussion to center-based clustering frameworks (e.g. $k$-means and $k$-medians). This excludes the possibility of clustering more complicated manifolds. However, I’m not sure how much this matters. It is easy to dream up toy, nonlinear datasets (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction&quot;&gt;the Swiss Roll&lt;/a&gt;) that cause center-based clustering to fail. Are real-world datasets this pathological? &lt;a href=&quot;https://en.wikipedia.org/wiki/Consensus_clustering&quot;&gt;Ensemble clustering&lt;/a&gt; provides a nice way to cluster non-linear manifolds with center-based techniques. Thus, to address this concern, it would be interesting to extend the theoretical results covered in this post to ensemble-based algorithms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Throughout this post we have assumed that the number of clusters is known beforehand. Estimating the number of clusters ($k$) is a well-known and generally unsolved problem.&lt;a href=&quot;#f7b&quot; id=&quot;f7t&quot;&gt;&lt;sup&gt;[7]&lt;/sup&gt;&lt;/a&gt; In practice, we typically run clustering algorithms for various choices of $k$, and compare results in a somewhat &lt;em&gt;ad hoc&lt;/em&gt; manner. For clustering to truly be “easy”, we need simple, consistent, and accurate methods for estimating $k$. While there is some work on this issue (e.g., &lt;a href=&quot;http://statweb.stanford.edu/~gwalther/gap&quot;&gt;Tibshirani et al., 2001&lt;/a&gt;), most of it is constrained to the case of “well-separated” clusters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusions-and-related-work&quot;&gt;Conclusions and related work&lt;/h3&gt;

&lt;p&gt;A theoretical understanding of clustering algorithms is desperately needed, and despite substantial caveats, it seems that we are beginning to make progress. I find the theoretical analysis in this area to be quite interesting and worthy of further work. However, it may be overly optimistic to conclude the &lt;em&gt;“Clustering is only difficult when it does not matter”&lt;/em&gt;. Given current results, it is probably safer to conclude that &lt;em&gt;“Clustering is difficult, except when it isn’t”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The essential flavor of this work is part of a &lt;a href=&quot;http://sunju.org/research/nonconvex/&quot;&gt;growing literature&lt;/a&gt; on finding provably accurate and efficient algorithms to solve problems that were traditionally thought to be difficult (often NP-Hard) to solve. A well-known example in the machine learning community is &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-negative_matrix_factorization&quot;&gt;nonnegative matrix factorization (NMF)&lt;/a&gt; under the “separability condition.” While NMF is NP-hard in general, work by &lt;a href=&quot;http://arxiv.org/abs/1111.0952&quot;&gt;Arora et al. (2012)&lt;/a&gt; showed that it could be solved in polynomial time under certain assumptions (which were typically satisfied or nearly satisfied, in practice). &lt;a href=&quot;http://arxiv.org/abs/1310.7529&quot;&gt;Further&lt;/a&gt; &lt;a href=&quot;http://arxiv.org/abs/1208.1237&quot;&gt;work&lt;/a&gt; by &lt;a href=&quot;https://scholar.google.be/citations?user=pVIJV7wAAAAJ&quot;&gt;Nicolas Gilles&lt;/a&gt; on this problem is worthy of special mention.&lt;/p&gt;

&lt;p&gt;While all of this may seem a bit tangential to the topic of clustering, it really isn’t. One of the reasons NMF is useful is that it produces a sparse representation of a dataset, which can be thought of as an approximate clustering, or &lt;em&gt;soft clustering&lt;/em&gt; of a dataset &lt;a href=&quot;https://smartech.gatech.edu/bitstream/handle/1853/20058/GT-CSE-08-01.pdf?sequence=1&quot;&gt;(Park &amp;amp; Kim, 2008)&lt;/a&gt;. In other words, the very recent and very exciting work on provable NMF algorithms raises the tantalizing possibility that these ideas will soon provide deep insight into clustering.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share this on:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=Clustering is hard, except when it&#39;s not&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;amp;title=Clustering is hard, except when it&#39;s not&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;amp;t=Clustering is hard, except when it&#39;s not&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f1t&quot; id=&quot;f1b&quot;&gt;&lt;b&gt;[1]&lt;/b&gt;&lt;/a&gt; Explicitly, $\mathcal{C} = \{ \mathcal{K}_1,\mathcal{K}_2,…,\mathcal{K}_k\}$, where each $\mathcal{K}_i$ is a set of datapoints, $\mathcal{K}_i = \{\mathbf{x}_1^{(i)},\mathbf{x}_2^{(i)},…\}$, where $\mathbf{x}_j^{(i)}$, is a datapoint (a vector) in cluster $i$, indexed by $j$.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f2t&quot; id=&quot;f2b&quot;&gt;&lt;b&gt;[2]&lt;/b&gt;&lt;/a&gt; It is instructive to prove that the arithmetic mean (i.e. centroid) of a set of points &lt;a href=&quot;http://math.stackexchange.com/questions/967138/formal-proof-that-mean-minimize-squared-error-function&quot;&gt;minimizes the sum-of-squared residuals&lt;/a&gt;. Similarly, the median &lt;a href=&quot;http://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations&quot;&gt;minimizes the sum-of-absolute residuals&lt;/a&gt;.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Speaking loosely, the necessity of this brute-force search makes finding the solution to the k-means optimization problem NP-hard &lt;a href=&quot;https://dx.doi.org/10.1007%2Fs10994-009-5103-0&quot;&gt;(Aloise et al., 2009)&lt;/a&gt;. Note that there are simple and efficient algorithms that find local minima, given an initial guess. However, solving the problem (i.e. finding and certifying that you’ve found the global minimum) is NP-hard in the worst-case.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f4t&quot; id=&quot;f4b&quot;&gt;&lt;b&gt;[4]&lt;/b&gt;&lt;/a&gt; What, exactly, does it mean for a clustering problem to be “stable enough”? This is a very critical question, that is revisited in the &lt;a href=&quot;#caveats&quot;&gt;caveats section&lt;/a&gt;.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f5t&quot; id=&quot;f5b&quot;&gt;&lt;b&gt;[5]&lt;/b&gt;&lt;/a&gt; They run in &lt;a href=&quot;https://en.wikipedia.org/wiki/P_(complexity)&quot;&gt;polynomial time&lt;/a&gt;. Again, clustering problems are NP-hard in the worst case; they (probably) take exponential time to solve (assuming P $\neq$ NP).
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f6t&quot; id=&quot;f6b&quot;&gt;&lt;b&gt;[6]&lt;/b&gt;&lt;/a&gt; I borrowed his phrase — the &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter”&lt;/em&gt; hypothesis — for this post, although it is also a title of a &lt;a href=&quot;http://arxiv.org/abs/1205.4891&quot;&gt;older paper&lt;/a&gt; from a different group.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f7t&quot; id=&quot;f7b&quot;&gt;&lt;b&gt;[7]&lt;/b&gt;&lt;/a&gt; I discussed this point to some extent in &lt;a href=&quot;/itsneuronalblog/2015/09/11/clustering1/&quot;&gt;a previous post&lt;/a&gt;
&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Nov 2015 00:00:00 -0800</pubDate>
        <link>http://alexhwilliams.info//2015/11/18/clustering-is-easy/</link>
        <guid isPermaLink="true">http://alexhwilliams.info//2015/11/18/clustering-is-easy/</guid>
      </item>
    
    
    
      <item>
        <title>Is clustering mathematically impossible?</title>
        <description>&lt;p&gt;In the &lt;a href=&quot;http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&quot;&gt;previous post&lt;/a&gt;, we saw intuitive reasons why clustering is a hard,&lt;a href=&quot;#f1b&quot; id=&quot;f1t&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; and maybe even &lt;em&gt;ill-defined&lt;/em&gt;, problem. In practice, we are often stuck using heuristics that can sometimes perform quite badly when their assumptions are violated (see &lt;a href=&quot;https://en.wikipedia.org/wiki/No_free_lunch_theorem&quot;&gt;&lt;em&gt;No free lunch theorem&lt;/em&gt;&lt;/a&gt;). Is there a mathematical way of expressing all of these difficulties? This post will cover some theoretical results of &lt;a href=&quot;/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf&quot;&gt;Kleinberg (2002)&lt;/a&gt; related to this question.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Notation.&lt;/em&gt;&lt;/strong&gt; Suppose we have a set of $N$ datapoints $x^{(1)}, x^{(2)}, …, x^{(N)}$. A &lt;em&gt;clustering function&lt;/em&gt; produces a &lt;a href=&quot;https://en.wikipedia.org/wiki/Partition_of_a_set&quot;&gt;&lt;em&gt;partition&lt;/em&gt;&lt;/a&gt; (i.e. a set of clusters), based on the pairwise distances between datapoints. The distance between two points $x^{(i)}$ and $x^{(j)}$ is given by $d(x^{(i)},x^{(j)})$, where $d$ is the &lt;em&gt;distance function&lt;/em&gt;. We could choose different ways to measure distance,&lt;a href=&quot;#f2b&quot; id=&quot;f2t&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt; for simplicity you can imagine we are using &lt;a href=&quot;https://en.wikipedia.org/wiki/Euclidean_distance&quot;&gt;Euclidean distance&lt;/a&gt;, $\sqrt{ (x^{(i)}-x^{(j)}) \cdot (x^{(i)}-x^{(j)})}$.&lt;/p&gt;

&lt;h3 id=&quot;an-axiomatic-approach-to-clustering&quot;&gt;An axiomatic approach to clustering&lt;/h3&gt;

&lt;p&gt;There are many possible clustering functions we could come up with. Some are stupid — randomly split the data into two groups — and others are useful in practice. We would like to precisely define what it means for a clustering function to be “useful in practice.”&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf&quot;&gt;Kleinberg (2002)&lt;/a&gt; proposed that the ideal clustering function would achieve three properties: &lt;a href=&quot;/itsneuronalblog/2015/10/01/clustering2/#scale-invariance&quot;&gt;&lt;em&gt;scale-invariance&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;/itsneuronalblog/2015/10/01/clustering2/#consistency&quot;&gt;&lt;em&gt;consistency&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;/itsneuronalblog/2015/10/01/clustering2/#richness&quot;&gt;&lt;em&gt;richness&lt;/em&gt;&lt;/a&gt;. The idea is that these principles should align with your intuitive notion of what a “good clustering function” is:&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;strong&gt;1. Scale-invariance:&lt;/strong&gt; An ideal clustering function does not change its result when the data are scaled equally in all directions.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Scale-invariance.. &lt;/b&gt;For any scalar $\alpha &gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-scale-invariance.png&quot; alt=&quot;For any scalar $\alpha &amp;gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Scale-invariance.&lt;/b&gt;
	For any scalar $\alpha &amp;gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;2. Consistency:&lt;/strong&gt; If we stretch the data so that the distances between clusters increases and/or the distances within clusters decreases, then the clustering shouldn’t change.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Consistency.. &lt;/b&gt;Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the &lt;i&gt;same&lt;/i&gt; cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to &lt;i&gt;different&lt;/i&gt; clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn&#39;t change: $f(d) = f(d^\prime)$&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-consistency.png&quot; alt=&quot;Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the &amp;lt;i&amp;gt;same&amp;lt;/i&amp;gt; cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to &amp;lt;i&amp;gt;different&amp;lt;/i&amp;gt; clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn&#39;t change: $f(d) = f(d^\prime)$&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Consistency.&lt;/b&gt;
	Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the &lt;i&gt;same&lt;/i&gt; cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to &lt;i&gt;different&lt;/i&gt; clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn&#39;t change: $f(d) = f(d^\prime)$
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;3. Richness:&lt;/strong&gt; Suppose a dataset contains $N$ points, but we are not told anything about the distances between points. An ideal clustering function would be flexible enough to produce all possible partition/clusterings of this set. This means that the it automatically determines both the number and proportions of clusters in the dataset. This is shown schemetically below for a set of six datapoints:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Richness.. &lt;/b&gt;For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-richness.png&quot; alt=&quot;For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Richness.&lt;/b&gt;
	For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;kleinbergs-impossibility-theorem&quot;&gt;Kleinberg’s &lt;em&gt;Impossibility Theorem&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf&quot;&gt;Kleinberg’s paper&lt;/a&gt; is a bait-and-switch though. &lt;strong&gt;&lt;em&gt;It turns out that no clustering function can satisfy all three axioms!&lt;/em&gt;&lt;/strong&gt; &lt;a href=&quot;#f3b&quot; id=&quot;f3t&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt; The proof in Kleinberg’s paper is a little terse — A simpler proof is given in &lt;a href=&quot;http://www.cs.fsu.edu/~ackerman/thesisPhD.pdf&quot;&gt;Margareta Ackerman’s thesis&lt;/a&gt;, specifically Theorem 21. The intuition provided there is diagrammed below.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Intuition behind impossibility.. &lt;/b&gt;A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/impossibility-intuition.png&quot; alt=&quot;A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.&quot; width=&quot;350px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Intuition behind impossibility.&lt;/b&gt;
	A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;clustering-functions-that-satisfy-two-of-the-three-axioms&quot;&gt;Clustering functions that satisfy two of the three axioms&lt;/h3&gt;

&lt;p&gt;The above explanation may still be a bit difficult to digest. Another perspective for understanding the impossibility theory is to examine clustering functions that come close to satisfying the three axioms.&lt;/p&gt;

&lt;p&gt;Kleinberg mentions three variants of &lt;a href=&quot;https://en.wikipedia.org/wiki/Single-linkage_clustering&quot;&gt;single-linkage clustering&lt;/a&gt; as an illustration. Single-linkage clustering starts by assigning each point to its own cluster, and then repeatedly fusing together the nearest clusters (where &lt;em&gt;nearest&lt;/em&gt; is measured by our specified distance function). To complete the clustering function we need a &lt;em&gt;stopping condition&lt;/em&gt; — something that tells us when to terminate and return the current set of clusters as our solution. Kleinberg outlines three different stopping conditions, each of which violates one of his three axioms, while satisfying the other two.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. $k$-cluster stopping condition:&lt;/strong&gt; Stop fusing clusters once we have $k$ clusters (where $k$ is some number provided beforehand, similar to the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means algorithm&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This clearly violates the &lt;em&gt;richness&lt;/em&gt; axiom. For example, if we choose $k=3$, then we could never return a result with 2 clusters, 4 clusters, etc. However, it satisfies &lt;em&gt;scale-invariance&lt;/em&gt; and &lt;em&gt;consistency&lt;/em&gt;. To check this, notice that the transformations in the above diagrams above do not change which $k$ clusters are nearest to each other. It is only once we start merging and dividing clusters that we get into trouble.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;$k$-cluster stopping does not satisfy richness. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/k-stopping-violation.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;$k$-cluster stopping does not satisfy richness&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;2. Distance-$r$ stopping condition:&lt;/strong&gt; Stop when the nearest two clusters are farther than a pre-defined distance $r$.&lt;/p&gt;

&lt;p&gt;This satisfies &lt;em&gt;richness&lt;/em&gt; — we can place $N$ points to end up in $N$ clusters by having the minimum distance between any two points to be greater than $r$, we can place $N$ points to end up in one cluster by having the maximum distance be less than $r$, and we can generate all partitions between these extremes.&lt;/p&gt;

&lt;p&gt;It also satisfies &lt;em&gt;consistency&lt;/em&gt;. Shrinking the distances between points in a cluster keeps the maximum distance less than $r$ (our criterion for defining a cluster in the first place). Expanding the distances between points in different clusters keeps the minimum distance greater than $r$. Thus, the clusters remain the same.&lt;/p&gt;

&lt;p&gt;However, &lt;em&gt;scale-invariance&lt;/em&gt; is violated. If we multiply the data by a large enough number, then the $N$ points will be assigned $N$ different clusters (all points are more than distance $r$ from each other). If we multiply the data by a number close to zero, everything ends up in the same cluster.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;distance-$r$ stopping does not satisfy scale-invariance. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/distance-r-violation.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;distance-$r$ stopping does not satisfy scale-invariance&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;3. Scale-$\epsilon$ stopping condition:&lt;/strong&gt; Stop when the nearest two clusters are farther than a fraction of the maximum distance between two points. This is like the distance-$r$ stopping condition, except we choose $r = \epsilon \cdot \Delta$, where $\Delta$ is the maximum distance between any two data points and $\epsilon$ is a number between 0 and 1.&lt;/p&gt;

&lt;p&gt;By adapting $r$ to the scale of the data, this procedure now satisfies &lt;em&gt;scale-invariance&lt;/em&gt; in addition to &lt;em&gt;richness&lt;/em&gt;. However, it &lt;strong&gt;does not&lt;/strong&gt; satisfy &lt;em&gt;consistency&lt;/em&gt;. To see this, consider the following transformation of data, in which one cluster (the green one) is pulled much further away from the other two clusters. This increases the maximum distance between data points, leading us to merge the blue and red clusters into one:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;scale-$\epsilon$ stopping does not satisfy consistency. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-consistency-violation.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;scale-$\epsilon$ stopping does not satisfy consistency&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;sidestepping-impossibility-and-subsequent-work&quot;&gt;Sidestepping impossibility and subsequent work&lt;/h3&gt;

&lt;p&gt;Kleinberg’s analysis outlines what we &lt;strong&gt;should not expect&lt;/strong&gt; clustering algorithms to do for us. It is good not to have unrealistic expectations. But can we circumvent his impossibility theorem, and are his axioms even really desirable?&lt;/p&gt;

&lt;p&gt;The consistency axiom is particularly suspect as illustrated below:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Is consistency a desirable axiom?. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-consistency-problem.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Is consistency a desirable axiom?&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The problem is that our intuitive sense of clustering would probably lead us to merge the two clusters in the lower left corner. This criticism is taken up in &lt;a href=&quot;http://www.cs.fsu.edu/~ackerman/thesisPhD.pdf&quot;&gt;Margareta Ackerman’s thesis&lt;/a&gt;, which I hope to summarize in a future blog post.&lt;/p&gt;

&lt;p&gt;Many clustering algorithms also ignore the &lt;em&gt;richness&lt;/em&gt; axiom by specifying the number of clusters beforehand. For example, we can run $k$-means multiple times with different choices of $k$, allowing us to re-interpret the same dataset at different levels of granularity. &lt;a href=&quot;http://stanford.edu/~rezab/papers/slunique.pdf&quot;&gt;Zadeh &amp;amp; Ben-David (2009)&lt;/a&gt; study a relaxation of the richness axiom, which they call $k$-richness — a desirable clustering function should produce all possible $k$-partitions of a datset (rather than &lt;strong&gt;all&lt;/strong&gt; partitions).&lt;/p&gt;

&lt;p&gt;Overall, Kleinberg’s axiomatic approach provides an interesting perspective on clustering, but his analysis serves more as a starting point, rather than a definitive theoretical characterization of clustering.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share this on:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=Is clustering mathematically impossible?&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;amp;title=Is clustering mathematically impossible?&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;amp;t=Is clustering mathematically impossible?&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f1t&quot; id=&quot;f1b&quot;&gt;&lt;b&gt;[1]&lt;/b&gt;&lt;/a&gt; I am using loose language when I say clustering is a “hard problem.” Similar to the &lt;a href=&quot;http://localhost:4000/itsneuronalblog/2015/09/11/clustering1/&quot;&gt;previous post&lt;/a&gt;, we will be concerned with why clustering is hard on a conceptual/theoretical level. But it is also worth pointing out that clustering is hard on a computational level — it takes a long time to compute a provably optimal solution. For example, &lt;em&gt;k&lt;/em&gt;-means is provably NP-hard for even k=2 clusters &lt;a href=&quot;https://dx.doi.org/10.1007%2Fs10994-009-5103-0&quot;&gt;(Aloise et al., 2009)&lt;/a&gt;. This is because cluster assignment is a discrete variable (a point &lt;em&gt;either&lt;/em&gt; belongs to a cluster or does not); in many cases, discrete optimization problems are more difficult to solve than continuous problems because we can compute the derivatives of the objective function and thus take advantage of gradient-based methods. (However this &lt;a href=&quot;http://cstheory.stackexchange.com/questions/31054/is-it-a-rule-that-discrete-problems-are-np-hard-and-continuous-problems-are-not&quot;&gt;doesn’t entirely account for&lt;/a&gt; the hardness.)
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f2t&quot; id=&quot;f2b&quot;&gt;&lt;b&gt;[2]&lt;/b&gt;&lt;/a&gt; Kleinberg (2002) only requires that the distance be nonnegative and symmetric, $d(x_i,x_j) = d(x_j,x_i)$, and not necessarily satisfy the &lt;a href=&quot;https://en.wikipedia.org/wiki/Triangle_inequality&quot;&gt;triangle inequality&lt;/a&gt;. According to Wikipedia these are called &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_(mathematics)#Semimetrics&quot;&gt;&lt;em&gt;semimetrics&lt;/em&gt;&lt;/a&gt;. There are many other exotic distance functions that fit within this space. For example, we can choose other &lt;a href=&quot;https://en.wikipedia.org/wiki/Norm_(mathematics)&quot;&gt;vector norms&lt;/a&gt; $d(x,y) = ||x -y||$ or information theoretic quantities like &lt;a href=&quot;https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&quot;&gt;&lt;em&gt;Jensen-Shannon divergence&lt;/em&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Interesting side note: the title of Kleinberg’s paper — &lt;em&gt;An Impossibility Theorem for Clustering&lt;/em&gt; — is an homage to &lt;a href=&quot;https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem&quot;&gt;&lt;em&gt;Kenneth Arrow’s impossibility theorem&lt;/em&gt;&lt;/a&gt;, which roughly states that there is no “fair” voting system in which voters rank three or more choices. As in Kleinberg’s approach, “fairness” is defined by three axioms, which cannot be simultaneously satisfied.
&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Oct 2015 00:00:00 -0700</pubDate>
        <link>http://alexhwilliams.info//2015/10/01/clustering2/</link>
        <guid isPermaLink="true">http://alexhwilliams.info//2015/10/01/clustering2/</guid>
      </item>
    
    
    
      <item>
        <title>What is clustering and why is it hard?</title>
        <description>&lt;p&gt;I’ve been working on some clustering techniques to &lt;a href=&quot;http://alexhwilliams.info/pubs/DoE_2015_DNA_meth.compressed.pdf&quot;&gt;identify cell types from DNA methylation data&lt;/a&gt;. When you dive into the literature on clustering, two things becomes immediately apparent: first, clustering is fundamental to many scientific questions, and second, there is &lt;a href=&quot;http://stanford.edu/~rezab/papers/slunique.pdf&quot;&gt;“distressingly little general theory”&lt;/a&gt; on how it works or how to apply it to your particular data.&lt;/p&gt;

&lt;p&gt;This was surprising to me. I imagine that most biologists and neuroscientists come across &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means clustering&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;hierarchical clustering&lt;/a&gt;, and similar techniques all the time in papers related to their work. Given how commonplace these techniques are, one would think that we have a solid handle on how they work and what can go wrong.&lt;/p&gt;

&lt;p&gt;This will be the first post in a short series on clustering techniques. I will try to explain why clustering is hard from a high-level, intuitive perspective. The next post will cover some more technical theoretical results. I’ll focus on &lt;a href=&quot;http://web.stanford.edu/~rezab/classes/cme305/W15/Notes/Kleinberg%20-%20impossibility%20theorem.pdf&quot;&gt;Jon Kleinberg’s paper&lt;/a&gt; which precisely defines an ideal clustering function, but then proves that &lt;strong&gt;&lt;em&gt;no such function exists&lt;/em&gt;&lt;/strong&gt; and that there are inevitable tradeoffs that must be made. The final few posts will cover other theoretical work and some current projects of mine.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;what-is-clustering&quot;&gt;What is clustering?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Clustering&quot;&gt;&lt;em&gt;Clustering&lt;/em&gt;&lt;/a&gt; is difficult because it is an &lt;em&gt;unsupervised learning&lt;/em&gt; problem: we are given a dataset and are asked to infer structure within it (in this case, the latent clusters/categories in the data). The problem is that there isn’t necessarily a “correct” or ground truth solution that we can refer to if we want to check our answers. This is in contrast to &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_classification&quot;&gt;&lt;em&gt;classification problems&lt;/em&gt;&lt;/a&gt;, where we do know the ground truth. Deep artificial neural networks are very good at classification (&lt;a href=&quot;http://bits.blogs.nytimes.com/2014/08/18/computer-eyesight-gets-a-lot-more-accurate/&quot;&gt;&lt;em&gt;NYT article&lt;/em&gt;&lt;/a&gt;; &lt;a href=&quot;http://www.image-net.org/papers/imagenet_cvpr09.pdf&quot;&gt;Deng et al. 2009&lt;/a&gt;), but clustering is still a very open problem.&lt;/p&gt;

&lt;p&gt;For example, it is a classification problem to predict whether or not a patient has a common disease based on a list of symptoms. In this case, we can draw upon past clinical records to make this judgment, and we can gather further data (e.g. a blood test) to confirm our prediction. In other words, we assume there is a self-evident ground truth (the patient either has or does not have disease X) that can be observed.&lt;/p&gt;

&lt;p&gt;For clustering, we lack this critical information. For example, suppose you are given a large number of beetles and told to group them into clusters based on their appearance. Assuming that you aren’t an entomologist, this will involve some judgment calls and guesswork.&lt;sup&gt;[1]&lt;/sup&gt; If you and a friend sort the same 100 beetles into 5 groups, you will likely come up with slightly different answers. And — here’s the important part — there isn’t really a way to determine which one of you is “right”.&lt;/p&gt;

&lt;h3 id=&quot;approaches-for-clustering&quot;&gt;Approaches for clustering&lt;/h3&gt;

&lt;p&gt;There is a lot of material written on this already, so rather than re-hash what’s out there I will just point you to the best resources.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;K-means clustering&lt;/strong&gt;  (&lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;&lt;em&gt;Wikipedia&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;http://www.bytemuse.com/post/k-means-clustering-visualization/&quot;&gt;Visualization by @ChrisPolis&lt;/a&gt;, &lt;a href=&quot;http://tech.nitoyon.com/en/blog/2013/11/07/k-means/&quot;&gt;Visualization by TECH-NI blog&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Hierarchical clustering&lt;/strong&gt; works by starting with each datapoint in its own cluster and fusing the nearest clusters together repeatedly (&lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;&lt;em&gt;Wikipedia&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://youtu.be/XJ3194AmH40&quot;&gt;&lt;em&gt;Youtube #1&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://youtu.be/VMyXc3SiEqs&quot;&gt;&lt;em&gt;Youtube #2&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Single-linkage_clustering&quot;&gt;&lt;strong&gt;Single-linkage clustering&lt;/strong&gt;&lt;/a&gt; is a particularly popular and well-characterized form of hierarchical clustering. Briefly, single-linkage begins by initializing each point as its own cluster, and then repeatedly combining the two closest clusters (as measured by their closest points of approach) until the desired number of clusters is achieved.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Bayesian methods&lt;/strong&gt; include &lt;a href=&quot;http://ifas.jku.at/gruen/BayesMix/bayesmix-intro.pdf&quot;&gt;finite mixture models&lt;/a&gt; and &lt;a href=&quot;http://www.kyb.tue.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2299.pdf&quot;&gt;infinite mixture models&lt;/a&gt;.&lt;sup&gt;[2]&lt;/sup&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The important thing to realize is that all of these approaches are &lt;a href=&quot;http://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf&quot;&gt;very computationally difficult&lt;/a&gt; to solve exactly for large datasets (more on this in my next post). As a result, we often resort to optimization heuristics that may or may not produce reasonable results. And, as we will soon see, even if the results are “reasonable” from the algorithm’s perspective, they might not align with our intuition, prior knowledge, or desired outcome.&lt;/p&gt;

&lt;h3 id=&quot;it-is-difficult-to-determine-the-number-of-clusters-in-a-dataset&quot;&gt;It is difficult to determine the number of clusters in a dataset&lt;/h3&gt;

&lt;p&gt;This has to be the most widely understood problem with clustering. In fact, there is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set&quot;&gt;&lt;em&gt;entire Wikipedia article&lt;/em&gt;&lt;/a&gt; devoted to it. If you think about the problem for long enough, you will come to the inescapable conclusion is that there is no “true” number of clusters (though some numbers &lt;strong&gt;&lt;em&gt;feel&lt;/em&gt;&lt;/strong&gt; better than others), and that the same dataset is appropriately viewed at various levels of granularity depending on analysis goals.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering_ambiguity.png&quot; alt=&quot;&quot; width=&quot;400px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;While this problem cannot be “solved” definitively, there are some nice ways of dealing with it. Hierarchical clustering approaches provide cluster assignments for all possible number of clusters, allowing the analyst or reader to view the data across different levels of granularity. There are also Bayesian approaches such as &lt;a href=&quot;http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/&quot;&gt;Dirichlet Process Mixture Models&lt;/a&gt; that adaptively estimate the number of clusters based on a hyperparameter which tunes dispersion. A number of recent papers have focused on &lt;em&gt;convex clustering&lt;/em&gt; techniques that fuse cluster centroids together in a continuous manner along a regularization path; this exposes a hierarchical structure for a clustering approach (roughly) similar to k-means.&lt;sup&gt;[3]&lt;/sup&gt; Of course, there are many other papers out there on this subject.&lt;sup&gt;[4]&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;it-is-difficult-to-cluster-outliers-even-if-they-form-a-common-group&quot;&gt;It is difficult to cluster outliers (even if they form a common group)&lt;/h3&gt;

&lt;p&gt;I recommend you read David Robinson’s excellent post on &lt;a href=&quot;http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means&quot;&gt;the shortcomings of k-means clustering&lt;/a&gt;. The following example he provides is particularly compelling:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Raw Data.. &lt;/b&gt;Three spherical clusters with variable numbers of elements/points.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering_01.png&quot; alt=&quot;Three spherical clusters with variable numbers of elements/points.&quot; width=&quot;400px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Raw Data.&lt;/b&gt;
	Three spherical clusters with variable numbers of elements/points.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The human eye can pretty easily separate these data into three groups, but the k-means algorithm fails pretty hard:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering_02.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Rather than assigning the points in the upper left corner to their own cluster, the algorithm breaks the largest cluster (in the upper right) into two clusters. In other words it tolerates a few large errors (upper left) in order to decrease the errors where data is particularly dense (upper right). This likely doesn’t align with our analysis, but it is &lt;em&gt;completely reasonable&lt;/em&gt; from the perspective of the algorithm. And again, &lt;strong&gt;&lt;em&gt;there isn’t a ground truth to show that the algorithm is “wrong” per se.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;it-is-difficult-to-cluster-non-spherical-overlapping-data&quot;&gt;It is difficult to cluster non-spherical, overlapping data&lt;/h3&gt;

&lt;p&gt;A final, related problem arises from the shape of the data clusters. Every clustering algorithm makes structural assumptions about the dataset that need to be considered. For example, k-means works by minimizing the total sum-of-squared distance to the cluster centroids. This can produce undesirable results when the clusters are elongated in certain directions — particularly when the between-cluster distance is smaller than the maximum within-cluster distance. Single-linkage clustering, in contrast, can perform well in these cases, since points are clustered together based on their nearest neighbor, which facilitates clustering along ‘paths’ in the dataset.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Datasets where single-linkage outperforms k-means.. &lt;/b&gt;If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C).&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/kmeans_fail.png&quot; alt=&quot;If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C).&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Datasets where single-linkage outperforms k-means.&lt;/b&gt;
	If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C).
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;However, there is &lt;a href=&quot;https://en.wikipedia.org/wiki/No_free_lunch_theorem&quot;&gt;no free lunch&lt;/a&gt;.&lt;sup&gt;[5]&lt;/sup&gt; Single-linkage clustering is more sensitive to noise, because each clustering assignment is based on a single pair of datapoints (the pair with minimal distance). This can cause paths to form between overlapping clouds of points. In contrast, k-means uses a more global calculation — minimizing the distance to the nearest centroid summed over all points. As a result, k-means typically does a better job of identifying partially overlapping clusters.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;A dataset where k-means outperforms single-linkage.. &lt;/b&gt;Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/linkage_fail.png&quot; alt=&quot;Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances.&quot; width=&quot;550px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;A dataset where k-means outperforms single-linkage.&lt;/b&gt;
	Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The above figures were schematically reproduced from &lt;a href=&quot;http://www.stat.cmu.edu/~cshalizi/350/lectures/08/lecture-08.pdf&quot;&gt;these lecture notes&lt;/a&gt; from a statistics course at Carnegie Mellon.&lt;/p&gt;

&lt;h3 id=&quot;are-there-solutions-these-problems&quot;&gt;Are there solutions these problems?&lt;/h3&gt;

&lt;p&gt;This post was meant to highlight the inherent difficulty of clustering rather than propose solutions to these issues. It may therefore come off as a bit pessimistic. There are many heuristics that can help overcome the above issues, but I think it is important to emphasize that these are &lt;strong&gt;&lt;em&gt;only heuristics&lt;/em&gt;&lt;/strong&gt;, not guarantees. While many of biologists treat k-means as an “off the shelf” clustering algorithm, we need to be at least a little careful when we do this.&lt;/p&gt;

&lt;p&gt;One of the more interesting heuristics worth reading up on is called &lt;a href=&quot;http://dx.doi.org/10.1109/ICPR.2002.1047450&quot;&gt;ensemble clustering&lt;/a&gt;. The basic idea is to average the outcomes of several clustering techniques or from the same technique fit from different random initializations. Each clustering fit may suffer from instability, but the average behavior of the ensemble of models will tend to be more desirable. This general trick is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_averaging&quot;&gt;&lt;em&gt;ensemble averaging&lt;/em&gt;&lt;/a&gt; and has been successfully applied to a number of machine learning problems.&lt;sup&gt;[6]&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;This post only provides a quick outline of the typical issues that arise for clustering problems. The details of the algorithms have been purposefully omitted, although a deep understanding of these issues likely requires a closer look at these specifics. &lt;a href=&quot;http://www.cse.msu.edu/biometrics/Publications/Clustering/JainClustering_PRL10.pdf&quot;&gt;Jain (2010)&lt;/a&gt; provides a more comprehensive review that is worth reading.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share this on:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=What is clustering and why is it hard?&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;amp;title=What is clustering and why is it hard?&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;amp;t=What is clustering and why is it hard?&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p&gt;&lt;span class=&quot;footnotes&quot;&gt;
&lt;strong&gt;[1]&lt;/strong&gt; It will probably involve guesswork even if you are.&lt;br /&gt;
&lt;strong&gt;[2]&lt;/strong&gt; Highlighting the difficulty of clustering, &lt;a href=&quot;https://normaldeviate.wordpress.com/2012/08/04/mixture-models-the-twilight-zone-of-statistics/&quot;&gt;Larry Wasserman&lt;/a&gt; has joked that “mixtures, like tequila, are inherently evil and should be avoided at all costs.” &lt;a href=&quot;http://andrewgelman.com/2012/08/15/how-to-think-about-mixture-models/&quot;&gt;Andrew Gelman&lt;/a&gt; is slightly less pessimistic.&lt;br /&gt;
&lt;strong&gt;[3]&lt;/strong&gt; Convex clustering performs continuous clustering, similar to how &lt;a href=&quot;http://statweb.stanford.edu/~tibs/lasso/lasso.pdf&quot;&gt;LASSO&lt;/a&gt; performs continuous variable selection.&lt;br /&gt;
&lt;strong&gt;[4]&lt;/strong&gt; See &lt;a href=&quot;http://dx.doi.org/10.1111/1467-9868.00293&quot;&gt;Tibshirani et al. (2001)&lt;/a&gt;, &lt;a href=&quot;http://dx.doi.org/10.1186/gb-2002-3-7-research0036&quot;&gt;Dudoit &amp;amp; Fridlyand (2002)&lt;/a&gt;, &lt;a href=&quot;http://dx.doi.org/10.1109/34.990138&quot;&gt;Figueiredo &amp;amp; Jain (2002)&lt;/a&gt;, &lt;a href=&quot;http://dx.doi.org/10.1111/j.1541-0420.2007.00784.x&quot;&gt;Yan &amp;amp; Ye (2007)&lt;/a&gt;, &lt;a href=&quot;http://www.cs.berkeley.edu/~jordan/papers/kulis-jordan-icml12.pdf&quot;&gt;Kulis &amp;amp; Jordan (2012)&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;[5]&lt;/strong&gt; The &lt;a href=&quot;http://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf&quot;&gt;“no free lunch” theorem&lt;/a&gt; roughly states that whenever an algorithm performs well on a certain class of problems it is because it makes &lt;em&gt;good assumptions&lt;/em&gt; about those problems; however, you can always construct new problems that violate these assumptions, leading to worse performance. Interestingly, this basic idea pops up in other contexts. For example, certain feedback control systems can be engineered so that they are robust to particular perturbations, but such engineering renders them &lt;strong&gt;&lt;em&gt;more sensitive&lt;/em&gt;&lt;/strong&gt; to other forms of perturbations (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Bode%27s_sensitivity_integral&quot;&gt;“waterbed effect.”&lt;/a&gt;)&lt;br /&gt;
&lt;strong&gt;[6]&lt;/strong&gt; See &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrap_aggregating&quot;&gt;&lt;em&gt;bootstrap aggregation&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;&gt;&lt;em&gt;random forests&lt;/em&gt;&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot;&gt;&lt;em&gt;ensemble learning&lt;/em&gt;&lt;/a&gt;. Seminal work on this topic was done by &lt;a href=&quot;https://en.wikipedia.org/wiki/Leo_Breiman&quot;&gt;Leo Breiman &lt;/a&gt; — his papers are lucid, fascinating, and accessible, and I particularly recommend his 1996 article &lt;a href=&quot;http://dx.doi.org/10.1007/BF00058655&quot;&gt;“Bagging Predictors”&lt;/a&gt;. This is also covered in any modern textbook, such as &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;.
&lt;/span&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 11 Sep 2015 00:00:00 -0700</pubDate>
        <link>http://alexhwilliams.info//2015/09/11/clustering1/</link>
        <guid isPermaLink="true">http://alexhwilliams.info//2015/09/11/clustering1/</guid>
      </item>
    
    
    
      <item>
        <title>Roadmap for this blog</title>
        <description>&lt;p&gt;I’ve been searching for a good way to organize my thoughts/research interests for the past couple of years. This website/blog has gone through a few iterations as a result, but I feel like I’ve finally converged on something that works and is manageable for me to maintain.&lt;/p&gt;

&lt;p&gt;I will update this post periodically as my research evolves and my interests shift. For now, I plan to be writing quick posts roughly once a month on a few different subjects/categories. For now, here is an outline:&lt;/p&gt;

&lt;h2 id=&quot;tutorials&quot;&gt;Tutorials:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;FORCE learning&lt;/li&gt;
  &lt;li&gt;Independent Components Analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;small-stuff-that-im-working-on&quot;&gt;Small stuff that I’m working on:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Optimization and Matrix Factorization Tools in Julia&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stuff-i-want-to-publish-soon&quot;&gt;Stuff I want to publish soon:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Models of mRNA and protein transport in dendrites&lt;/li&gt;
  &lt;li&gt;Clustering algorithms and factor analysis of DNA methylation patterns (and other epigenetic datasets)&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 13 Aug 2015 00:00:00 -0700</pubDate>
        <link>http://alexhwilliams.info//2015/08/13/roadmap/</link>
        <guid isPermaLink="true">http://alexhwilliams.info//2015/08/13/roadmap/</guid>
      </item>
    
    
  </channel>
</rss>


