<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Is backprop necessary to train neural networks? &middot; Its 
 Neuronal
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>
    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">
      <a class="sidebar-nav-item" href="/itsneuronalblog/">Home</a>

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
          
        
      
        
          
        
      

      <h2 style="color:rgba(255,255,255,0.9)">Online Texts</h2>
      <hr style="margin:0.5rem 0">

      <a class="sidebar-nav-item" href="/theory_book" target="_blank">Intro to Comp Neuro</a>
    </nav>

    <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
      <img alt="Creative Commons License" style="margin:0 auto;display:block" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
    </a>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Is backprop necessary to train neural networks?</h1>
  <span class="post-date">30 Dec 2016</span>
  <!-- Look the author details up from the site config. -->
  

  <!-- Output author details if some exist. -->
  
      <div style="height:50px">
          <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

          <small style="vertical-align:middle;">
          Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
          </small>
      </div>
      <br>
  
  <p>Introductory remarks here….</p>

<!--more-->

<h3 id="linear-network">Linear network</h3>

<p>Consider a two-layer linear neural network (perhaps make an illustration?):</p>

<script type="math/tex; mode=display">\mathbf{h} = A \mathbf{x}</script>

<script type="math/tex; mode=display">\mathbf{y} = W \mathbf{h}</script>

<p>We have a target output $\tilde{\mathbf{y}} = T \mathbf{x}$</p>

<p>Using a quadratic loss function, the error is $\mathbf{e} = \frac{1}{2} \big\vert\big\vert \tilde{\mathbf{y}} - \mathbf{y} \big\vert\big\vert^2_2$</p>

<p>We would like to do gradient descent, which means we update $A$ and $W$ according to:</p>

<script type="math/tex; mode=display">\Delta W = -\eta \frac{\partial \mathbf{e}}{\partial W}</script>

<script type="math/tex; mode=display">\Delta A = -\eta \frac{\partial \mathbf{e}}{\partial A}</script>

<p>For $W$ we compute the gradient directly</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial W} \frac{1}{2} \big\vert\big\vert \tilde{\mathbf{y}} - \mathbf{y} \big\vert\big\vert^2_2 = (\tilde{\mathbf{y}} - \mathbf{y}) \frac{\partial}{\partial W} \mathbf{y} = (\tilde{\mathbf{y}} - \mathbf{y}) \mathbf{h}</script>

<h4 id="acknowledgements">Acknowledgements</h4>

<p>Freidamann, Ben</p>

<h4 id="footnotes">Footnotes</h4>

<p class="footnotes">
<a href="#f1t" id="f1b"><b>[1]</b></a> It is instructive to prove that the arithmetic mean (i.e. centroid) of a set of points <a href="http://math.stackexchange.com/questions/967138/formal-proof-that-mean-minimize-squared-error-function">minimizes the sum-of-squared residuals</a>. Similarly, the median <a href="http://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations">minimizes the sum-of-absolute residuals</a>.
</p>
<p class="footnotes">
<a href="#f2t" id="f2b"><b>[2]</b></a> Finding the solution to the k-means optimization problem is known to be NP-hard <a href="https://dx.doi.org/10.1007%2Fs10994-009-5103-0">(Aloise et al., 2009)</a>. Note that there are simple and efficient algorithms that find local minima, given an initial guess. However, solving the problem (i.e. finding and certifying that you’ve found the global minimum) is NP-hard in the worst-case.
</p>

</div>

<!-- <div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/01/10/nonidentifiable-neural-codes/">
            What is a neuron's firing rate? Is it even well-defined?
            <small>10 Jan 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/12/15/nips/">
            Highlights of NIPS2015
            <small>15 Dec 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/11/18/clustering-is-easy/">
            Clustering is hard, except when it's not
            <small>18 Nov 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div> -->


      
      
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'quantneuro';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>


    </div>
   
  </body>
</html>
