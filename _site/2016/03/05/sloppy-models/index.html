<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Sloppy Models &middot; Its 
 Neuronal
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/misc.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">


  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link href="/itsneuronalblog/feed.xml" rel='alternate' type='application/atom+xml'>

</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>

    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
      
        
          
        
      


      <h2 style="color:rgba(255,255,255,0.9)">Other Blogs</h2>
      <hr style="margin:0.5rem 0">
      <a class="sidebar-nav-item" href="https://neuroecology.wordpress.com/">Pillow Lab</a>
      <a class="sidebar-nav-item" href="http://romainbrette.fr/category/blog/">Romain Brette</a>
      <a class="sidebar-nav-item" href="https://memming.wordpress.com/">Memming</a>
      <a class="sidebar-nav-item" href="http://sxcole.com/blog/">Quasiworking Memory</a>
      <a class="sidebar-nav-item" href="https://neuroecology.wordpress.com/">neuroecology</a>


      <a href='http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml'  target='blank'><img id='feedlyFollow' src='http://s3.feedly.com/img/follows/feedly-follow-rectangle-volume-medium_2x.png' style="float:left;margin-top:12px" alt='follow us in feedly' width='71' height='28' ></a>

      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
        <img alt="Creative Commons License" style="float:right;margin-top:12px" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
      </a>

    </nav>

    
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Sloppy Models</h1>
  <span class="post-date">05 Mar 2016</span>
  <!-- Look the author details up from the site config. -->
  

  <!-- Output author details if some exist. -->
  
      <div style="height:50px">
          <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

          <small style="vertical-align:middle;">
          Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
          </small>
      </div>
      <br>
  
  <h1 id="ben-machta">Ben Machta</h1>

<ul>
  <li>Water is not a simple liquid but solute motion is described by the very simple diffusion equation</li>
  <li>Signal cascade model (PC12, NGF) complaints
    <ul>
      <li>Models are way too simple – real biology is high-dimensional (biologists)</li>
      <li>Models are way too complex – prediction space is low-dimensional (physicists)</li>
    </ul>
  </li>
  <li>What is sloppiness?
    <ul>
      <li>Anisotropic parameter uncertainty / ‘hyper-ribbon’ structure</li>
      <li>Anisotropy in Hessian</li>
    </ul>
  </li>
  <li>Simpler model for theory – radioactive decay (least-squares fit)</li>
  <li>Embed possible model parameters into the space of predictions
    <ul>
      <li>hyper-ribbon structure animation</li>
      <li>Why? Because constraining y(t) at some point highly constrains possible behaviors</li>
    </ul>
  </li>
  <li>
    <p>Microscopic models are sloppy after coarsening</p>
  </li>
  <li>Despite constant plasticity, firing rates preserved over days</li>
</ul>

<hr />

<div class="share-page">

    <strong>
    Share:
    </strong>
    
    <a href="https://twitter.com/intent/tweet?text=Sloppy Models&amp;url=http://alexhwilliams.info/itsneuronalblog/2016/03/05/sloppy-models/&amp;via=ItsNeuronal&amp;related=ItsNeuronal" rel="nofollow" target="_blank" title="Share on Twitter" class="btn-social btn-twitter">
        <i class="fa fa-twitter"></i>
    </a>

    <a href="http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2016/03/05/sloppy-models/&amp;title=Sloppy Models" rel="nofollow" target="_blank" title="Share on Reddit" class="btn-social btn-reddit">
        <i class="fa fa-reddit"></i>
    </a>

    <a href="https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2016/03/05/sloppy-models/&amp;t=Sloppy Models" rel="nofollow" target="_blank" title="Share on Facebook" class="btn-social btn-facebook">
        <i class="fa fa-facebook-official"></i>
    </a>

    <a href="https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2016/03/05/sloppy-models/" rel="nofollow" target="_blank" title="Share on Google+" class="btn-social btn-google">
        <i class="fa fa-google-plus"></i>
    </a>

    <strong>
    &nbsp;&nbsp;&nbsp;&nbsp;Follow:
    </strong>

    <a href="http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml" rel="nofollow" target="_blank" title="Follow on Feedly" class="btn-social btn-rss">
        <i class="fa fa-rss"></i>
    </a>

</div>

<h4 id="references">References</h4>

<ul>
  <li>P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2(1):53–58, 1989.</li>
</ul>

<h4 id="footnotes">Footnotes</h4>

<p class="footnotes">
<a href="#f1t" id="f1b"><b>[1]</b></a> There are many interesting remarks to make for the aficionados. Note that the covariance matrix $Q = A^T A$ is a symmetric, <a href="https://en.wikipedia.org/wiki/Positive-definite_matrix">positive semi-definite matrix</a>. This means that $\mathbf{z}^T Q \mathbf{z} \geq 0$ for any vector $\mathbf{z}$, and equivalently that all eigenvalues of $Q$ are nonnegative. PCA maximizes $\mathbf{w}^T Q \mathbf{w}$; the <a href="http://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component">solution to this problem</a> is to set $\mathbf{w}$ to the eigenvector of $Q$ associated with the largest eigenvalue. All <a href="http://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal">symmetric matrices have orthogonal eigenvectors</a>, which is why the principal component vectors are always orthogonal. PCA could be achieved by doing an <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">eigendecomposition</a> of the covariance matrix. <br /><br />Even better, instead of computing $A^T A$ and then an eigendecomposition, one can directly compute the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular-value decomposition</a> directly on the raw data matrix. SVD works for non-square matrices (unlike eigendecomposition) and produces $A = U S V^T$ where $S$ is a diagonal matrix of <a href="https://en.wikipedia.org/wiki/Singular_value">singular values</a> and $U$ and $V$ are <a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrices</a>. Since the transpose of an orthogonal matrix is its inverse, and basic properties of the transpose operator: $A^T A = V S U^T U S V^T = V S S V^T =  V \Lambda V^T$, where $\Lambda$ is just a diagonal matrix of eigenvalues, which are simply the squared singular values in $S$. Thus, doing the SVD on the raw data directly gives you the eigendecomposition of the covariance matrix.
</p>
<p class="footnotes">
<a href="#f2t" id="f2b"><b>[2]</b></a> Drop it into conversation at parties. You’ll sound smart and not at all obnoxious.
</p>
<p>
Check out Appendix A of [Madeleine Udell's thesis](https://courses2.cit.cornell.edu/mru8/doc/udell15_thesis.pdf), which showcases five equivalent formulations of PCA as optimization problems.
</p>

</div>

<!-- <div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/03/27/pca/">
            Everything you did and didn't know about PCA
            <small>27 Mar 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/12/14/nips/">
            Highlights of NIPS2015
            <small>14 Dec 2015</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/11/18/clustering-is-easy/">
            Clustering is hard, except when it's not
            <small>18 Nov 2015</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div> -->


      
      
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'quantneuro';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>


    </div>
   
  </body>
</html>
