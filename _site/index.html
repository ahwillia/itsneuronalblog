<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Its 
 Neuronal &middot; Math and Computation in Neuoscience
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>
    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">
      <a class="sidebar-nav-item" href="/itsneuronalblog/">Home</a>

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
          
        
      
        
          
        
      

      <h2 style="color:rgba(255,255,255,0.9)">Online Texts</h2>
      <hr style="margin:0.5rem 0">

      <a class="sidebar-nav-item" href="/theory_book" target="_blank">Intro to Comp Neuro</a>
    </nav>

    <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
      <img alt="Creative Commons License" style="margin:0 auto;display:block" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
    </a>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/12/01/clustering-is-easy/">
          Clustering is hard, except when it's not
        </a>
      </h1>
      <span class="post-date">01 Dec 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>The previous two posts (<a href="/itsneuronalblog/2015/09/11/clustering1/">part 1</a>, <a href="/itsneuronalblog/2015/09/11/clustering2/">part 2</a>) on clustering have been somewhat depressing and pessimistic. However, the reality is that scientists use simple clustering heuristics <em>all the time</em>, and often find interpretable results. What gives? Is the theoretical hardness of clustering flawed? Or have we just been deluding ourselves? Have we been fooled into believing results that are in some sense fundamentally flawed?</p>

<p>This post will explore a more optimistic possibility, which has been referred to as the <em>“Clustering is Only Difficult When It Does Not Matter” hypothesis</em>. Proponents argue that, while we can construct worst-case scenarios that cause algorithms to fail, clustering techniques work very well in practice because real-world datasets often have characteristic structure that more-or-less guarantees the success of these algorithms. Put differently, <a href="http://arxiv.org/abs/1205.4891">Daniely et al. (2012)</a> say that “clustering is easy, otherwise it is pointless” — whenever clustering fails, it is probably because the data in question were not amenable to clustering in the first place.</p>


      <h4>
        <a href="/itsneuronalblog/2015/12/01/clustering-is-easy/" style="text-decoration:underline">
        Continue Reading About Clustering >>
        </a>
      </h3>
    </div>
    
  
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/10/01/clustering2/">
          Is clustering mathematically impossible?
        </a>
      </h1>
      <span class="post-date">01 Oct 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>In the <a href="http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/">previous post</a>, we saw intuitive reasons why clustering is a hard,<a href="#f1b" id="f1t"><sup>[1]</sup></a> and maybe even <em>ill-defined</em>, problem. In practice, we are often stuck using heuristics that can sometimes perform quite badly when their assumptions are violated (see <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem"><em>No free lunch theorem</em></a>). Is there a mathematical way of expressing all of these difficulties? This post will cover some theoretical results of <a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg (2002)</a> related to this question.</p>

<p><strong><em>Notation.</em></strong> Suppose we have a set of $N$ datapoints $x^{(1)}, x^{(2)}, …, x^{(N)}$. A <em>clustering function</em> produces a <a href="https://en.wikipedia.org/wiki/Partition_of_a_set"><em>partition</em></a> (i.e. a set of clusters), based on the pairwise distances between datapoints. The distance between two points $x^{(i)}$ and $x^{(j)}$ is given by $d(x^{(i)},x^{(j)})$, where $d$ is the <em>distance function</em>. We could choose different ways to measure distance,<a href="#f2b" id="f2t"><sup>[2]</sup></a> for simplicity you can imagine we are using <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>, $\sqrt{ (x^{(i)}-x^{(j)}) \cdot (x^{(i)}-x^{(j)})}$.</p>

<h3 id="an-axiomatic-approach-to-clustering">An axiomatic approach to clustering</h3>

<p>There are many possible clustering functions we could come up with. Some are stupid — randomly split the data into two groups — and others are useful in practice. We would like to precisely define what it means for a clustering function to be “useful in practice.”</p>

<p><a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg (2002)</a> proposed that the ideal clustering function would achieve three properties: <a href="/itsneuronalblog/2015/10/01/clustering2/#scale-invariance"><em>scale-invariance</em></a>, <a href="/itsneuronalblog/2015/10/01/clustering2/#consistency"><em>consistency</em></a>, <a href="/itsneuronalblog/2015/10/01/clustering2/#richness"><em>richness</em></a>. The idea is that these principles should align with your intuitive notion of what a “good clustering function” is:</p>


      <h4>
        <a href="/itsneuronalblog/2015/10/01/clustering2/" style="text-decoration:underline">
        Continue Reading About Clustering >>
        </a>
      </h3>
    </div>
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/09/11/clustering1/">
          What is clustering and why is it hard?
        </a>
      </h1>
      <span class="post-date">11 Sep 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>I’ve been working on some clustering techniques to <a href="http://alexhwilliams.info/pubs/DoE_2015_DNA_meth.compressed.pdf">identify cell types from DNA methylation data</a>. When you dive into the literature on clustering, two things becomes immediately apparent: first, clustering is fundamental to many scientific questions, and second, there is <a href="http://stanford.edu/~rezab/papers/slunique.pdf">“distressingly little general theory”</a> on how it works or how to apply it to your particular data.</p>

<p>This was surprising to me. I imagine that most biologists and neuroscientists come across <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering</a>, <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">hierarchical clustering</a>, and similar techniques all the time in papers related to their work. Given how commonplace these techniques are, one would think that we have a solid handle on how they work and what can go wrong.</p>

<p>This will be the first post in a short series on clustering techniques. I will try to explain why clustering is hard from a high-level, intuitive perspective. The next post will cover some more technical theoretical results. I’ll focus on <a href="http://web.stanford.edu/~rezab/classes/cme305/W15/Notes/Kleinberg%20-%20impossibility%20theorem.pdf">Jon Kleinberg’s paper</a> which precisely defines an ideal clustering function, but then proves that <strong><em>no such function exists</em></strong> and that there are inevitable tradeoffs that must be made. The final few posts will cover other theoretical work and some current projects of mine.</p>


      <h4>
        <a href="/itsneuronalblog/2015/09/11/clustering1/" style="text-decoration:underline">
        Continue Reading About Clustering >>
        </a>
      </h3>
    </div>
    
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/itsneuronalblog/page2">Older</a>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>
      
      

    </div>
   
  </body>
</html>
