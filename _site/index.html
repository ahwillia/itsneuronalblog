<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Its 
 Neuronal &middot; Math and Computation in Neuoscience
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/misc.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">


  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link href="/itsneuronalblog/feed.xml" rel='alternate' type='application/atom+xml'>

</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>

    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
      
        
          
        
      


      <h2 style="color:rgba(255,255,255,0.9)">Other Blogs</h2>
      <hr style="margin:0.5rem 0">
      <a class="sidebar-nav-item" href="https://neuroecology.wordpress.com/">Pillow Lab</a>
      <a class="sidebar-nav-item" href="http://romainbrette.fr/category/blog/">Romain Brette</a>
      <a class="sidebar-nav-item" href="https://memming.wordpress.com/">Memming</a>
      <a class="sidebar-nav-item" href="http://sxcole.com/blog/">Quasiworking Memory</a>
      <a class="sidebar-nav-item" href="https://neuroecology.wordpress.com/">neuroecology</a>


      <a href='http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml'  target='blank'><img id='feedlyFollow' src='http://s3.feedly.com/img/follows/feedly-follow-rectangle-volume-medium_2x.png' style="float:left;margin-top:12px" alt='follow us in feedly' width='71' height='28' ></a>

      <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
        <img alt="Creative Commons License" style="float:right;margin-top:12px" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
      </a>

    </nav>

    
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2016/03/27/pca/">
          Everything you did and didn't know about PCA
        </a>
      </h1>
      <span class="post-date">27 Mar 2016</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p style="color:red">
<b>Note:</b> I am in the process of proofreading this post and making absolutely certain that there are no errors or misleading statements. Small edits may be made over the next week or so. All feedback is very welcome at this stage.
</p>

<p>Many scientists are familiar with organizing and handling data in 2D tables. For example, we might record the mRNA expression level of $p$ genes in $n$ tissue samples. We might store these data in a $n \times p$ matrix, where each row corresponds to a sample, and each column corresponds to a gene. Principle components analysis (PCA) is a standard way to reduce the dimension $p$ (which can be quite large) to something more manageable.</p>

<p>While it is quite common for biologists to apply PCA to their data, it is less common for them to really understand the mechanics and assumptions implicit in this analysis. Opening up the black box on a statistical technique is worthwhile in and of itself, but the real reason I’m motivated to write this is the number of seriously cool and super useful extensions/variations of PCA (e.g., <a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization">Non-negative matrix factorization</a>, <a href="https://en.wikipedia.org/wiki/Sparse_PCA">Sparse PCA</a>, <a href="http://dx.doi.org/10.1137/07070111X">Tensor Decompositions</a>), which have will have a growing impact on modern neuroscience and biology. I want to blog about techniques of this flavor for the next few posts.</p>

<p>If you are completely unfamiliar with PCA, there are <a href="https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf">some</a> <a href="http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf">great</a> <a href="http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html">explanations</a> <a href="http://jeremykun.com/2012/06/28/principal-component-analysis/">online</a> that you should reference concurrently with reading this post. While these materials are quite good, many of them don’t explain PCA in way that naturally lends itself to more complex (but fun and useful!) extensions.</p>

<p>I aimed to be as pedagogical as possible in this post, but you will need to be familiar with some linear algebra to follow along. You don’t need to know what an eigenvalue is (though it will help you understand certain results more deeply), but <a href="https://www.youtube.com/watch?v=kT4Mp9EdVqs">basic matrix operations</a> are needed. Also, go teach yourself what an eigenvalue is, it’s good for you.</p>

<p>If you think you’re already a PCA whiz and don’t care for the background stuff, you can skip to <a href="#some-things-you-maybe-didnt-know-about-pca">some things you maybe didn’t know about PCA</a> or just <a href="#tldr">read the tl;dr</a>.</p>


      <h4>
        <a href="/itsneuronalblog/2016/03/27/pca/" style="text-decoration:underline">
        Continue Reading About Dimensionality Reduction >>
        </a>
      </h3>
    </div>
    
  
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/12/14/nips/">
          Highlights of NIPS2015
        </a>
      </h1>
      <span class="post-date">14 Dec 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>I was warned that NIPS is an overwhelming conference, but I didn’t listen because I’ve gotten used to SfN, which is several times larger. But for what NIPS lacks in size (nearly 4,000 attendees, still no joke) it more than makes up for in it’s energy. It feels like I haven’t talked about anything other than statistics and machine learning for the last 7 days, and I don’t even remember what a good night’s sleep feels like anymore. I’m writing this up on the bus home, physically and emotionally defeated. But my boss told me to consolidate some brief notes from the conference, so here is my attempt.</p>


      <h4>
        <a href="/itsneuronalblog/2015/12/14/nips/" style="text-decoration:underline">
        Continue Reading About NIPS >>
        </a>
      </h3>
    </div>
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/11/18/clustering-is-easy/">
          Clustering is hard, except when it's not
        </a>
      </h1>
      <span class="post-date">18 Nov 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>The previous two posts (<a href="/itsneuronalblog/2015/09/11/clustering1/">part 1</a>, <a href="/itsneuronalblog/2015/10/01/clustering2/">part 2</a>) on clustering have been somewhat depressing and pessimistic. However, the reality is that scientists use simple clustering heuristics <em>all the time</em>, and often find interpretable results. What gives? Is the theoretical hardness of clustering flawed? Or have we just been deluding ourselves? Have we been fooled into believing results that are in some sense fundamentally flawed?</p>

<p>This post will explore a more optimistic possibility, which has been referred to as the <em>“Clustering is Only Difficult When It Does Not Matter” hypothesis</em>. Proponents argue that, while we can construct worst-case scenarios that cause algorithms to fail, clustering techniques work very well in practice because real-world datasets often have characteristic structure that more-or-less guarantees the success of these algorithms. Put differently, <a href="http://arxiv.org/abs/1205.4891">Daniely et al. (2012)</a> say that “clustering is easy, otherwise it is pointless” — whenever clustering fails, it is probably because the data in question were not amenable to clustering in the first place.</p>


      <h4>
        <a href="/itsneuronalblog/2015/11/18/clustering-is-easy/" style="text-decoration:underline">
        Continue Reading About Clustering >>
        </a>
      </h3>
    </div>
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/10/01/clustering2/">
          Is clustering mathematically impossible?
        </a>
      </h1>
      <span class="post-date">01 Oct 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>In the <a href="http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/">previous post</a>, we saw intuitive reasons why clustering is a hard,<a href="#f1b" id="f1t"><sup>[1]</sup></a> and maybe even <em>ill-defined</em>, problem. In practice, we are often stuck using heuristics that can sometimes perform quite badly when their assumptions are violated (see <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem"><em>No free lunch theorem</em></a>). Is there a mathematical way of expressing all of these difficulties? This post will cover some theoretical results of <a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg (2002)</a> related to this question.</p>

<p><strong><em>Notation.</em></strong> Suppose we have a set of $N$ datapoints $x^{(1)}, x^{(2)}, …, x^{(N)}$. A <em>clustering function</em> produces a <a href="https://en.wikipedia.org/wiki/Partition_of_a_set"><em>partition</em></a> (i.e. a set of clusters), based on the pairwise distances between datapoints. The distance between two points $x^{(i)}$ and $x^{(j)}$ is given by $d(x^{(i)},x^{(j)})$, where $d$ is the <em>distance function</em>. We could choose different ways to measure distance,<a href="#f2b" id="f2t"><sup>[2]</sup></a> for simplicity you can imagine we are using <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>, $\sqrt{ (x^{(i)}-x^{(j)}) \cdot (x^{(i)}-x^{(j)})}$.</p>

<h3 id="an-axiomatic-approach-to-clustering">An axiomatic approach to clustering</h3>

<p>There are many possible clustering functions we could come up with. Some are stupid — randomly split the data into two groups — and others are useful in practice. We would like to precisely define what it means for a clustering function to be “useful in practice.”</p>

<p><a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg (2002)</a> proposed that the ideal clustering function would achieve three properties: <a href="/itsneuronalblog/2015/10/01/clustering2/#scale-invariance"><em>scale-invariance</em></a>, <a href="/itsneuronalblog/2015/10/01/clustering2/#consistency"><em>consistency</em></a>, <a href="/itsneuronalblog/2015/10/01/clustering2/#richness"><em>richness</em></a>. The idea is that these principles should align with your intuitive notion of what a “good clustering function” is:</p>


      <h4>
        <a href="/itsneuronalblog/2015/10/01/clustering2/" style="text-decoration:underline">
        Continue Reading About Clustering >>
        </a>
      </h3>
    </div>
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/09/11/clustering1/">
          What is clustering and why is it hard?
        </a>
      </h1>
      <span class="post-date">11 Sep 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>I’ve been working on some clustering techniques to <a href="http://alexhwilliams.info/pubs/DoE_2015_DNA_meth.compressed.pdf">identify cell types from DNA methylation data</a>. When you dive into the literature on clustering, two things becomes immediately apparent: first, clustering is fundamental to many scientific questions, and second, there is <a href="http://stanford.edu/~rezab/papers/slunique.pdf">“distressingly little general theory”</a> on how it works or how to apply it to your particular data.</p>

<p>This was surprising to me. I imagine that most biologists and neuroscientists come across <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering</a>, <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">hierarchical clustering</a>, and similar techniques all the time in papers related to their work. Given how commonplace these techniques are, one would think that we have a solid handle on how they work and what can go wrong.</p>

<p>This will be the first post in a short series on clustering techniques. I will try to explain why clustering is hard from a high-level, intuitive perspective. The next post will cover some more technical theoretical results. I’ll focus on <a href="http://web.stanford.edu/~rezab/classes/cme305/W15/Notes/Kleinberg%20-%20impossibility%20theorem.pdf">Jon Kleinberg’s paper</a> which precisely defines an ideal clustering function, but then proves that <strong><em>no such function exists</em></strong> and that there are inevitable tradeoffs that must be made. The final few posts will cover other theoretical work and some current projects of mine.</p>


      <h4>
        <a href="/itsneuronalblog/2015/09/11/clustering1/" style="text-decoration:underline">
        Continue Reading About Clustering >>
        </a>
      </h3>
    </div>
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/08/13/roadmap/">
          Roadmap for this blog
        </a>
      </h1>
      <span class="post-date">13 Aug 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>I’ve been searching for a good way to organize my thoughts/research interests for the past couple of years. This website/blog has gone through a few iterations as a result, but I feel like I’ve finally converged on something that works and is manageable for me to maintain.</p>

<p>I will update this post periodically as my research evolves and my interests shift. For now, I plan to be writing quick posts roughly once a month on a few different subjects/categories. For now, here is an outline:</p>

<h2 id="tutorials">Tutorials:</h2>
<ul>
  <li>FORCE learning</li>
  <li>Independent Components Analysis</li>
</ul>

<h2 id="small-stuff-that-im-working-on">Small stuff that I’m working on:</h2>
<ul>
  <li>Optimization and Matrix Factorization Tools in Julia</li>
</ul>

<h2 id="stuff-i-want-to-publish-soon">Stuff I want to publish soon:</h2>
<ul>
  <li>Models of mRNA and protein transport in dendrites</li>
  <li>Clustering algorithms and factor analysis of DNA methylation patterns (and other epigenetic datasets)</li>
</ul>


      <h4>
        <a href="/itsneuronalblog/2015/08/13/roadmap/" style="text-decoration:underline">
        Continue Reading About My Plan >>
        </a>
      </h3>
    </div>
    
  
</div>

<div class="pagination">
  
    <span class="pagination-item older">Older</span>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>
      
      

    </div>
   
  </body>
</html>
