<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Its 
 Neuronal &middot; Math and Computation in Neuoscience
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/itsneuronalblog/public/css/poole.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/syntax.css">
  <link rel="stylesheet" href="/itsneuronalblog/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/itsneuronalblog/public/leech.jpg">
                                 <link rel="shortcut icon" href="/itsneuronalblog/public/leech.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40613975-2', 'auto');
  ga('send', 'pageview');
</script>

  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
	  jax: ["input/TeX","input/MathML"],
	  extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js"],
	  TeX: {
	  	equationNumbers: { autoNumber: "AMS" },
	    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
	  }
	});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config(
    	{
        	displayAlign: "center"
       	}
        );
</script>

<script type="text/javascript"
   src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/itsneuronalblog/">
          Its 
 Neuronal
        </a>
      </h1>
      <p class="lead">A blog on math and computation in neuroscience.</p>
    </div>

    <nav class="sidebar-nav">
      <h2 style="color:rgba(255,255,255,0.9)">Blog</h2>
      <hr style="margin:0.5rem 0">
      <a class="sidebar-nav-item" href="/itsneuronalblog/">Home</a>

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/about/">About</a>
          
        
      
        
          
            <a class="sidebar-nav-item" href="/itsneuronalblog/archive/">Archive</a>
          
        
      
        
      
        
          
        
      

      <h2 style="color:rgba(255,255,255,0.9)">Online Texts</h2>
      <hr style="margin:0.5rem 0">

      <a class="sidebar-nav-item" href="/theory_book" target="_blank">Intro to Comp Neuro</a>
    </nav>

    <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.en_US">
      <img alt="Creative Commons License" style="margin:0 auto;display:block" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png">
    </a>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/10/01/clustering2/">
          Is clustering mathematically impossible?
        </a>
      </h1>
      <span class="post-date">01 Oct 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>In the <a href="http://localhost:4000/itsneuronalblog/2015/09/11/clustering1/">previous post</a>, we saw intuitive reasons why clustering is a hard,<a href="#f1b" id="f1t"><sup>[1]</sup></a> and maybe even <em>ill-defined</em>, problem. In practice, we are often stuck using heuristics that can sometimes perform quite badly when their assumptions are violated (see <a href="https://en.wikipedia.org/wiki/No_free_lunch_theorem"><em>No free lunch theorem</em></a>). Is there a mathematical way of expressing all of these difficulties? This post will cover some theoretical results of <a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg (2002)</a> related to this question.</p>

<p><strong><em>Notation.</em></strong> Suppose we have a set of $N$ datapoints $x^{(1)}, x^{(2)}, …, x^{(N)}$. A <em>clustering function</em> produces a <a href="https://en.wikipedia.org/wiki/Partition_of_a_set"><em>partition</em></a> (i.e. a set of clusters), based on the pairwise distances between datapoints. The distance between two points $x^{(i)}$ and $x^{(j)}$ is given by $d(x^{(i)},x^{(j)})$, where $d$ is the <em>distance function</em>. We could choose different ways to measure distance,<a href="#f2b" id="f2t"><sup>[2]</sup></a> for simplicity you can imagine we are using <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>, $\sqrt{ (x^{(i)}-x^{(j)}) \cdot (x^{(i)}-x^{(j)})}$.</p>

<h3 id="an-axiomatic-approach-to-clustering">An axiomatic approach to clustering</h3>

<p>There are many possible clustering functions we could come up with. Some are stupid — randomly split the data into two groups — and others are useful in practice. We would like to precisely define what it means for a clustering function to be “useful in practice.”</p>

<p><a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg (2002)</a> proposed that the ideal clustering function would achieve three properties: <a href="/itsneuronalblog/2015/10/01/clustering2/#scale-invariance"><em>scale-invariance</em></a>, <a href="/itsneuronalblog/2015/10/01/clustering2/#consistency"><em>consistency</em></a>, <a href="/itsneuronalblog/2015/10/01/clustering2/#richness"><em>richness</em></a>. The idea is that these principles should align with your intuitive notion of what a “good clustering function” is:</p>

<p><strong>1. Scale-invariance:</strong> An ideal clustering function does not change its result when the data are scaled equally in all directions.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Scale-invariance.. </b>For any scalar $\alpha > 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-scale-invariance.png" alt="For any scalar $\alpha &gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$." width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Scale-invariance.</b>
	For any scalar $\alpha &gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.
</p></td></tr>
</table>

<p><strong>2. Consistency:</strong> If we stretch the data so that the distances between clusters increases and/or the distances within clusters decreases, then the clustering shouldn’t change.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Consistency.. </b>Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the <i>same</i> cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to <i>different</i> clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn't change: $f(d) = f(d^\prime)$</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-consistency.png" alt="Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the &lt;i&gt;same&lt;/i&gt; cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to &lt;i&gt;different&lt;/i&gt; clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn't change: $f(d) = f(d^\prime)$" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Consistency.</b>
	Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the <i>same</i> cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to <i>different</i> clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn't change: $f(d) = f(d^\prime)$
</p></td></tr>
</table>

<p><strong>3. Richness:</strong> Suppose a dataset contains $N$ points, but we are not told anything about the distances between points. An ideal clustering function would be flexible enough to produce all possible partition/clusterings of this set. This means that the it automatically determines both the number and proportions of clusters in the dataset. This is shown schemetically below for a set of six datapoints:</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Richness.. </b>For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-richness.png" alt="For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$." width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Richness.</b>
	For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.
</p></td></tr>
</table>

<h3 id="kleinbergs-impossibility-theorem">Kleinberg’s <em>Impossibility Theorem</em></h3>

<p><a href="/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf">Kleinberg’s paper</a> is a bait-and-switch though. <strong><em>It turns out that no clustering function can satisfy all three axioms!</em></strong> <a href="#f3b" id="f3t"><sup>[3]</sup></a> The proof in Kleinberg’s paper is a little terse — A simpler proof is given in <a href="http://www.cs.fsu.edu/~ackerman/thesisPhD.pdf">Margareta Ackerman’s thesis</a>, specifically Theorem 21. The intuition provided there is diagrammed below.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Intuition behind impossibility.. </b>A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom right), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.</caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/impossibility-intuition.png" alt="A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom right), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation." width="350px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Intuition behind impossibility.</b>
	A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom right), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.
</p></td></tr>
</table>

<h3 id="clustering-functions-that-satisfy-two-of-the-three-axioms">Clustering functions that satisfy two of the three axioms</h3>

<p>The above explanation may still be a bit difficult to digest. Another perspective for understanding the impossibility theory is to examine clustering functions that come close to satisfying the three axioms.</p>

<p>Kleinberg mentions three variants of <a href="https://en.wikipedia.org/wiki/Single-linkage_clustering">single-linkage clustering</a> as an illustration. Single-linkage clustering starts by assigning each point to its own cluster, and then repeatedly fusing together the nearest clusters (where <em>nearest</em> is measured by our specified distance function). To complete the clustering function we need a <em>stopping condition</em> — something that tells us when to terminate and return the current set of clusters as our solution. Kleinberg outlines three different stopping conditions, each of which violates one of his three axioms, while satisfying the other two.</p>

<p><strong>1. $k$-cluster stopping condition:</strong> Stop fusing clusters once we have $k$ clusters (where $k$ is some number provided beforehand, similar to the <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means algorithm</a>).</p>

<p>This clearly violates the <em>richness</em> axiom. For example, if we choose $k=3$, then we could never return a result with 2 clusters, 4 clusters, etc. However, it satisfies <em>scale-invariance</em> and <em>consistency</em>. To check this, notice that the transformations in the above diagrams above do not change which $k$ clusters are nearest to each other. It is only once we start merging and dividing clusters that we get into trouble.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>$k$-cluster stopping does not satisfy richness. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/k-stopping-violation.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>$k$-cluster stopping does not satisfy richness</b>
	
</p></td></tr>
</table>

<p><strong>2. Distance-$r$ stopping condition:</strong> Stop when the nearest two clusters are farther than a pre-defined distance $r$.</p>

<p>This satisfies <em>richness</em> — we can place $N$ points to end up in $N$ clusters by having the minimum distance between any two points to be greater than $r$, we can place $N$ points to end up in one cluster by having the maximum distance be less than $r$, and we can generate all partitions between these extremes.</p>

<p>It also satisfies <em>consistency</em>. Shrinking the distances between points in a cluster keeps the maximum distance less than $r$ (our criterion for defining a cluster in the first place). Expanding the distances between points in different clusters keeps the minimum distance greater than $r$. Thus, the clusters remain the same.</p>

<p>However, <em>scale-invariance</em> is violated. If we multiply the data by a large enough number, then the $N$ points will be assigned $N$ different clusters (all points are more than distance $r$ from each other). If we multiply the data by a number close to zero, everything ends up in the same cluster.</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>distance-$r$ stopping does not satisfy scale-invariance. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/distance-r-violation.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>distance-$r$ stopping does not satisfy scale-invariance</b>
	
</p></td></tr>
</table>

<p><strong>3. Scale-$\epsilon$ stopping condition:</strong> Stop when the nearest two clusters are farther than a fraction of the maximum distance between two points. This is like the distance-$r$ stopping condition, except we choose $r = \epsilon \cdot \Delta$, where $\Delta$ is the maximum distance between any two data points and $\epsilon$ is a number between 0 and 1.</p>

<p>By adapting $r$ to the scale of the data, this procedure now satisfies <em>scale-invariance</em> in addition to <em>richness</em>. However, it <strong>does not</strong> satisfy <em>consistency</em>. To see this, consider the following transformation of data, in which one cluster (the green one) is pulled much further away from the other two clusters. This increases the maximum distance between data points, leading us to merge the blue and red clusters into one:</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>scale-$\epsilon$ stopping does not satisfy consistency. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-consistency-violation.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>scale-$\epsilon$ stopping does not satisfy consistency</b>
	
</p></td></tr>
</table>

<h3 id="sidestepping-impossibility-and-subsequent-work">Sidestepping impossibility and subsequent work</h3>

<p>Kleinberg’s analysis outlines what we <strong>should not expect</strong> clustering algorithms to do for us. It is good not to have unrealistic expectations. But can we circumvent his impossibility theorem, and are his axioms even really desirable?</p>

<p>The consistency axiom is particularly suspect as illustrated below:</p>

<!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 -->
<table class="image">
<!-- <caption align="bottom"><b>Is consistency a desirable axiom?. </b></caption> -->
<tr><td><img src="/itsneuronalblog/img/clustering/clustering-consistency-problem.png" alt="" width="500px" /></td></tr>
<tr><td><p style="text-align:center; padding:0 50px; font-size:15px;">
	<b>Is consistency a desirable axiom?</b>
	
</p></td></tr>
</table>

<p>The problem is that our intuitive sense of clustering would probably lead us to merge the two clusters in the lower left corner. This criticism is taken up in <a href="http://www.cs.fsu.edu/~ackerman/thesisPhD.pdf">Margareta Ackerman’s thesis</a>, which I hope to summarize in a future blog post.</p>

<p>Many clustering algorithms also ignore the <em>richness</em> axiom by specifying the number of clusters beforehand. For example, we can run $k$-means multiple times with different choices of $k$, allowing us to re-interpret the same dataset at different levels of granularity. <a href="http://stanford.edu/~rezab/papers/slunique.pdf">Zadeh &amp; Ben-David (2009)</a> study a relaxation of the richness axiom, which they call $k$-richness — a desirable clustering function should produce all possible $k$-partitions of a datset (rather than <strong>all</strong> partitions).</p>

<p>Overall, Kleinberg’s axiomatic approach provides an interesting perspective on clustering, but his analysis serves more as a starting point, rather than a definitive theoretical characterization of clustering.</p>

<hr />

<div class="share-page">
    <strong>
    Share this on &rarr; 
    &nbsp;&nbsp;
    <a href="https://twitter.com/intent/tweet?text=Is clustering mathematically impossible?&amp;url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;via=ItsNeuronal&amp;related=ItsNeuronal" rel="nofollow" target="_blank" title="Share on Twitter">Twitter</a>
    &nbsp;&nbsp;
    <a href="https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;t=Is clustering mathematically impossible?" rel="nofollow" target="_blank" title="Share on Facebook">Facebook</a>
    &nbsp;&nbsp;
    <a href="https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/" rel="nofollow" target="_blank" title="Share on Google+">Google+</a>
	&nbsp;&nbsp;
	<a href="http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;title=Is clustering mathematically impossible?" rel="nofollow" target="_blank" title="Share on Google+">Reddit</a>
    </strong>
</div>

<h4 id="footnotes">Footnotes</h4>

<p class="footnotes">
<a href="#f1t" id="f1b"><b>[1]</b></a> I am using loose language when I say clustering is a “hard problem.” Similar to the <a href="http://localhost:4000/itsneuronalblog/2015/09/11/clustering1/">previous post</a>, we will be concerned with why clustering is hard on a conceptual/theoretical level. But it is also worth pointing out that clustering is hard on a computational level — it takes a long time to compute a provably optimal solution. For example, <em>k</em>-means is provably NP-hard for even k=2 clusters <a href="https://dx.doi.org/10.1007%2Fs10994-009-5103-0">(Aloise et al., 2009)</a>. This is because cluster assignment is a discrete variable (a point <em>either</em> belongs to a cluster or does not); in many cases, discrete optimization problems are more difficult to solve than continuous problems because we can compute the derivatives of the objective function and thus take advantage of gradient-based methods. (However this <a href="http://cstheory.stackexchange.com/questions/31054/is-it-a-rule-that-discrete-problems-are-np-hard-and-continuous-problems-are-not">doesn’t entirely account for</a> the hardness.)
</p>
<p class="footnotes">
<a href="#f2t" id="f2b"><b>[2]</b></a> Kleinberg (2002) only requires that the distance be nonnegative and symmetric, $d(x_i,x_j) = d(x_j,x_i)$, and not necessarily satisfy the <a href="https://en.wikipedia.org/wiki/Triangle_inequality">triangle inequality</a>. According to Wikipedia these are called <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)#Semimetrics"><em>semimetrics</em></a>. There are many other exotic distance functions that fit within this space. For example, we can choose other <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">vector norms</a> $d(x,y) = ||x -y||$ or information theoretic quantities like <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence"><em>Jensen-Shannon divergence</em></a>.
</p>
<p class="footnotes">
<a href="#f3t" id="f3b"><b>[3]</b></a> Interesting side note: the title of Kleinberg’s paper — <em>An Impossibility Theorem for Clustering</em> — is an homage to <a href="https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem"><em>Kenneth Arrow’s impossibility theorem</em></a>, which roughly states that there is no “fair” voting system in which voters rank three or more choices. As in Kleinberg’s approach, “fairness” is defined by three axioms, which cannot be simultaneously satisfied.
</p>


      <h4>
        <a href="/itsneuronalblog/2015/10/01/clustering2/" style="text-decoration:underline">
        Continue Reading About Clustering >>
        </a>
      </h3>
    </div>
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/09/11/clustering1/">
          What is clustering and why is it hard?
        </a>
      </h1>
      <span class="post-date">11 Sep 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>I’ve been working on some clustering techniques to <a href="http://alexhwilliams.info/pubs/DoE_2015_DNA_meth.compressed.pdf">identify cell types from DNA methylation data</a>. When you dive into the literature on clustering, two things becomes immediately apparent: first, clustering is fundamental to many scientific questions, and second, there is <a href="http://stanford.edu/~rezab/papers/slunique.pdf">“distressingly little general theory”</a> on how it works or how to apply it to your particular data.</p>

<p>This was surprising to me. I imagine that most biologists and neuroscientists come across <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering</a>, <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">hierarchical clustering</a>, and similar techniques all the time in papers related to their work. Given how commonplace these techniques are, one would think that we have a solid handle on how they work and what can go wrong.</p>

<p>This will be the first post in a short series on clustering techniques. I will try to explain why clustering is hard from a high-level, intuitive perspective. The next post will cover some more technical theoretical results. I’ll focus on <a href="http://web.stanford.edu/~rezab/classes/cme305/W15/Notes/Kleinberg%20-%20impossibility%20theorem.pdf">Jon Kleinberg’s paper</a> which precisely defines an ideal clustering function, but then proves that <strong><em>no such function exists</em></strong> and that there are inevitable tradeoffs that must be made. The final few posts will cover other theoretical work and some current projects of mine.</p>


      <h4>
        <a href="/itsneuronalblog/2015/09/11/clustering1/" style="text-decoration:underline">
        Continue Reading About Clustering >>
        </a>
      </h3>
    </div>
    
  
    
    <div class="post">
      <h1 class="post-title">
        <a href="/itsneuronalblog/2015/08/13/roadmap/">
          Roadmap for this blog
        </a>
      </h1>
      <span class="post-date">13 Aug 2015</span>
      
      
        <div style="height:50px">
            <img src="http://gravatar.com/avatar/ef6e49b7c42ff02f2953881bc462267c" height=50 style="float:left; margin-right:20px">

            <small style="vertical-align:middle;">
            Contributed by <a href="http://alexhwilliams.info" target="_blank">Alex Williams</a>
            </small>
        </div>
        <br>
      

      <p>I’ve been searching for a good way to organize my thoughts/research interests for the past couple of years. This website/blog has gone through a few iterations as a result, but I feel like I’ve finally converged on something that works and is manageable for me to maintain.</p>

<p>I will update this post periodically as my research evolves and my interests shift. For now, I plan to be writing quick posts roughly once a month on a few different subjects/categories. For now, here is an outline:</p>

<h2 id="tutorials">Tutorials:</h2>
<ul>
  <li>FORCE learning</li>
  <li>Independent Components Analysis</li>
</ul>

<h2 id="small-stuff-that-im-working-on">Small stuff that I’m working on:</h2>
<ul>
  <li>Optimization and Matrix Factorization Tools in Julia</li>
</ul>

<h2 id="stuff-i-want-to-publish-soon">Stuff I want to publish soon:</h2>
<ul>
  <li>Models of mRNA and protein transport in dendrites</li>
  <li>Clustering algorithms and factor analysis of DNA methylation patterns (and other epigenetic datasets)</li>
</ul>


      <h4>
        <a href="/itsneuronalblog/2015/08/13/roadmap/" style="text-decoration:underline">
        Continue Reading About My Plan >>
        </a>
      </h3>
    </div>
    
  
</div>

<div class="pagination">
  
    <span class="pagination-item older">Older</span>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>
      
      

    </div>
   
  </body>
</html>
