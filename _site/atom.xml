<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Its 
 Neuronal</title>
 <link href="http://alexhwilliams.info/atom.xml" rel="self"/>
 <link href="http://alexhwilliams.info/"/>
 <updated>2018-02-26T15:04:49-08:00</updated>
 <id>http://alexhwilliams.info</id>
 <author>
   <name>Alex Williams</name>
   <email></email>
 </author>

 
 <entry>
   <title>How to cross-validate PCA, clustering, and matrix decomposition models</title>
   <link href="http://alexhwilliams.info/2018/02/26/crossval/"/>
   <updated>2018-02-26T00:00:00-08:00</updated>
   <id>http://alexhwilliams.info/2018/02/26/crossval</id>
   <content type="html">&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; I cover how cross-validation is a somewhat tricky problem for matrix factorization models (including PCA &amp;amp; clustering as special cases) and provide some Python code snippets for fitting these models with held out data.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;cross-validation-in-linear-regression&quot;&gt;Cross-validation in Linear Regression&lt;/h2&gt;

&lt;p&gt;Cross-validation is a fundamental paradigm in modern data analysis. However, it is largely applied to supervised settings, such as regression and classification. Here, the procedure is simple: fit your model on, say, 90% of the data (the training set), and evaluate its performance on the remaining 10% (the test set). However, this idea does not easily extend to other unsupervised methods, such as dimensionality reduction methods or clustering.&lt;/p&gt;

&lt;p&gt;It is easiest to see why visually. Take a simple linear regression problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\underset{\mathbf{x}}{\text{minimize}} \quad \left \lVert \mathbf{A} \mathbf{x} - \mathbf{b} \right \lVert^2
\end{equation}&lt;/script&gt;

&lt;p&gt;Here, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}&lt;/script&gt; is a &lt;script type=&quot;math/tex&quot;&gt;m \times n&lt;/script&gt; matrix, $\mathbf{x}$ is vector with $n$ elements, &lt;script type=&quot;math/tex&quot;&gt;\mathbf{b}&lt;/script&gt; is a vector containing $m$ elements. This model has $n$ parameters (the elements of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{b}&lt;/script&gt;) and &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; datapoints. Rows of $\mathbf{A}$ correspond to independent/predictor variables, and elements of &lt;script type=&quot;math/tex&quot;&gt;\mathbf{b}&lt;/script&gt; correspond to dependent variables.&lt;/p&gt;

&lt;p&gt;The basic idea behind cross-validation (and related techniques, like bootstrapping) is to leave out datapoints and quantify the effect on the model. We can leave out rows of $\mathbf{A}$ and corresponding elements of $\mathbf{b}$. Critically, this leaves the length of $\mathbf{x}$ unchanged — there are still $n$ variables to predict, but the number of datapoints $m$ is smaller.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Procedure to hold out data for linear regression.. &lt;/b&gt;Note that $\mathbf{x}$ does not change in length.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/lstsq.png&quot; alt=&quot;Note that $\mathbf{x}$ does not change in length.&quot; width=&quot;600px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Procedure to hold out data for linear regression.&lt;/b&gt;
	Note that $\mathbf{x}$ does not change in length.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To be explicit: let $\mathbf{A}_\text{tr}$ and $\mathbf{b}_\text{tr}$ respectively denote the &lt;em&gt;training set&lt;/em&gt; of independent and dependent variables. Then fitting our model amounts to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{x}} = \underset{\mathbf{x}}{\text{argmin}} \quad \left \lVert \mathbf{A}_\text{tr} \mathbf{x} - \mathbf{b}_\text{tr} \right \lVert^2&lt;/script&gt;

&lt;p&gt;The beauty is that this reduces to the same optimization problem we started with (see equation 1). Furthermore, we know how to solve this particular problem very efficiently (see &lt;a href=&quot;https://www.youtube.com/watch?v=ZKhZclqfR5E&quot;&gt;&lt;em&gt;least-squares&lt;/em&gt;&lt;/a&gt;). After fitting the model, we can evaluate its performance on the held-out datapoints (i.e. the &lt;em&gt;test set&lt;/em&gt;), which we denote $\mathbf{A}_\text{te}$ and $\mathbf{b}_\text{te}$. In the end we obtain an estimate of the generalization error of our model as $\lVert \mathbf{A}_\text{te} \hat{\mathbf{x}} - \mathbf{b}_\text{te} \lVert^2$.&lt;/p&gt;

&lt;p&gt;This procedure is easily adapted to nonlinear regression models, of course.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; there are subtleties about when and whether this estimate of generalization error is good — if you use cross-validation for model comparison (described below in a bit), then you probably want to split your data into three sets: training set / validation set / test set. If you select your model based on the test set, then you will generally underestimate your generalization error. You will underestimate it &lt;em&gt;by more&lt;/em&gt; the more models you test.&lt;/p&gt;

&lt;h2 id=&quot;cross-validation-in-pca&quot;&gt;Cross-validation in PCA&lt;/h2&gt;

&lt;p&gt;So what’s the problem with cross-validating PCA? I like to think of PCA as the following optimization problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\underset{\mathbf{U}, \mathbf{V}}{\text{minimize}} \quad \left \lVert \mathbf{U} \mathbf{V}^T - \mathbf{Y} \right \lVert^2
\end{equation}&lt;/script&gt;

&lt;p&gt;This is much like linear regression, except we are optimizing over both $\mathbf{A}$ and $\mathbf{X}$ (which have been renamed to $\mathbf{U}$ and $\mathbf{V}^T$ to avoid confusion between the two models). Here, $\mathbf{Y}$ is a $m \times n$ matrix of data. In large-scale studies both $m$ and $n$ can be very high-dimensional and we may seek a simple low-dimensional linear model. This model is captured by $\mathbf{U}$ which is a tall-skinny matrix and $\mathbf{V}^T$ which is a short-fat matrix. Concretely, lets define $\mathbf{U}$ to me an $m \times r$ matrix and define $\mathbf{V}^T$ to be a $r \times n$ matrix, and choose $r$ to be less than $m$ and $n$. Then our model estimate $\hat{\mathbf{Y}} = \mathbf{U} \mathbf{V}^T$ is a rank-$r$ matrix&lt;/p&gt;

&lt;p&gt;Some of you are probably grumpy that the above problem is not “really PCA” because PCA places an additional orthogonality constraint on the columns of $\mathbf{U}$ and $\mathbf{V}$. For a more careful discussion about the connection between PCA and equation 2 see &lt;a href=&quot;http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/&quot;&gt;my other post on PCA&lt;/a&gt;, &lt;a href=&quot;https://cbmm.mit.edu/video/dimensionality-reduction-matrix-and-tensor-coded-data-part-1&quot;&gt;this talk/tutorial I gave&lt;/a&gt;, and Madeleine Udell’s &lt;a href=&quot;https://people.orie.cornell.edu/mru8/doc/udell16_glrm.pdf&quot;&gt;thesis work&lt;/a&gt;. Those resources also explain how nonnegative matrix factorization, clustering, and many other unsupervised models are also closely related to equation 2. Thus, our discussion of cross-validation will quickly generalize to these other very interesting models.&lt;/p&gt;

&lt;p&gt;Once you are on board with the matrix factorization framework, you’ll see why cross-validation is tricky. Ultimately, our problem now looks like this:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Matrix Factorization Model.. &lt;/b&gt;In keeping with figure 1, I&#39;ve colored the model parameters (here, $\mathbf{U}$ and $\mathbf{V}$) in orange, while the data (here, $\mathbf{Y}$) is in blue. We optimize over $\mathbf{U}$ and $\mathbf{V}$ to minimize reconstruction error with respect to $\mathbf{Y}$&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/matrixfac.png&quot; alt=&quot;In keeping with figure 1, I&#39;ve colored the model parameters (here, $\mathbf{U}$ and $\mathbf{V}$) in orange, while the data (here, $\mathbf{Y}$) is in blue. We optimize over $\mathbf{U}$ and $\mathbf{V}$ to minimize reconstruction error with respect to $\mathbf{Y}$&quot; width=&quot;600px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Matrix Factorization Model.&lt;/b&gt;
	In keeping with figure 1, I&#39;ve colored the model parameters (here, $\mathbf{U}$ and $\mathbf{V}$) in orange, while the data (here, $\mathbf{Y}$) is in blue. We optimize over $\mathbf{U}$ and $\mathbf{V}$ to minimize reconstruction error with respect to $\mathbf{Y}$
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Ok, so how exactly should we hold out data in this setting? You might think we could leave out rows of $\mathbf{Y}$, however this would mean that you would have to leave out the corresponding row of $\mathbf{U}$. Thus, you couldn’t fit all of your model parameters! Likewise, leaving out a column of $\mathbf{A}$ would mean that you’d have to leave out a column of $\mathbf{V}^T$.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Not so great ideas for cross-validating matrix factorization.. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/holdout_naive.png&quot; alt=&quot;&quot; width=&quot;700px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Not so great ideas for cross-validating matrix factorization.&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Maybe you only care about evaluating held-out/test error on our estimate of $\mathbf{V}$. That is, you don’t care about cross-validating $\mathbf{U}$ — you &lt;em&gt;only&lt;/em&gt; care about cross-validating $\mathbf{V}$. Thus, you might suggest a two-step procedure in which we leave out rows of $\mathbf{Y}$ and fit $\mathbf{V}$ along with a subset of the rows of $\mathbf{U}$ on this training set. Then, to evaluate the performance of $\mathbf{V}$, we bend the rules of cross-validation ever so slightly and use the test set (held out rows) to fill out our estimate of $\mathbf{U}$.&lt;/p&gt;

&lt;p&gt;Even this is not a good idea! It violates a core tenet of cross-validation that you don’t get to touch the the held out data. Your estimate of $\mathbf{V}$ could be horribly overfit, but by fitting $\mathbf{U}$ to on the test data, you could set these rows to zero and essentially blunt the effects of overfitting. Note that we &lt;em&gt;have to&lt;/em&gt; fit this held out row of $\mathbf{U}$ in order to evaluate model performance along the corresponding row in the data matrix $\mathbf{Y}$ (which is the whole point of cross-validation).&lt;/p&gt;

&lt;p&gt;Problems with holding out a whole column (or row) of the data matrix are discussed in more detail by &lt;a href=&quot;https://doi.org/10.1007/s00216-007-1790-1&quot;&gt;Bro et al. (2008)&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/0908.2062&quot;&gt;Owen &amp;amp; Perry (2009)&lt;/a&gt;. (Just in case you don’t believe me.)&lt;/p&gt;

&lt;p&gt;In a moment, I’ll describe a very simple cross-validation procedure that draws a connection between matrix factorization models and the well-studied &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_completion&quot;&gt;matrix completion&lt;/a&gt; problem. But I should mention at the outset that there is much more detailed published work on this topic, which I touch on briefly &lt;a href=&quot;#conclusions-and-references&quot;&gt;at the end&lt;/a&gt;. Some of these articles discuss ways of leaving out entire rows and columns of the data matrix, but those procedures require a bit more care than what we will focus on.&lt;/p&gt;

&lt;h2 id=&quot;why-cross-validate-pca-and-related-methods&quot;&gt;Why cross-validate PCA (and related methods)?&lt;/h2&gt;

&lt;p&gt;Before jumping into a solution, let’s remind ourselves why cross-validation is great and why it would be great to apply it to PCA. In the case of regression and other supervised learning techniques, the goal of cross-validation is to monitor overfitting and calibrate hyperparameters. If we have a large number of regression parameters (i.e., $\mathbf{x}$ in equation 1 is a very long vector) then we’d commonly add regularization to the model. To take a very simple example, consider &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot;&gt;LASSO regression&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{\mathbf{x}}{\text{minimize}} \quad \left \lVert \mathbf{A} \mathbf{x} - \mathbf{b} \right \lVert^2 + \lambda \lVert \mathbf{x} \lVert_1&lt;/script&gt;

&lt;p&gt;which will tend to produce an parameter estimate, $\hat{\mathbf{x}}$, that is sparse (containing many zeros). The scalar hyperparameter $\lambda &amp;gt; 0$ tunes how sparse our parameter estimate will be. If $\lambda$ is too large, then $\hat{\mathbf{x}}$ will very sparse and do a poor job of predicting $\mathbf{y}$. If $\lambda$ is too small, then $\hat{\mathbf{x}}$ will be not sparse at all, and our model could be overfit.&lt;/p&gt;

&lt;p&gt;We can use cross-validation to estimate a good value for $\lambda$, by finding the value which minimizes the error of our model on the held-out test data. This procedure for model selection is machine learning 101 (see Chap. 7 in &lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/ESLII.pdf&quot;&gt;&lt;em&gt;ESL&lt;/em&gt;&lt;/a&gt;). The basic picture looks like this:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Cross-validation schematic for LASSO. &lt;/b&gt;dashed vertical line denotes the best value for $\lambda$, which achieves lowest error on the held-out test set.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/crossval-lasso.png&quot; alt=&quot;dashed vertical line denotes the best value for $\lambda$, which achieves lowest error on the held-out test set.&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Cross-validation schematic for LASSO&lt;/b&gt;
	dashed vertical line denotes the best value for $\lambda$, which achieves lowest error on the held-out test set.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;PCA also has an important hyperparameter that people worry about — the number of components in the model. People have published a lot of papers on this (e.g. &lt;a href=&quot;https://arxiv.org/abs/1305.5870&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;http://papers.nips.cc/paper/1853-automatic-choice-of-dimensionality-for-pca.pdf&quot;&gt;here&lt;/a&gt;, &amp;amp; &lt;a href=&quot;https://arxiv.org/abs/math/0609042&quot;&gt;here&lt;/a&gt;). Similarly, many clustering models require the user to choose the number of clusters prior to fitting the model. Choosing the number of clusters has also received &lt;a href=&quot;https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set&quot;&gt;a lot of attention&lt;/a&gt;. Viewed from the perspective of matrix factorization, these are the &lt;em&gt;exact same problem&lt;/em&gt; – i.e. how to choose $r$, the width of $\mathbf{U}$ and the height of $\mathbf{V}^T$.&lt;/p&gt;

&lt;h2 id=&quot;a-cross-validation-procedure-for-matrix-decomposition&quot;&gt;A cross-validation procedure for matrix decomposition&lt;/h2&gt;

&lt;p&gt;Without further ado, here are some plots that demonstrate how cross-validation can help you choose the number of components in PCA, NMF, and K-means clustering. In each example, I generated data from a ground truth model with $r=4$ and then added noise. That is, for PCA, the correct number of PCs was 4. For k-means clustering, the correct number of clusters was 4. From the test error, you can see that all models begin to overfit when $r&amp;gt;4$. From the training data alone, the cutoff is maybe not so clear.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Cross-validation plots for some matrix decomposition models.. &lt;/b&gt;Note that the k-means implementation is a bit noisy so I averaged across multiple optimization runs. The code to generate these plots is &lt;a href=&#39;https://gist.github.com/ahwillia/65d8f87fcd4bded3676d67b55c1a3954&#39; target=&#39;_blank&#39;&gt;posted here&lt;/a&gt;.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/cv_curves.png&quot; alt=&quot;Note that the k-means implementation is a bit noisy so I averaged across multiple optimization runs. The code to generate these plots is &amp;lt;a href=&#39;https://gist.github.com/ahwillia/65d8f87fcd4bded3676d67b55c1a3954&#39; target=&#39;_blank&#39;&amp;gt;posted here&amp;lt;/a&amp;gt;.&quot; width=&quot;900px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Cross-validation plots for some matrix decomposition models.&lt;/b&gt;
	Note that the k-means implementation is a bit noisy so I averaged across multiple optimization runs. The code to generate these plots is &lt;a href=&quot;https://gist.github.com/ahwillia/65d8f87fcd4bded3676d67b55c1a3954&quot; target=&quot;_blank&quot;&gt;posted here&lt;/a&gt;.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To make these plots I used a “speckled” holdout pattern (&lt;a href=&quot;http://dx.doi.org/10.2307/1267639&quot;&gt;Wold, 1978&lt;/a&gt;). For simplicity and demonstration, I left out a small number of elements of $\mathbf{Y}$ at random:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;A good solution is to hold out data at random.. &lt;/b&gt;Importantly, we can still fit all parameters in $\mathbf{U}$ and $\mathbf{V}$ as long as no column or row of $\mathbf{Y}$ is fully removed. Intuitively, we should keep at least $r$ observations in each row or column.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/holdout.png&quot; alt=&quot;Importantly, we can still fit all parameters in $\mathbf{U}$ and $\mathbf{V}$ as long as no column or row of $\mathbf{Y}$ is fully removed. Intuitively, we should keep at least $r$ observations in each row or column.&quot; width=&quot;700px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;A good solution is to hold out data at random.&lt;/b&gt;
	Importantly, we can still fit all parameters in $\mathbf{U}$ and $\mathbf{V}$ as long as no column or row of $\mathbf{Y}$ is fully removed. Intuitively, we should keep at least $r$ observations in each row or column.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Formally, we define a binary matrix $\mathbf{M}$, which acts as a mask over our data. That is every element in $\mathbf{M}$ is either $m_{ij} = 0$, indicating a left out datapoint, or $m_{ij}=1$, indicating a datapoint in the training set. We are left with the following optimization problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\underset{\mathbf{U}, \mathbf{V}}{\text{minimize}} \quad \left \lVert \mathbf{M} \circ \left ( \mathbf{U} \mathbf{V}^T - \mathbf{Y} \right ) \right \lVert_F^2 
\end{equation}&lt;/script&gt;

&lt;p&gt;Where $ \circ $ denotes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hadamard_product_(matrices)&quot;&gt;Hadamard product&lt;/a&gt;. After solving the above optimization problem, we can then evaluate the error of our model on the held-out datapoints as: $\left \lVert (1-\mathbf{M}) \circ \left ( \mathbf{U} \mathbf{V}^T - \mathbf{Y} \right ) \right \lVert_F^2$.&lt;/p&gt;

&lt;p&gt;We need not choose $\mathbf{M}$ to be random! We can select whatever holdout pattern we like, and indeed to ensure that all columns and rows are left out at equal rates we can leave out pseudo-diagonals of the matrix. In the interest of brevity and simplicity we will sweep these choices under the rug, but see &lt;a href=&quot;http://dx.doi.org/10.2307/1267639&quot;&gt;Fig.1 in Wold (1978)&lt;/a&gt; for more discussion.&lt;/p&gt;

&lt;p&gt;So how do we solve this optimization problem? As I mentioned before, equation 3 amounts to the well-studied low-rank &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_completion&quot;&gt;matrix completion&lt;/a&gt; problem. However many papers that I’ve looked at do not provide algorithms for solving the problem in the particular form that we care about. In particular, they tend to optimize over a single matrix, call it $\hat{\mathbf{Y}}$, rather than jointly optimize over a factorized representation where $\hat{\mathbf{Y}} = \mathbf{U} \mathbf{V}^T$ (which is what we’d like to do). For example, &lt;a href=&quot;https://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf&quot;&gt;Candes and Recht (2008)&lt;/a&gt; consider:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \underset{\hat{\mathbf{Y}}}{\text{minimize}} &amp; &amp; \lVert \hat{\mathbf{Y}} \lVert_* \\
&amp; \text{subject to} &amp; &amp; \mathbf{M} \circ \hat{\mathbf{Y}} = \mathbf{M} \circ \mathbf{Y}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\lVert \cdot \lVert_*$ denotes the &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms&quot;&gt;nuclear norm&lt;/a&gt; of a matrix. Minimizing the nuclear norm is a good surrogate for minimizing the rank of $\hat{\mathbf{Y}}$ directly, which is more computationally challenging. The advantage of this approach is that the optimization problem is convex and therefore comes with really nice guarantees and mathematical analysis.&lt;/p&gt;

&lt;p&gt;The problem with this is that it solves the matrix completion problem without giving us $\mathbf{U}$ and $\mathbf{V}$, which are of direct interest to us in the context of PCA and clustering. Furthermore, I don’t think this approach scales particularly well to very large matrices or tensor datasets since you are optimizing over a very large number of variables $mn$, as opposed to $mr + nr$ variables in the factorized representation.&lt;/p&gt;

&lt;p&gt;A very simple and effective procedure for fitting matrix decomposition is the alternating minimization algorithm, which I’ve &lt;a href=&quot;http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/&quot;&gt;blogged&lt;/a&gt; and &lt;a href=&quot;https://cbmm.mit.edu/video/dimensionality-reduction-matrix-and-tensor-coded-data-part-1&quot;&gt;talked&lt;/a&gt; about in the past. For the case of PCA, this amounts to:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Algorithm:&lt;/strong&gt; &lt;em&gt;Alternating minimization:&lt;/em&gt;&lt;br /&gt;&lt;br /&gt;
1      initialize $\mathbf{U}$ randomly&lt;br /&gt;&lt;br /&gt;
2      &lt;strong&gt;while&lt;/strong&gt; not converged&lt;br /&gt;&lt;br /&gt;
3           $\mathbf{V}  \leftarrow \underset{\tilde{\mathbf{V}}}{\text{argmin}} \quad \left \lVert \mathbf{M} \circ \left ( \mathbf{U} \tilde{\mathbf{V}}^T - \mathbf{Y} \right ) \right \lVert_F^2$&lt;br /&gt;&lt;br /&gt;
4           $\mathbf{U}  \leftarrow \underset{\tilde{\mathbf{U}}}{\text{argmin}} \quad \left \lVert \mathbf{M} \circ \left ( \tilde{\mathbf{U}} \mathbf{V}^T - \mathbf{Y} \right ) \right \lVert_F^2$&lt;br /&gt;&lt;br /&gt;
5      &lt;strong&gt;end while&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While other papers (e.g. &lt;a href=&quot;https://arxiv.org/abs/1312.0925&quot;&gt;Hardt 2013&lt;/a&gt;) use alternating minimization to solve matrix completion problems, I haven’t come across many good sources that explain how to implement this idea in practice (please email me if you find good ones!). So I’ll give some guidance below.&lt;/p&gt;

&lt;h4 id=&quot;implementation-notes-pca&quot;&gt;Implementation Notes: PCA&lt;/h4&gt;

&lt;p&gt;Each parameter update (lines 3 and 4) of the alternating minimization algorithm boils down to a least-squares problem &lt;em&gt;with missing data&lt;/em&gt;. In this interest of brevity, I derived the solution to least squares under missing data &lt;a href=&quot;/itsneuronalblog/2018/02/26/censored-lstsq/&quot;&gt;in a separate post&lt;/a&gt;. The answer is kinda cool and involves tensors! The basic solution we arrive at is this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;censored_lstsq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Least squares of M * (AX - B) over X, where M is a masking matrix.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rhs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# n x r x 1 tensor&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Wow — only three lines thanks to the magic of numpy broadcasting! And we can use this subroutine to cross-validate PCA as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cv_pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_holdout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Fit PCA while holding out a fraction of the dataset.
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# create masking matrix&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p_holdout&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# fit pca&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Vt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;censored_lstsq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;censored_lstsq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# We could orthogonalize U and Vt and then rotate to align&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# with directions of maximal variance, but we won&#39;t bother.&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# return result and test/train error&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;resid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test_err&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_err&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;A few disclaimers about the above code, which is only meant to give the gist of a solution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the spirit of brevity, I don’t check for convergence. Obviously, in real code, you should either monitor the reconstruction error or the change in &lt;code class=&quot;highlighter-rouge&quot;&gt;U&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Vt&lt;/code&gt; and break the loop when these converge.&lt;/li&gt;
  &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;censored_lstsq&lt;/code&gt; function is also not optimized. See discussion in my &lt;a href=&quot;/itsneuronalblog/2018/02/26/censored-lstsq/&quot;&gt;other post&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;If there are too many missing values (e.g. an entire row or column is left out) then the &lt;code class=&quot;highlighter-rouge&quot;&gt;censored_lstsq&lt;/code&gt; function will fail due to a singular/non-invertible matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;implementation-notes-nmf-and-k-means&quot;&gt;Implementation Notes: NMF and K-means&lt;/h4&gt;

&lt;p&gt;NMF is very similar to PCA when viewed from the perspective of matrix factorization. Each subproblem becomes a &lt;em&gt;nonnegative least-squares problem&lt;/em&gt; with missing data in the dependent variable. Nonnegative least-squares is a very well-studied problem, and the methods discussed for classic least squares can be generalized without that much effort. Jingu Kim has a really nice Python library called &lt;a href=&quot;https://github.com/kimjingu/nonnegfac-python&quot;&gt;nonnegfac-python&lt;/a&gt; that handles these kind of things.&lt;/p&gt;

&lt;p&gt;Another possibility is to modify the &lt;a href=&quot;http://www.almoststochastic.com/2013/06/nonnegative-matrix-factorization.html&quot;&gt;classic multiplicative update rules for NMF&lt;/a&gt;. These rules are originally:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_{ij} \leftarrow U_{ij} \frac{(\mathbf{Y} \mathbf{V})_{ij}}{(\mathbf{U} \mathbf{V}^T \mathbf{V})_{ij}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_{ij} \leftarrow V_{ij} \frac{(\mathbf{U}^T \mathbf{Y})_{ij}}{(\mathbf{U}^T \mathbf{U} \mathbf{V}^T)_{ij}}&lt;/script&gt;

&lt;p&gt;Under missing data, these rules become:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;U_{ij} \leftarrow U_{ij} \frac{((\mathbf{M} \circ \mathbf{Y}) \mathbf{V})_{ij}}{((\mathbf{M} \circ \mathbf{U} \mathbf{V}^T) \mathbf{V})_{ij}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_{ij} \leftarrow V_{ij} \frac{(\mathbf{U}^T (\mathbf{M} \circ \mathbf{Y}))_{ij}}{(\mathbf{U}^T (\mathbf{M} \circ \mathbf{U} \mathbf{V}^T))_{ij}}&lt;/script&gt;

&lt;p&gt;K-means clustering was a bit more finicky (as you can see I averaged over many random initializations in the figure above). But the basic cross-validation principles are the same. See &lt;a href=&quot;https://arxiv.org/abs/1411.7013&quot;&gt;Chi et al. (2016)&lt;/a&gt; for fitting K-means clustering with missing data. There is also a really short and simple implementation of this idea in a &lt;a href=&quot;https://stackoverflow.com/questions/35611465/python-scikit-learn-clustering-with-missing-data&quot;&gt;Stack Overflow answer&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;conclusions-and-references&quot;&gt;Conclusions and References&lt;/h3&gt;

&lt;p&gt;Everything here is a brief overview of a topic that has been studied more deeply in the literature. The take-home message is that cross-validation is a bit tricky for unsupervised learning, but there is a simple approach that generalizes to many methods. I just showed you three (PCA, NMF, and $k$-means clustering), but the basic idea likely applies elsewhere.&lt;/p&gt;

&lt;p&gt;How can the ideas I covered here be improved upon? From a computational standpoint, leaving out data at random made our lives difficult. A nicer choice may have been to hold out some of the data from a subset of rows and columns. This means that our holdout pattern partitions the data matrix into four blocks, and without loss of generality we can rearrange the rows and columns so that the upper left block is held out as shown below:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Bi-cross-validation holdout pattern.. &lt;/b&gt;.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/bcv_holdout.png&quot; alt=&quot;.&quot; width=&quot;900px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Bi-cross-validation holdout pattern.&lt;/b&gt;
	.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;This holdout pattern was considered by &lt;a href=&quot;https://arxiv.org/abs/0908.2062&quot;&gt;Owen &amp;amp; Perry (2009)&lt;/a&gt; who attribute the basic idea to Gabriel (although he only proposed holding out a single entry of the matrix, rather than a block). A neat thing about this approach is that one can fit a PCA or NMF model to the (fully observed) bottom right block, $\mathbf{Y}_{(2,2)}$, and then use that model to predict the (held-out test) block $\mathbf{Y}_{(1,1)}$ based on the off-diagonal blocks. Understanding how this works takes a bit of effort, but the take-home message is that their approach offers significant speed ups to what I outlined in this post.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.02658&quot;&gt;Fu &amp;amp; Perry (2017)&lt;/a&gt; recently extended the above idea to K-means clustering. And I’m still digging through it but &lt;a href=&quot;https://arxiv.org/abs/0909.3052&quot;&gt;Perry’s PhD thesis&lt;/a&gt; seems like a good reference as well.&lt;/p&gt;

&lt;p&gt;Predating the above work, there is a nice review of these topics by &lt;a href=&quot;https://doi.org/10.1007/s00216-007-1790-1&quot;&gt;Bro et al. (2008)&lt;/a&gt;. They show that many previous algorithms for cross-validating PCA don’t work that well in practice and aren’t very well-motivated. So be careful if you are reading older papers on this subject!&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Solving Least-Squares Regression with Missing Data</title>
   <link href="http://alexhwilliams.info/2018/02/26/censored-lstsq/"/>
   <updated>2018-02-26T00:00:00-08:00</updated>
   <id>http://alexhwilliams.info/2018/02/26/censored-lstsq</id>
   <content type="html">&lt;p&gt;I recently got interested in figuring out how to perform &lt;a href=&quot;/itsneuronalblog/2018/02/26/crossval/&quot;&gt;cross-validation on PCA&lt;/a&gt; and other matrix factorization models. The way I chose to solve the cross-validation problem (see my other post) revealed another interesting problem: how to fit a linear regression model with missing dependent variables. Since I did not find too many existing resources on this material, I decided to briefly document what I learned in this blog post.&lt;/p&gt;

&lt;h3 id=&quot;the-problem&quot;&gt;The Problem&lt;/h3&gt;

&lt;p&gt;We want to solve the following optimization problem, which corresponds to least-squares regression with missing data:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\underset{\mathbf{X}}{\text{minimize}} \quad \left \lVert \mathbf{M} \circ (\mathbf{A} \mathbf{X} - \mathbf{B}) \right \lVert^2_F
\end{equation}&lt;/script&gt;

&lt;p&gt;The columns of the matrix $\mathbf{B}$ hold different dependent variables. The columns of the matrix $\mathbf{A}$ hold independent variables. We would like to find the regression coefficients, contained in $\mathbf{X}$, that minimize the squared error between our model prediction $\mathbf{A} \mathbf{X}$ and the dependent variables, $\mathbf{B}$.&lt;/p&gt;

&lt;p&gt;However, suppose some entries in the matrix $\mathbf{B}$ are missing. We can encode the missingness with a masking matrix, $\mathbf{M}$. If element $B_{ij}$ is missing, we set $M_{ij} = 0$. Otherwise, we set $M_{ij} = 1$, meaning that element $B_{ij}$ was observed. The “$\circ$” operator denotes the Hadamard product between two matrices. Thus, in equation 1, the masking matrix $\mathbf{M}$ has the effect of zeroing out, or ignoring, the reconstruction error wherever $\mathbf{B}$ has a missing element.&lt;/p&gt;

&lt;p&gt;Visually, the problem we are trying to solve looks like this:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Least squares with missing data.. &lt;/b&gt;Black squares denote missing data values. Note that we only consider missing dependent variables in this post. The masking matrix $\mathbf{M}$ would have zeros along the black squares and ones elsewhere.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/lstsq-missing.png&quot; alt=&quot;Black squares denote missing data values. Note that we only consider missing dependent variables in this post. The masking matrix $\mathbf{M}$ would have zeros along the black squares and ones elsewhere.&quot; width=&quot;700px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Least squares with missing data.&lt;/b&gt;
	Black squares denote missing data values. Note that we only consider missing dependent variables in this post. The masking matrix $\mathbf{M}$ would have zeros along the black squares and ones elsewhere.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Though it is not entirely correct, you can think of the black boxes in the above visualization as &lt;code class=&quot;highlighter-rouge&quot;&gt;NaN&lt;/code&gt; entries in the data matrix. &lt;em&gt;The black boxes are not zeros&lt;/em&gt;. If we replaced the &lt;code class=&quot;highlighter-rouge&quot;&gt;NaN&lt;/code&gt;s with zeros, we obviously get the wrong result. The missing datapoints could be any (nonzero) value!&lt;/p&gt;

&lt;p&gt;The optimization problem shown in equation 1 is convex, and it turns out we can derive an analytic solution (similar to least-squares in the abscence of missing data). We can differentiate the objective function with respect to $\mathbf{X}$ and set the gradient to zero. Solving the resulting expression for $\mathbf{X}$ will give us the minimum of the optimization problem. After some computations (see &lt;a href=&quot;#appendix&quot;&gt;the appendix&lt;/a&gt; for details) we arrive at:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\mathbf{A}^T ( \mathbf{M} \circ (\mathbf{A} \mathbf{X})) = \mathbf{A}^T (\mathbf{M} \circ \mathbf{B})
\end{equation}&lt;/script&gt;

&lt;p&gt;which we’d like to solve for $\mathbf{X}$. Computing the right hand side is easy, but the Hadamard product mucks things up on the left hand size. So we need to do some clever rearranging. Consider element $(i,j)$ on the left hand side above, which looks like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_k a_{ki} m_{kj} \sum_p a_{kp} x_{pj}&lt;/script&gt;

&lt;p&gt;Pulling the sum over $p$ out front, we get a nicer expression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_p x_{pj} \left [ \sum_k a_{ki} m_{kj} a_{kp} \right ]&lt;/script&gt;

&lt;p&gt;The term in square brackets has three indices: $i$, $p$, and $j$. So it is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tensor&quot;&gt;tensor&lt;/a&gt;! In fact, the above expression is the multiplication of a matrix, $\mathbf{X}$, with a tensor. See section 2.5 of &lt;a href=&quot;http://www.kolda.net/publication/koba09/&quot;&gt;Kolda &amp;amp; Bader (2009)&lt;/a&gt; for a summary of this matrix-tensor operation.&lt;/p&gt;

&lt;p&gt;Let’s define the tensor as $\mathcal{T}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\mathcal{T}_{ip}^{(j)} = \sum_k a_{ki} m_{kj} a_{kp}
\end{equation}&lt;/script&gt;

&lt;p&gt;I suggestively decided to index along mode $j$ in the superscript. Consider a slice through the tensor at index $j$. We are left with a matrix, which can be written as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{T}^{(j)} = \mathbf{A}^T \text{diag}(\mathbf{m}_{j}) \mathbf{A}&lt;/script&gt;

&lt;p&gt;Where $\text{diag}(\cdot)$ transforms a vector into a diagonal matrix (a standard operation available in MATLAB/Python). Let’s draw an illustration to summarize what we’ve done so far:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Representation of equation 2 using the tensor described in equation 3.. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/tensor.png&quot; alt=&quot;&quot; width=&quot;700px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Representation of equation 2 using the tensor described in equation 3.&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;It turns out that we are basically done due to the magic of &lt;a href=&quot;https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt; broadcasting&lt;/a&gt;. We simply need to construct the tensor, $\mathcal{T}$, and the matrix $\mathbf{A}^T (\mathbf{M} \circ \mathbf{B})$, and then call &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;numpy.linalg.solve&lt;/code&gt;&lt;/a&gt;. The following code snippet does exactly this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;censored_lstsq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Solves least squares problem subject to missing data.

    Note: uses a broadcasted solve for speed.

    Args
    ----
    A (ndarray) : m x r matrix
    B (ndarray) : m x n matrix
    M (ndarray) : m x n binary matrix (zeros indicate missing values)

    Returns
    -------
    X (ndarray) : r x n matrix that minimizes norm(M*(AX - B))
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# Note: we should check A is full rank but we won&#39;t bother...&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# if B is a vector, simply drop out corresponding rows in A&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;leastsq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# else solve via tensor representation&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rhs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# n x r x 1 tensor&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:,:])&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# n x r x r tensor&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rhs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# transpose to get r x n&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Since $\mathbf{A}^T \text{diag}(\mathbf{m}_{j}) \mathbf{A}$ is symmetric and positive definite we could use the Cholesky decomposition to solve the system rather than the more generic numpy solver. If anyone has an idea/comment about how to implement this efficiently in Python, I’d love to know!&lt;/p&gt;

&lt;p&gt;Here’s my rough analysis of time complexity (let me know if you spot an error):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The Hadamard product &lt;code class=&quot;highlighter-rouge&quot;&gt;M * B&lt;/code&gt; is $\mathcal{O}(mn)$&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Strassen_algorithm&quot;&gt;Strassen&lt;/a&gt; fanciness aside, matrix multiplication &lt;code class=&quot;highlighter-rouge&quot;&gt;np.dot(A.T, M * B)&lt;/code&gt; is $\mathcal{O}(mnr)$&lt;/li&gt;
  &lt;li&gt;The broadcasted Hadamard &lt;code class=&quot;highlighter-rouge&quot;&gt;M.T[:,:,None] * A[None,:,:]&lt;/code&gt; is $\mathcal{O}(mnr)$&lt;/li&gt;
  &lt;li&gt;Building the tensor involves &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; matrix multiplications, totalling $\mathcal{O}(m n r^2)$.&lt;/li&gt;
  &lt;li&gt;Then solving each of the &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; slices in the tensor takes $\mathcal{O}(n r^3)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So the total number of operations is $\mathcal{O}(n r^3 + m n r^2)$. In practice, this does run noticeably slower than a regular least-squares solve, so I’d love suggestions for improvements!&lt;/p&gt;

&lt;h3 id=&quot;a-less-fun-solution&quot;&gt;A less fun solution&lt;/h3&gt;

&lt;p&gt;There is another simple solution to this least-squares problem, but it doesn’t involve tensors and requires a &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; loop. The idea is to solve for each column of $\mathbf{X}$ sequentially. Let $\mathbf{x}_i$ be the $i^\text{th}$ column of $\mathbf{X}$ and let $\mathbf{b}_i$ likewise by the $i^\text{th}$ column of $\mathbf{B}$. It is intuitive that the least-squares solution for $\mathbf{x}_i$ is given by dropping the rows of $\mathbf{A}$ where $\mathbf{b}_i$ has a missing entry.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Least squares with missing data for a single column of $B$.. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca-crossval/lstsq-col.png&quot; alt=&quot;&quot; width=&quot;700px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Least squares with missing data for a single column of $B$.&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;censored_lstsq_slow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Solves least squares problem subject to missing data.

    Note: uses a for loop over the columns of B, leading to a
    slower but more numerically stable algorithm

    Args
    ----
    A (ndarray) : m x r matrix
    B (ndarray) : m x n matrix
    M (ndarray) : m x n binary matrix (zeros indicate missing values)

    Returns
    -------
    X (ndarray) : r x n matrix that minimizes norm(M*(AX - B))
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# drop rows where mask is zero&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lstsq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It has a similar complexity of $\mathcal{O}(m n r^2)$, due to solving &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; least squares equations each with $\mathcal{O}(m r^2)$ operations. Since we have added a &lt;code class=&quot;highlighter-rouge&quot;&gt;for&lt;/code&gt; loop this solution does run noticeably slower on my laptop than the first solution for large &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt;. In C++ or Julia, this for loop would be less of a worry.&lt;/p&gt;

&lt;p&gt;Also, I think this second (less fun) solution should be more accurate numerically because it does not compute the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gramian_matrix&quot;&gt;Gramian&lt;/a&gt;, $\mathbf{A}^T \mathbf{A}$, whereas the first method I offered essentially does this. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Condition_number&quot;&gt;condition number&lt;/a&gt; of the Gramian is the square of the original matrix, $\kappa ( \mathbf{A}^T \mathbf{A}) = \kappa (\mathbf{A})^2$, so the result will be less stable. This is why some least-squares solvers do not use the normal equations under the hood (they instead use QR decomposition).&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I’ve outlined a couple of simple ways to solve the least-squares regression problem with missing data in the dependent variables. The two Python functions I offered have a tradeoff in speed and accuracy, and while the code could certainly be further optimized - I expect these functions will work well enough for some simple applications.&lt;/p&gt;

&lt;h3 id=&quot;appendix&quot;&gt;Appendix&lt;/h3&gt;

&lt;p&gt;This appendix contains some simple manipulations on the objective function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left \lVert \mathbf{M} \circ (\mathbf{A} \mathbf{X} - \mathbf{B}) \right \lVert^2_F&lt;/script&gt;

&lt;p&gt;Let’s define $\mathbf{E} = \mathbf{A} \mathbf{X} - \mathbf{B}$, which is a matrix of unmasked residuals. Then, since $m_{ij} \in \{0, 1\}$ the objective function simplifies:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left \lVert \mathbf{M} \circ \mathbf{E} \right \lVert^2_F &amp;= \textbf{Tr} \left [ (\mathbf{M} \circ \mathbf{E})^T (\mathbf{M} \circ \mathbf{E}) \right ] \\
&amp;= \sum_{i=1}^n \sum_{j=1}^m m_{ji} e_{ji} m_{ji} e_{ji} \\
&amp;= \sum_{i=1}^n \sum_{j=1}^m m_{ji} e_{ji}^2 \\
&amp;= \textbf{Tr}[\mathbf{E}^T (\mathbf{M} \circ \mathbf{E})]
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We are using $\textbf{Tr} [ \cdot ]$ to denote the trace of a matrix, and the fact that $\lVert \mathbf{X} \lVert^2_F = \textbf{Tr} [\mathbf{X}^T \mathbf{X}]$ for any matrix $\mathbf{X}$. Now we’ll substitute $\mathbf{A}\mathbf{X} - \mathbf{B}$ back in and expand the expression:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;\textbf{trace}[(\mathbf{A} \mathbf{X} - \mathbf{B})^T (\mathbf{M} \circ (\mathbf{A} \mathbf{X} - \mathbf{B}))]\\
&amp;= \textbf{Tr}[ \mathbf{X}^T \mathbf{A}^T (\mathbf{M} \circ (\mathbf{A} \mathbf{X}))] - 2 \cdot \textbf{Tr} [ \mathbf{X}^T \mathbf{A}^T (\mathbf{M} \circ \mathbf{B})] + \textbf{Tr}[\mathbf{B}^T (\mathbf{M} \circ \mathbf{B})]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now we differentiate these three terms with respect to $\mathbf{X}$. The term on the right goes to zero as it does not depend on $\mathbf{X}$. The term in the middle is quite standard and can be found in the &lt;a href=&quot;http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf&quot;&gt;matrix cookbook&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
\frac{\partial}{\partial\mathbf{X}} \textbf{Tr} \left [ \mathbf{X}^T \mathbf{A}^T (\mathbf{M} \circ \mathbf{B}) \right ] = \mathbf{A}^T (\mathbf{M} \circ \mathbf{B})
\end{equation}&lt;/script&gt;

&lt;p&gt;The first term in equation 5 is a bit of a pain and we’ll derive it manually. We resort to a summation notation and compute the partial derivative with respect to element $(a, b)$ of $\mathbf{X}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial x_{ab}} \left [ \textbf{Tr} \left [ \mathbf{X}^T \mathbf{A}^T (\mathbf{M} \circ (\mathbf{A} \mathbf{X})) \right ] \right ] = \frac{\partial}{\partial x_{ab}} \frac{\partial}{\partial x_{ab}} \sum_i \sum_j \sum_k a_{jk} x_{ki} m_{ji} \sum_p a_{jp} x_{pi}&lt;/script&gt;

&lt;p&gt;Now we’ll pull all of the sums out front and make our notation more compact. Then we’ll differentiate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\frac{\partial}{\partial x_{ab}} \sum_{i,j,k,p} a_{jk} x_{ki} m_{ji} a_{jp} x_{pi} \\
&amp;= 
\sum_{i,j,k,p} \frac{\partial}{\partial x_{ab}} a_{jk} x_{ki} m_{ji} a_{jp} x_{pi} \\
&amp;= 
\sum_{i,j,k,p} a_{jk} \frac{\partial x_{ki}}{\partial x_{ab}} m_{ji} a_{jp} x_{pi} + a_{jk} x_{ki} m_{ji} a_{jp} \frac{\partial x_{pi}}{\partial x_{ab}} \\
&amp;= 
\sum_{i,j,k,p} a_{jk} \delta_{ka} \delta_{ib} m_{ji} a_{jp} x_{pi} + \sum_{i,j,k,p} a_{jk} x_{ki} m_{ji} a_{jp} \delta_{pa} \delta_{ib} \\
&amp;= \sum_{j,p} a_{ja} m_{jb} a_{jp} x_{pb} + \sum_{j,k} a_{jk} x_{kb} m_{jb} a_{ja}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;By inspection, you can convince yourself that this final expression maps onto $2 \mathbf{A^T (\mathbf{M} \circ (\mathbf{A} \mathbf{X}))}$ in matrix notation. Combining this result with equation 6, we arrive at:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2 \cdot \mathbf{A}^T ( \mathbf{M} \circ (\mathbf{A} \mathbf{X})) - 2 \cdot \mathbf{A}^T (\mathbf{M} \circ \mathbf{B}) = 0&lt;/script&gt;

&lt;p&gt;Which immediately implies equation 2 in the main text.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>On the identifiability of PCA and related methods.</title>
   <link href="http://alexhwilliams.info/2016/12/01/uniqueness/"/>
   <updated>2016-12-01T00:00:00-08:00</updated>
   <id>http://alexhwilliams.info/2016/12/01/uniqueness</id>
   <content type="html">&lt;p&gt;Justification for using orthogonal components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We tend to plot and think about data in orthogonal coordinate systems. Specifically, imposing orthogonality means the principal components are uncorrelated, so we can talk about the contribution of individual components without referring to the others.&lt;/li&gt;
  &lt;li&gt;Imposing this means that there is a unique solution to the optimization problem so that there is a standard protocol for the field.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the goal was to minimize $\lVert A - W C^T \lVert_F^2$ without constraints, then there are an infinite number of solutions! To see this let $R$ be any invertible $r \times r$ matrix. Then for any candidate solution $\{W, C\}$ there is an equally viable solution $\{W^\prime, C^\prime\}$ with $W^\prime = W R^{-1}$ and $C^\prime = C R^T$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert A - W C^T \lVert_F^2 = \lVert A - W R^{-1} R C^T \lVert_F^2 = \lVert A - W^\prime C^{\prime T} \lVert_F^2&lt;/script&gt;

&lt;p&gt;Thus, there is a very general set of linear transformations we can apply to components and loadings without affecting the reconstruction error. This fundamental non-uniqueness is important to understand because it limits our ability to interpret and assign meaning to the components. In other cases, we can understand outcome of the model in this way. For example, suppose we are trying to develop a model of some black-box system that has multiple inputs and multiple outputs. We observe a sequence of inputs $\mathbf{x}$ and their corresponding outputs $\mathbf{y}$:&lt;/p&gt;

&lt;p&gt;PICTURE&lt;/p&gt;

&lt;p&gt;If the black-box implements a linear transformation, then we can determine this transformation by doing linear regression. The resulting model is a matrix, $B$, which can be used to predict the outcome for any input: $\mathbf{y} = B \mathbf{x}$. Each element in the matrix has a nice interpretation: $B_{ij}$ tells us the effect of input $j$ on output $i$. If $B_{ij}$ is nearly zero then changing input $x_j$ barely effects output $y_i$. If $B_{ij}$ is positive (resp. negative) then increasing $x_j$ increases (resp. decreases) $y_i$. We can even attach units to each $B_{ij}$ as the units of $y_i$ divided by the units of $x_j$.&lt;/p&gt;

&lt;p&gt;All of this is very appealing, but falls apart when we move to PCA. In this case, we observe some output of a black-box system, but we don’t know the underlying inputs that produced this dataset. For example, we might measure the expression of many genes across many different samples. We may suspect that the input of the system is low-dimensional and more simple that then output we observed (e.g. we might think activity of a handful of transcription factors determines the expression of many genes in our experiment). However, in this hypothetical case, we weren’t able to identify or measure these inputs. Nevertheless, there is some real set of inputs $\mathbf{w}^*_i$ and a real linear transformation $C^*$ for each observation $\mathbf{a}_i$. Relabeling the input-output diagram we get:&lt;/p&gt;

&lt;p&gt;PICTURE&lt;/p&gt;

&lt;p&gt;In this case we can consolidate our observations into a data matrix $A$, and do PCA to get a loadings $W$ and components $C$. But it is somewhat intuitive that we can’t recover both the inputs and the actual transformation of the system. This is sort of like asking someone to tell you which two numbers you multiplied together to get the number 16 (4 and 4? or 2 and 8?).&lt;/p&gt;

&lt;p&gt;However, PCA &lt;strong&gt;does&lt;/strong&gt; capture the correct linear subspace.&lt;/p&gt;

&lt;p&gt;Perhaps the best known example is &lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_component_analysis&quot;&gt;independent components analysis (ICA)&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Demystifying Factor Analysis</title>
   <link href="http://alexhwilliams.info/2016/04/20/pca-appendix/"/>
   <updated>2016-04-20T00:00:00-07:00</updated>
   <id>http://alexhwilliams.info/2016/04/20/pca-appendix</id>
   <content type="html">&lt;h3 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#noise-models&quot; id=&quot;markdown-toc-noise-models&quot;&gt;Noise Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#least-squares-regression-iff-gaussian-noise-model&quot; id=&quot;markdown-toc-least-squares-regression-iff-gaussian-noise-model&quot;&gt;Least-squares regression $\iff$ Gaussian noise model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#a-noise-model-for-pca&quot; id=&quot;markdown-toc-a-noise-model-for-pca&quot;&gt;A noise model for PCA&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#other-references&quot; id=&quot;markdown-toc-other-references&quot;&gt;Other references:&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Long time no blog. My last post &lt;a href=&quot;http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/&quot;&gt;&lt;em&gt;everything you did and didn’t know about PCA&lt;/em&gt;&lt;/a&gt; was poorly named. Clearly, I didn’t cover &lt;strong&gt;&lt;em&gt;everything&lt;/em&gt;&lt;/strong&gt; about PCA. In particular, I think I may have offended statisticians by not using probability, and not mentioning noise models.&lt;/p&gt;

&lt;p&gt;I recently heard from an esteemed person that adding an explicit noise model to PCA helps tremendously in practice. I was skeptical. So let’s explore that claim.&lt;/p&gt;

&lt;p&gt;Here, I’ll show that &lt;a href=&quot;#&quot;&gt;probabilistic PCA&lt;/a&gt; is equivalent to PCA with quadratic regularization and that &lt;a href=&quot;#&quot;&gt;factor analysis&lt;/a&gt; is equivalent to ??.&lt;/p&gt;

&lt;h3 id=&quot;noise-models&quot;&gt;Noise Models&lt;/h3&gt;

&lt;p&gt;So what is a &lt;em&gt;noise model&lt;/em&gt; anyways? It is easiest to explain this for the case of linear regression, where we predict a dependent variable, $y$, from an independent variable, $x$ (both scalars, for now). Then the linear regression model is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
y_i = \beta_1 x_i + \beta_0 + \epsilon \, , \quad \epsilon \sim \text{Normal}(0, \sigma^2)
\end{equation}&lt;/script&gt;

&lt;p&gt;Here, $\beta_1$ and $\beta_0$ are the slope and intercept of the regression line — the parameters of our model that we want to estimate from data. Each datapoint or observation is indexed by $i = 1, 2, …, n$. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_variable&quot;&gt;random variable&lt;/a&gt; $\epsilon$ adds noise to every observation. The classic noise model here is a Normal (Gaussian) distribution, with variance $\sigma^2$. The visual picture to have in mind is this:&lt;/p&gt;

&lt;p&gt;PICTURE&lt;/p&gt;

&lt;p&gt;But we can generalize linear regression by changing the noise model. For example, we could assume that $\epsilon$ follows a &lt;a href=&quot;https://en.wikipedia.org/wiki/Laplace_distribution&quot;&gt;Laplacian distribution&lt;/a&gt;, which contains &lt;a href=&quot;https://stats.stackexchange.com/questions/180952/t-distribution-having-heavier-tail-than-normal-distribution&quot;&gt;heavier tails&lt;/a&gt; than the Normal distribution. This makes the noise model more accomodating to outliers, and ends up leading to a flavor of &lt;a href=&quot;https://en.wikipedia.org/wiki/Robust_regression&quot;&gt;robust regression&lt;/a&gt;.&lt;br /&gt;
So changing the noise model can help us develop other useful models for different applications.&lt;/p&gt;

&lt;p&gt;One more quick example. Both Gaussian and Laplacian noise models add &lt;br /&gt;
By changing to a Poisson noise model we can accomodate &lt;a href=&quot;https://en.wikipedia.org/wiki/Count_data&quot;&gt;count data&lt;/a&gt; more naturally. The Poisson distribution places probability only on nonnegative, integer values of $y_i$. For example, the number of neuron spikes within a small time window can be modeled as a Poisson random variable. The visual picture to have in mind is this:&lt;/p&gt;

&lt;p&gt;PICTURE&lt;/p&gt;

&lt;p&gt;This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Poisson_regression&quot;&gt;Poisson regression&lt;/a&gt;. All of these models can be beautifully unified into a class of models called &lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_linear_model&quot;&gt;&lt;em&gt;generalized linear models&lt;/em&gt; (GLMs)&lt;/a&gt;, which have become quite popular in neuroscience (CITATIONS).&lt;/p&gt;

&lt;p&gt;Thus, different noise distributions intuitively lead to different models. But how, precisely, does this happen? We’ll see that the noise model directly changes the optimization problem that you solve to fit the model parameters. In the language of my last post, it changes the &lt;em&gt;loss&lt;/em&gt; of the objective function.&lt;/p&gt;

&lt;h3 id=&quot;least-squares-regression-iff-gaussian-noise-model&quot;&gt;Least-squares regression $\iff$ Gaussian noise model&lt;/h3&gt;

&lt;p&gt;As a warm-up, let’s prove the following:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In linear regression, fitting a model to minimize the sum of squared residuals, is equivalent to fitting a model to maximize the likelihood function under a Gaussian noise model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Equation 1 means that each $y_i$ is a normally distributed random variable with mean $\beta_0 + \beta_1 x_i$ and variance $\sigma^2.$ We treat $\sigma^2$ as a constant. Formally,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i \overset{\scriptsize\text{i.i.d.}}{\sim} \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2)&lt;/script&gt;

&lt;p&gt;The symbol $\overset{\scriptsize\text{i.i.d.}}{\sim}$ means each observation is &lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&quot;&gt;“independent and identically distributed.”&lt;/a&gt; From the definition of the Normal distribution, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p (y_i \mid x_i, \beta_0, \beta_1) \propto \exp \left[ - \frac{(y_i-\hat{y}_i)^2}{2 \sigma^2 } \right]&lt;/script&gt;

&lt;p&gt;where $\hat y_i = \beta_0 + \beta_1 x_i$ is our linear model estimate. Each $y_i$ is an independent event (due to the $\text{i.i.d.}$ assumption) so we simply multiply these terms together to get the joint probability over &lt;em&gt;all&lt;/em&gt; $y_i$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta_0, \beta_1) = p(\mathbf{y} \mid \mathbf{x}, \beta_0, \beta_1) \propto \prod_{i=1}^n  \exp \left[ - \frac{(y_i-(\beta_0 + \beta_1 x_i))^2}{2 \sigma^2 } \right]&lt;/script&gt;

&lt;p&gt;This function, $L(\beta_0, \beta_1)$, is called the &lt;strong&gt;&lt;em&gt;likelihood&lt;/em&gt;&lt;/strong&gt; of the data.&lt;a href=&quot;#f1b&quot; id=&quot;f1t&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; Our goal is to find the parameters (slope $\beta_0$ and intercept $\beta_1$) that produce the &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;&lt;strong&gt;&lt;em&gt;maximum likelihood&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \underset{\beta_0, \beta_1}{\text{maximize}}
&amp; &amp; L(\beta_0, \beta_1)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In practice, we find the maximum likelihood estimates by &lt;em&gt;minimizing&lt;/em&gt; the &lt;em&gt;negative log-likelihood&lt;/em&gt; (these are equivalent problems). Taking the logarithm affords us &lt;a href=&quot;https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability&quot;&gt;numerical stability&lt;/a&gt;, and most general-purpose optimization packages are built to minimize functions rather than maximize them. Now the rest of the proof follows from basic algebraic manipulations.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; -\log L(\beta_0, \beta_1) \\
&amp; \quad \quad \Big \Updownarrow \\
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; -\log \left [ \prod_{i=1}^n  \exp \left[ - \frac{(y_i-(\beta_0 + \beta_1 x_i))^2}{2 \sigma^2 } \right ] \right ] \\
&amp; \quad \quad \Big \Updownarrow \\
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; \sum_{i=1}^n  - \log \left [ \exp \left[ - \frac{(y_i-\hat{y}_i)^2}{2 \sigma^2 } \right ] \right ] \\
&amp; \quad \quad \Big \Updownarrow \\
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; \frac{1}{2 \sigma^2 } \sum_{i=1}^n  (y_i-\hat{y}_i)^2 \\
&amp; \quad \quad \Big \Updownarrow \\
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; \sum_{i=1}^n  ( y_i - \hat{y}_i )^2\\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Where the final line follows since we can rescale the objective function by $1 / 2 \sigma^2$ without changing the values of $\beta_1$ and $\beta_0$ that are best fit to the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Notice!&lt;/strong&gt; The amount of noise &lt;strong&gt;&lt;em&gt;&lt;u&gt;does not affect&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; the best fit slope and intercept. That is, changing the value of $\sigma^2$ does not change the maximum likelihood estimates of $\beta_0$ and $\beta_1$.&lt;/p&gt;

&lt;h3 id=&quot;a-noise-model-for-pca&quot;&gt;A noise model for PCA&lt;/h3&gt;

&lt;p&gt;Linear regression seems to be taught to undergraduates as finding the line that minimizes the squared residuals. This makes sense to me — it is easy to teach and understand. We have now seen that this is the same as maximizing the likelihood under a &lt;em&gt;univariate&lt;/em&gt; (LINK) Gaussian noise model. It is really nice to know this connection because it helps to understand &lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_linear_model&quot;&gt;generalized linear models (GLMs)&lt;/a&gt;, which we’ve seen are a very flexible and useful extension to classic regression. However, I wouldn’t say the former interpretation of linear regression is wrong or inferior.&lt;/p&gt;

&lt;p&gt;I will now advance a similar argument for PCA. Classically, PCA is introduced as a technique that &lt;em&gt;maximizes variance&lt;/em&gt; or (equivalently) &lt;em&gt;minimizes squared residuals&lt;/em&gt; as I outlined in &lt;a href=&quot;http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/&quot;&gt;my last post&lt;/a&gt;. However, you can also think of PCA as a maximum likelihood model under an &lt;em&gt;isotropic&lt;/em&gt;, &lt;em&gt;multivariate&lt;/em&gt; Gaussian noise model (LINKS).&lt;/p&gt;

&lt;p&gt;In essence, PCA generalizes linear regression by assuming there is noise in both $x$ and $y$ (rather than just in $y$). Furthermore, the noise is &lt;em&gt;isotropic&lt;/em&gt;, meaning that $x$ and $y$ are equally noisy. For the two-dimensional case, the picture to have in mind is this:&lt;/p&gt;

&lt;p&gt;PICTURE&lt;/p&gt;

&lt;p&gt;Now let’s state this picture formally. Because $x$ and $y$ are equally noisy, there is no longer a solid distinction between the independent and dependent variables.  Thus, we will change notation so that all measured variables/datapoints are collected into a matrix $\mathbf{X}$. Each row of $\mathbf{X}$ is an observed datapoint, so for the 2-dimensional case sketched above $\mathbf{X}$ has two columns. Of course, we are interested in the higher-dimensional case, when $\mathbf{X}$ has many columns.&lt;/p&gt;

&lt;p&gt;The PCA model is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{X} = \mathbf{U} \mathbf{V}^T + \mathbf{E}&lt;/script&gt;

&lt;p&gt;where $\mathbf{E}$ is a Gaussian random matrix, with each element randomly drawn according to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{ij} \overset{\scriptsize\text{i.i.d.}}{\sim} \text{Normal}(0 \, , \, \sigma^2)&lt;/script&gt;

&lt;p&gt;We can also re-express this equation for a single row of $\mathbf{X}$. The PCA model for row $i$ of $\mathbf{X}$ is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}_{i:} \overset{\scriptsize\text{i.i.d.}}{\sim} \text{MvNormal} \left ( \hat{\mathbf{x}}_{i:} \, , \, \sigma^2 \mathbf{I} \right )&lt;/script&gt;

&lt;p&gt;Where the estimate is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{x}_{i:} = \sum_{r=1}^R u_{ir} \mathbf{v}_{:r}&lt;/script&gt;

&lt;p&gt;As before, our goal is to find model parameters that maximize the likelihood function. Here, the likelihood function is a product of multivariate Gaussians:&lt;/p&gt;

&lt;p&gt;EQUATION&lt;/p&gt;

&lt;p&gt;In fact, this is how experts viewed PCA until two papers by &lt;a href=&quot;http://papers.nips.cc/paper/1398-em-algorithms-for-pca-and-spca.pdf&quot;&gt;Roweis (1998)&lt;/a&gt; and &lt;a href=&quot;http://dx.doi.org/10.1111/1467-9868.00196&quot;&gt;Tipping \&amp;amp; Bishop (1999)&lt;/a&gt; drew the connection between PCA and a Gaussian noise model.&lt;/p&gt;

&lt;p&gt;Both authors insist on giving new names to their new model. Roweis calls it “Sensible PCA” and Tipping/Bishop call it “Probabilistic PCA”. I think the new names aren’t necessary. I particularly don’t like “Sensible PCA” as it suggests that PCA in the classic sense is &lt;em&gt;not sensible&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Opinions aside, what does PCA look like with noise? It turns out to actually looks a lot like the regression example except with Gaussian noise incorporated into $x$ as well as $y$. Because both variables contain noise, the distinction between dependent and independent variables disappears, so we change notation. Each datapoint is a vector in a high dimensional space, which we’ll denote $\mathbf{x}_i$. Our full dataset is collected into a matrix $\mathbf{X}$. Each $\mathbf{x}_i$ is a row of $\mathbf{X}$.&lt;/p&gt;

&lt;p&gt;PCA finds a low-dimensional space characterized by loadings $\mathbf{U}$ (a ) and components $V$&lt;/p&gt;

&lt;p&gt;Like the regression example, we still have a linear model since PCA identifies a low-dimensional linear space&lt;/p&gt;

&lt;p&gt;Both authors insist that their mode call PCA&lt;/p&gt;

&lt;p&gt;For some reason I don’t understand, people like to make a big&lt;/p&gt;

&lt;p&gt;So we have seen that minimizing the sum of squared residuals (probably how)&lt;/p&gt;

&lt;p&gt;The point of this post is to extend the idea of noise models to PCA and related models&lt;/p&gt;

&lt;!-- ### Probabilistic PCA $\iff$ Istropic Gaussian noise model 

The motivation of PCA is not to predict a set of dependent variable from independent variables. Instead, all observed variables are treated on an equal footing, and noise is present in all variables (not just the dependent variables). Following similar notation from the last post, let $\mathbf{a}_{i} = \\{ a_1, a_2, ..., a_p \\}$ denote the observation $i$; each observation consists of $p$ measured features. As always, we assume the data is mean-centered &amp;mdash; i.e. each feature has zero mean across the dataset.

PCA assumes that the data are described by a collection of $r$ latent factors. Each observation is formed by a linear combination of the $r$ factors (&quot;principal components&quot;) with weights $\mathbf{w}_i$ (&quot;loadings&quot;), plus a noise term $\boldsymbol{\xi} \in \mathbb{R}^p$.

$$ \mathbf{a} = w_1 \mathbf{c}_1 + w_2 \mathbf{c}_2 + ... + w_r \mathbf{c}_r + \boldsymbol{\xi}$$

We have again used similar notation so that the $k$th principal component is denoted by $\mathbf{c}_{k} = \\{ c_1, ..., c_p \\}$ and the loadings/weights are given by $\mathbf{w} = \\{ w_1, ..., w_r \\}$. This above equation describes a single observation. I abused notation by dropping the index $i$ to denote the $i$th observation. However, we will assume every observation is drawn $\small\text{i.i.d.}$ so this index is not too important. If you like, we can also represent the full dataset as matrix equation.&lt;a href=&quot;#f3b&quot; id=&quot;f3t&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt;

Our derivation of probabilistic PCA will assume the loadings are drawn from a standard, multivariate normal distribution and that $\boldsymbol{\xi}$ follows a zero-mean, isotropic Gaussian distribution:

$$ \mathbf{w} \overset{\scriptsize\text{i.i.d.}}{\sim} \mathcal{N}(0,I_r) \quad \quad \boldsymbol{\xi} \overset{\scriptsize\text{i.i.d.}}{\sim} \mathcal{N}(0,\sigma^2 I_p) $$

where $I_r$ denotes an $r \times r$ identity matrix, and $I_r$ denotes a $p \times p$ identity matrix. These assumptions have an important consequence and interpretation. Think of $\mathbf{w}$ and $\boldsymbol{\xi}$ as random Gaussian inputs to a fixed linear system (which is given by the components $\mathbf{c}_k$). Since these inputs are Gaussian, and the system is linear, then the output of the system (i.e. our data, $\mathbf{a}$) is also Gaussian distributed:

$$
\mathbf{a} \sim N(0, C C^T + \sigma^2 I_p )
$$

[Click here to see a derivation of this result](http://math.stackexchange.com/questions/332441/affine-transformation-applied-to-a-multivariate-gaussian-random-variable-what). As in the last post, the columns of $C \in \mathbb{R}^{p \times r}$ contain the principal components, $C = \[ \mathbf{c}_1 ... \mathbf{c}_r \]$. The vectors $\mathbf{a}$ and $\mathbf{w}$ are row vectors, see [\[3\]](#f3b).

Intuitively, this equation tells us that the inputs (spherical gaussian noise) are projected onto an $r$-dimensional subspace, and stretched/rotated by a linear transform,

$$
\mathbf{a} \sim N(0, C C^T + \sigma^2 I_p )
$$

Before digging deeper into the math, let&#39;s visualize this basic picture in two dimensions. This should be compared the graphical depiction of regression in figure 1.

PICTURE

LEGEND: Probabilistic PCA $p=2$ and $r=1$.

http://math.stackexchange.com/questions/332441/affine-transformation-applied-to-a-multivariate-gaussian-random-variable-what

To express this mathematically, we use the multivariate Normal probability distribution:

$$
\left[
\begin{array}{c}
    x_i \\ y_i
\end{array}
\right]
\sim
N \left ( w_i \left[ \begin{array}{c} c_x \\ c_y \end{array} \right],
\left [ \begin{array}{cc} \sigma^2 &amp; 0 \\ 0 &amp;\sigma^2 \end{array} \right ] \right )
$$

As in the linear regression warmup, we observe a bunch of datapoints $(x_i, y_i)$ and we want to find parameters that maximize the likelihood. We need to estimate two quantities, the direction of the line $[c_x c_y]$ and, for each datapoint, a scalar $w_i$ which tells us how far along that line that datapoint truly sits. The covariance matrix is diagonal and controlled by a single parameter $\sigma^2$ (the variance in $x$ and $y$); this corresponds to our assumption that noise is uncorrelated and equal in magnitude for $x$ and $y$.

ANOTHER PICTURE

Now that we&#39;ve visualized this for two dimensions, let&#39;s go to 

$$
\left[
\begin{array}{c}
    x_i \\ y_i
\end{array}
\right]
\sim
N \left (  \mathbf{w}_i C^T ,
\left [ \begin{array}{cc} \sigma^2 &amp; 0 \\ 0 &amp;\sigma^2 \end{array} \right ] \right )
$$

parameter $\sigma^2$ scales the noise, and 

$$
math
$$

define $[_x c_y]$

Mathematically, we can express this as follows

$$
\begin{aligned}
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; -\log \left [ \prod_{i=1}^n  \exp \left[ - \frac{(y_i-(\beta_0 + \beta_1 x_i))^2}{2 \sigma^2 } \right ] \right ] \\
&amp; \Big \Updownarrow \\
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; \sum_{i=1}^n  - \log \left [ \exp \left[ - \frac{(y_i-(\beta_0 + \beta_1 x_i))^2}{2 \sigma^2 } \right ] \right ] \\
&amp; \Big \Updownarrow \\
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; \frac{1}{2 \sigma^2 } \sum_{i=1}^n  (y_i-(\beta_0 + \beta_1 x_i))^2 \\
&amp; \Big \Updownarrow \\
&amp; \underset{\beta_0, \beta_1}{\text{minimize}}
&amp; &amp; \sum_{i=1}^n  (y_i - \hat{y}_i)^2 \\
\end{aligned}
$$

### What is the difference between probabilistic PCA and PCA?

Great question. Actually, if I may inject a short opinionated rant, I think this is explained terribly in pretty much every published paper I&#39;ve read.&lt;a href=&quot;#f4b&quot; id=&quot;f4t&quot;&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/a&gt; The best explanation in my view is [this stackexchange answer](http://stats.stackexchange.com/questions/123063/is-there-any-good-reason-to-use-pca-instead-of-efa/123136#123136). Also see the explanation in [Kevin Murphy&#39;s textbook on Machine Learning](https://www.cs.ubc.ca/~murphyk/MLbook/).

The short answer is that they are basically the same. They don&#39;t deserve different names. It just adds confusion.

The longer answer is that both pPCA and PCA recover the same linear subspace of $W$ and $C$, but pPCA shrinks the estimate of the loadings (i.e. $\mathbf{w}_i$ for each observation) towards zero.

### Logistic PCA, Robust PCA, etc.

In the previous post I enumerated several el

### Noise in pPCA and Factor Analysis

Factor analysis makes a weaker assumption that the noise is Gaussian and uncorrelated.

&gt; &quot;Principal Component Analysis&quot; is a dimensionally invalid method that gives people a delusion that they are doing something useful with their data. If you change the units that one of the variables is measured in, it will change all the &quot;principal components&quot;! It&#39;s for that reason that I made no mention of PCA in my book. I am not a slavish conformist, regurgitating whatever other people think should be taught. I think before I teach. David J C MacKay.

http://blog.explainmydata.com/2012/07/should-you-apply-pca-to-your-data.html

### Factor Analysis vs. PCA on synthetic data



### An opinionated conclusion: stop over-emphasizing probability

I&#39;m sure if you live and breath Bayesian statistics the notation and language.

, but the notation is bloated and pretty much terrible in every other way you can think of. Even if notation was equal (its really not) the optimization viewpoint is conceptually much simpler in my view. Undergraduates in biology and social sciences are taught to &quot;minimizing (squared) residuals&quot; and not &quot;maximizing likelihood under a Gaussian&quot; --&gt;

&lt;h4 id=&quot;other-references&quot;&gt;Other references:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://stats.stackexchange.com/questions/94048/pca-and-exploratory-factor-analysis-on-the-same-dataset-differences-and-similar&quot;&gt;StackExchange: PCA vs. Factor Analysis on the same dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=Demystifying Factor Analysis&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2016/04/20/pca-appendix/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2016/04/20/pca-appendix/&amp;amp;title=Demystifying Factor Analysis&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2016/04/20/pca-appendix/&amp;amp;t=Demystifying Factor Analysis&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2016/04/20/pca-appendix/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;strong&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Follow:
    &lt;/strong&gt;

    &lt;a href=&quot;http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Follow on Feedly&quot; class=&quot;btn-social btn-rss&quot;&gt;
        &lt;i class=&quot;fa fa-rss&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 class=&quot;no_toc&quot; id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f1t&quot; id=&quot;f1b&quot;&gt;&lt;b&gt;[1]&lt;/b&gt;&lt;/a&gt; Note that the likelihood treats $\mathbf{y}$ and $\mathbf{x}$ (vectors containing $x_i$ and $y_i$ for each observation) as constant variables.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f2t&quot; id=&quot;f2b&quot;&gt;&lt;b&gt;[2]&lt;/b&gt;&lt;/a&gt; First, note that $\log(x)$ is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Monotonic_function&quot;&gt;monotonic function&lt;/a&gt;. If $x^{*}$ is the value of $x$ that maximizes $f(x)$, then $x^{*}$ also maximizes $g(f(x))$ for any monotonic function $g$. 
it is generally nicer to work with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood&quot;&gt;log-likelihood function&lt;/a&gt; rather than directly with the likelihood function. This is perhaps especially true on computers, since the likelihood function involves the multiplication of many probabilities (numbers between 0 and 1) resulting in incredibly small numbers (think $10^{-n}$ for $n$ datapoints), which are difficult to represent on computers. All of these basic properties are good to know about. See more details &lt;a href=&quot;https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution&quot;&gt;here&lt;/a&gt; and refer to g optimization and statistics textbooks, see e.g. https://web.stanford.edu/~boyd/cvxbook/.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Roweis refers to probabilistic PCA as &lt;em&gt;sensible PCA&lt;/em&gt; in his paper. He published concurrently with Tipping and Bishop, and their name has mostly won out.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Applying $\log$ doesn’t change the optimization problem because it is a monotonic function. Maximizing a function is equivalent to minimizing that function multiplied by $-1$.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f4t&quot; id=&quot;f4b&quot;&gt;&lt;b&gt;[4]&lt;/b&gt;&lt;/a&gt; The matrix equation matches my last blog post, $ A = W C^T + R $. In this post $\mathbf{a}$ is a row in $A$, $\mathbf{w}$ is a row in $W$, and $\mathbf{c}$ is a column in $C$ (i.e. a row in $C^T$). The matrix $R \in \mathbb{R}^{n \times p}$ is the noise term: each $\boldsymbol{\xi}$ is a row in $R$.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f5t&quot; id=&quot;f5b&quot;&gt;&lt;b&gt;[5]&lt;/b&gt;&lt;/a&gt; It’s possible that there are more good explanation out there. Maybe I’ve just been unlucky with the papers I’ve read.
&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Everything you did and didn't know about PCA</title>
   <link href="http://alexhwilliams.info/2016/03/27/pca/"/>
   <updated>2016-03-27T00:00:00-07:00</updated>
   <id>http://alexhwilliams.info/2016/03/27/pca</id>
   <content type="html">&lt;h3 class=&quot;no_toc&quot; id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#intro&quot; id=&quot;markdown-toc-intro&quot;&gt;Intro&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#notation&quot; id=&quot;markdown-toc-notation&quot;&gt;Notation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#everything-you-did-know-or-do-now&quot; id=&quot;markdown-toc-everything-you-did-know-or-do-now&quot;&gt;Everything you did know (or do now)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#an-alternative-optimization-problem&quot; id=&quot;markdown-toc-an-alternative-optimization-problem&quot;&gt;An alternative optimization problem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#generalizations-of-pca-sparse-features-loss-functions&quot; id=&quot;markdown-toc-generalizations-of-pca-sparse-features-loss-functions&quot;&gt;Generalizations of PCA: sparse features, loss functions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#some-things-you-maybe-didnt-know-about-pca&quot; id=&quot;markdown-toc-some-things-you-maybe-didnt-know-about-pca&quot;&gt;Some things you maybe didn’t know about PCA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tldr&quot; id=&quot;markdown-toc-tldr&quot;&gt;TL;DR&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#further-reading&quot; id=&quot;markdown-toc-further-reading&quot;&gt;Further Reading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intro&quot;&gt;Intro&lt;/h3&gt;

&lt;p&gt;Many scientists are familiar with organizing and handling data in 2D tables. For example, we might record the mRNA expression level of $p$ genes in $n$ tissue samples. We might store these data in a $n \times p$ matrix, where each row corresponds to a sample, and each column corresponds to a gene. Principle components analysis (PCA) is a standard way to reduce the dimension $p$ (which can be quite large) to something more manageable.&lt;/p&gt;

&lt;p&gt;While it is quite common for biologists to apply PCA to their data, it is less common for them to really understand the mechanics and assumptions implicit in this analysis. Opening up the black box on a statistical technique is worthwhile in and of itself, but the real reason I’m motivated to write this is the number of seriously cool and super useful extensions/variations of PCA (e.g., &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-negative_matrix_factorization&quot;&gt;Non-negative matrix factorization&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Sparse_PCA&quot;&gt;Sparse PCA&lt;/a&gt;, &lt;a href=&quot;http://dx.doi.org/10.1137/07070111X&quot;&gt;Tensor Decompositions&lt;/a&gt;), which will have a growing impact on modern neuroscience and biology. I want to blog about techniques of this flavor for the next few posts.&lt;/p&gt;

&lt;p&gt;If you are completely unfamiliar with PCA, there &lt;a href=&quot;http://www.stat.cmu.edu/~cshalizi/350/lectures/13/lecture-13.pdf&quot;&gt;are&lt;/a&gt; &lt;a href=&quot;https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf&quot;&gt;some&lt;/a&gt; &lt;a href=&quot;http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf&quot;&gt;great&lt;/a&gt; &lt;a href=&quot;http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html&quot;&gt;explanations&lt;/a&gt; &lt;a href=&quot;http://jeremykun.com/2012/06/28/principal-component-analysis/&quot;&gt;online&lt;/a&gt; that you should reference concurrently with reading this post. While these materials are quite good, many of them don’t explain PCA in a way that naturally lends itself to more complex (but fun and useful!) extensions.&lt;/p&gt;

&lt;p&gt;I aimed to be as pedagogical as possible in this post, but you will need to be familiar with some linear algebra to follow along. You don’t need to know what an eigenvalue is (though it will help you understand certain results more deeply), but &lt;a href=&quot;https://www.youtube.com/watch?v=kT4Mp9EdVqs&quot;&gt;basic matrix operations&lt;/a&gt; are needed. Also, go teach yourself what an eigenvalue is, it’s good for you.&lt;/p&gt;

&lt;p&gt;If you think you’re already a PCA whiz and don’t care for the background stuff, you can skip to &lt;a href=&quot;#some-things-you-maybe-didnt-know-about-pca&quot;&gt;some things you maybe didn’t know about PCA&lt;/a&gt; or just &lt;a href=&quot;#tldr&quot;&gt;read the tl;dr&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;notation&quot;&gt;Notation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Boldface capital letters = matrices (e.g. $\mathbf{X}$)&lt;/li&gt;
  &lt;li&gt;Boldface lowercase letters = column vectors (e.g. $\mathbf{u}$)&lt;/li&gt;
  &lt;li&gt;Non-boldface letters = scalars (e.g. $i$, $j$, $I$, $J$, $x_{ij}$)&lt;/li&gt;
  &lt;li&gt;Note that scalars are often lowercase, but can be uppercase&lt;/li&gt;
  &lt;li&gt;Superscript $T$ = &lt;a href=&quot;https://en.wikipedia.org/wiki/Transpose&quot;&gt;transpose operator&lt;/a&gt; (e.g. $\mathbf{c}^T$, $\mathbf{X}^T$)&lt;/li&gt;
  &lt;li&gt;Subscripts = element position (e.g. $x_{ij}$ is the scalar in row $i$ column $j$ of the matrix $\mathbf{X}$)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Throughout, we suppose that we have a matrix of data $\mathbf{X}$ with dimensions $I \times J$, where $I$ is the number of observations, and $J$ is the number of features that are measured and associated with each observation. For example, we might record $I$ neurons and estimate their firing rate at $J$ timepoints; or we might measure the expression of $J$ genes across $I$ cells.&lt;/p&gt;

&lt;p&gt;We index into $\mathbf{X}$ using a lowercase index variable, i.e., $x_{ij}$ where $i$ and $j$ respectively range from $1$ to $I$ and $1$ to $J$.&lt;/p&gt;

&lt;h3 id=&quot;everything-you-did-know-or-do-now&quot;&gt;Everything you did know (or do now)&lt;/h3&gt;

&lt;p&gt;PCA tries to find “components” that capture the maximal variance within the data. For three dimensional data, this is the basic image you may have come across:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Classic view of PCA.. &lt;/b&gt;Each blue point corresponds to an observation (a row of $\mathbf{X}$). There are $n=20$ observations, each with $p=3$ features. In this schematic, PCX reduces the dimensionality from three to $r=2$. In particular, it finds a pair of orthogonal vectors (red arrows) that define a lower-dimensional space (grey plane) which captures as much variance as possible from the original dataset.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca/pca_classic.png&quot; alt=&quot;Each blue point corresponds to an observation (a row of $\mathbf{X}$). There are $n=20$ observations, each with $p=3$ features. In this schematic, PCX reduces the dimensionality from three to $r=2$. In particular, it finds a pair of orthogonal vectors (red arrows) that define a lower-dimensional space (grey plane) which captures as much variance as possible from the original dataset.&quot; width=&quot;300px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Classic view of PCA.&lt;/b&gt;
	Each blue point corresponds to an observation (a row of $\mathbf{X}$). There are $n=20$ observations, each with $p=3$ features. In this schematic, PCX reduces the dimensionality from three to $r=2$. In particular, it finds a pair of orthogonal vectors (red arrows) that define a lower-dimensional space (grey plane) which captures as much variance as possible from the original dataset.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Now let’s express the above picture mathematically. Assume that each column of $\mathbf{X}$ has been mean subtracted so that the datapoints are centered around the origin. Then finding the direction of maximal variance (i.e. the &lt;em&gt;first&lt;/em&gt; principal component) corresponds to solving the following optimization problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underset{\mathbf{c}}{\text{maximize}}
&amp; &amp; \mathbf{c}^T \mathbf{X}^T \mathbf{X} \mathbf{c} \\
&amp; \text{subject to}
&amp; &amp; \mathbf{c}^T \mathbf{c} = 1
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Why does $\mathbf{c}^T \mathbf{X}^T \mathbf{X} \mathbf{c}$ measure the variance we want to maximize? Let $\mathbf{w} = \mathbf{X} \mathbf{c}$, which is the &lt;a href=&quot;https://www.youtube.com/watch?v=27vT-NWuw0M&quot;&gt;projection of each datapoint&lt;/a&gt; onto the top principal component (since we imposed $\mathbf{c}^T \mathbf{c} = 1$). Because the data have been mean subtracted, the variance of the projected data is $\mathbf{w}^T \mathbf{w}$, which equals the objective function $\mathbf{c}^T \mathbf{X}^T \mathbf{X} \mathbf{c}$.&lt;a href=&quot;#f1b&quot; id=&quot;f1t&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; The vector $\mathbf{c}$ is the top principal component, and the vector $\mathbf{w}$ contains the “loadings” for each observation along this axis.&lt;/p&gt;

&lt;p&gt;There are a few ways to solve this optimization problem to determine $\mathbf{c}$ and $\mathbf{w}$. The classic approach would be to compute the eigenvalues of $\mathbf{X}^T \mathbf{X}$ (the &lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance_matrix&quot;&gt;covariance matrix&lt;/a&gt; with dimensions $p \times p$) and set $\mathbf{c}$ to the eigenvector associated with the largest eigenvalue.&lt;a href=&quot;#f2b&quot; id=&quot;f2t&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt; This is more-or-less what happens under the hood when you call pca() in MATLAB or python — the eigendecomposition of the covariance matrix is computed via the &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;singular value decomposition (SVD)&lt;/a&gt;. It turns out that this approach does not work for tensors, matrices with incomplete data, or many other interesting cases.&lt;/p&gt;

&lt;p&gt;Let’s assume that we solve the optimization problem (1) by some method. Then our best approximation of the data is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Outer_product&quot;&gt;outer product&lt;/a&gt; of $\mathbf{w}$ and $\mathbf{c}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{X} \approx \mathbf{w} \mathbf{c}^T \quad \text{or} \quad  x_{ij} \approx w_i c_j&lt;/script&gt;

&lt;p&gt;This is called a rank-one reconstruction of the data because $\mathbf{w} \mathbf{c}^T$ produces a matrix with rank=1 (&lt;a href=&quot;https://www.youtube.com/watch?v=JUgrBkPteTg&quot;&gt;&lt;em&gt;click here for explanation of matrix rank&lt;/em&gt;&lt;/a&gt;). Visually, our reconstruction looks something like this:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Example reconstruction of data with 1 principal component.. &lt;/b&gt;An example data matrix (&lt;i&gt;left&lt;/i&gt;) with $n=12$ observations and $p=8$ features is approximated by the outer product $\mathbf{w} \mathbf{c}^T$ (&lt;i&gt;middle&lt;/i&gt;) which produces a rank-one matrix (&lt;i&gt;right&lt;/i&gt;). Note $\mathbf{w}$ is labeled as &lt;i&gt;loadings&lt;/i&gt; and $\mathbf{c}^T$ is labeled as &lt;i&gt;component&lt;/i&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca/rank_one.png&quot; alt=&quot;An example data matrix (&amp;lt;i&amp;gt;left&amp;lt;/i&amp;gt;) with $n=12$ observations and $p=8$ features is approximated by the outer product $\mathbf{w} \mathbf{c}^T$ (&amp;lt;i&amp;gt;middle&amp;lt;/i&amp;gt;) which produces a rank-one matrix (&amp;lt;i&amp;gt;right&amp;lt;/i&amp;gt;). Note $\mathbf{w}$ is labeled as &amp;lt;i&amp;gt;loadings&amp;lt;/i&amp;gt; and $\mathbf{c}^T$ is labeled as &amp;lt;i&amp;gt;component&amp;lt;/i&amp;gt;&quot; width=&quot;600px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Example reconstruction of data with 1 principal component.&lt;/b&gt;
	An example data matrix (&lt;i&gt;left&lt;/i&gt;) with $n=12$ observations and $p=8$ features is approximated by the outer product $\mathbf{w} \mathbf{c}^T$ (&lt;i&gt;middle&lt;/i&gt;) which produces a rank-one matrix (&lt;i&gt;right&lt;/i&gt;). Note $\mathbf{w}$ is labeled as &lt;i&gt;loadings&lt;/i&gt; and $\mathbf{c}^T$ is labeled as &lt;i&gt;component&lt;/i&gt;
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Most data can’t be well-described by a single principal component. Typically, we compute multiple principal components by computing all eigenvectors of $\mathbf{X}^T \mathbf{X}$ and ranking them by their eigenvalues. This can be visualized by a &lt;em&gt;scree plot&lt;/em&gt;, which plots the variance explained by each successive principal component. People may have told you to look for the “knee” or inflection point in the scree plot to determine the number of components to keep (the rest are noise). &lt;em&gt;&lt;strong&gt;Spoiler alert:&lt;/strong&gt; you can do better than this, and we’ll see how by the end of the post.&lt;/em&gt;&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Scree plot.. &lt;/b&gt;Principal components are ranked by the amount of variance they capture in the original dataset, a scree plot can provide some sense of how many components are needed.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca/scree.png&quot; alt=&quot;Principal components are ranked by the amount of variance they capture in the original dataset, a scree plot can provide some sense of how many components are needed.&quot; width=&quot;300px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Scree plot.&lt;/b&gt;
	Principal components are ranked by the amount of variance they capture in the original dataset, a scree plot can provide some sense of how many components are needed.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;We can organize the top $r$ principal components into a matrix $C = [ \mathbf{c}_1, \mathbf{c}_2, … , \mathbf{c}_r ]$ and the loading weights into $W = [\mathbf{w}_1, \mathbf{w}_2, … , \mathbf{w}_r ]$. Our reconstruction of the data is now a sum of $r$ outer products:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{X} \approx \sum_{k=1}^r \mathbf{w}_k \mathbf{c}_k^T \quad \text{or} \quad \mathbf{X} \approx W C^T&lt;/script&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Example reconstruction of data with 3 principal components.. &lt;/b&gt;A data matrix (&lt;i&gt;left&lt;/i&gt;) is approximated by the product of a $n \times r$ matrix and a $r \times p$ matrix (i.e. $W C^T$). This product is at most a rank-$r$ matrix (in this example, $r=3$). Each paired column of $W$ and row of $C^T$ form an outer product, so the full reconstruction can also be thought of as a sum of $r$ rank-one matrices.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca/pca_3.png&quot; alt=&quot;A data matrix (&amp;lt;i&amp;gt;left&amp;lt;/i&amp;gt;) is approximated by the product of a $n \times r$ matrix and a $r \times p$ matrix (i.e. $W C^T$). This product is at most a rank-$r$ matrix (in this example, $r=3$). Each paired column of $W$ and row of $C^T$ form an outer product, so the full reconstruction can also be thought of as a sum of $r$ rank-one matrices.&quot; width=&quot;600px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Example reconstruction of data with 3 principal components.&lt;/b&gt;
	A data matrix (&lt;i&gt;left&lt;/i&gt;) is approximated by the product of a $n \times r$ matrix and a $r \times p$ matrix (i.e. $W C^T$). This product is at most a rank-$r$ matrix (in this example, $r=3$). Each paired column of $W$ and row of $C^T$ form an outer product, so the full reconstruction can also be thought of as a sum of $r$ rank-one matrices.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Each row of the components matrix, $C^T$, is a principal component, which is a mixture of the $p$ features measured in the dataset. Each row of the loadings matrix, $W$, provides a recipe for combining the $r$ components to approximately reconstruct a single datapoint (i.e. a row in $\mathbf{X}$). For example, if row $i$ of $W$ was $[1, 2, -1]$, then our estimate for row $i$ of $\mathbf{X}$ would be: $ \mathbf{c}_1 + 2 \mathbf{c}_2 - \mathbf{c}_3 $ where the $\mathbf{c}$’s are the top three components (the rows of $C^T$). That is, our reconstruction for the data at row $i$, column $j$ is simply a linear combination of the components:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{ij} \approx \sum_{k=1}^r W_{ik} C_{jk}&lt;/script&gt;

&lt;h3 id=&quot;an-alternative-optimization-problem&quot;&gt;An alternative optimization problem&lt;/h3&gt;

&lt;p&gt;As discussed above, a classic perspective is that PCA finds a set of directions (technically, a linear subspace) that maximizes the variance of the data once it is projected into that space. It turns out that this is equivalent to finding a linear subspace that minimizes the distance of the projection in a least-squares sense.&lt;a href=&quot;#f3b&quot; id=&quot;f3t&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Two equivalent views of principal component analysis.. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca/pca_two_views.png&quot; alt=&quot;&quot; width=&quot;600px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Two equivalent views of principal component analysis.&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In this second perspective on PCA, we can find the top $r$ principal components $C$ by solving:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underset{C}{\text{minimize}}
&amp; &amp; \lVert \mathbf{X} - \mathbf{X} C C^T \lVert_F^2 \\
&amp; \text{subject to}
&amp; &amp; C^T C = I
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;As before, the loadings are a $n \times r$ matrix holding the projected data ($W = \mathbf{X} C$), our reconstruction is a rank-$r$ matrix ($\mathbf{X} \approx \mathbf{X} C C^T = W C^T$). Above, $\lVert \cdot \lVert_F^2$ denotes the squared &lt;a href=&quot;http://mathworld.wolfram.com/FrobeniusNorm.html&quot;&gt;Frobenius norm&lt;/a&gt;, which sounds fancy,&lt;a href=&quot;#f4b&quot; id=&quot;f4t&quot;&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/a&gt; but it is just the sum of squared residuals:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert \mathbf{X} - W C^T \lVert_F^2 = \sum_{i=1}^n \sum_{j=1}^p (x_{ij} - \sum_{k=1}^K W_{ik} C_{jk})^2&lt;/script&gt;

&lt;p&gt;Why is minimizing squared residuals equivalent to maximizing variance? Consider a datapoint $\mathbf{a}_i$ (row $i$ of $\mathbf{X}$). Then the contribution of that datapoint to the variance is $\mathbf{a}_i^T \mathbf{a}_i$, or equivalently the squared &lt;a href=&quot;https://en.wikipedia.org/wiki/Euclidean_distance&quot;&gt;Euclidean length&lt;/a&gt; $\lVert \mathbf{a}_i \lVert^2_2$. Applying the Pythagorean theorem shows that this total variance equals the sum of variance lost (the squared residual) and variance remaining. Thus, it is equivalent to either maximize remaining variance or minimize lost variance to find the principal components. The figure below visualizes this for 2 dimensions:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Maximizing variance in principal component space is equivalent to minimizing least-squares reconstruction error.. &lt;/b&gt;Consider a datapoint $\mathbf{a}_i$ (row $i$ of the data matrix $\mathbf{X}$). Assuming the data are mean-centered, the projection of $\mathbf{a}_i$ onto the principal components relates the remaining variance to the squared residual by the Pythagorean theorem. Choosing the components to maximize variance is the same as choosing them to minimize the squared residuals.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca/projection_intuition.png&quot; alt=&quot;Consider a datapoint $\mathbf{a}_i$ (row $i$ of the data matrix $\mathbf{X}$). Assuming the data are mean-centered, the projection of $\mathbf{a}_i$ onto the principal components relates the remaining variance to the squared residual by the Pythagorean theorem. Choosing the components to maximize variance is the same as choosing them to minimize the squared residuals.&quot; width=&quot;600px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Maximizing variance in principal component space is equivalent to minimizing least-squares reconstruction error.&lt;/b&gt;
	Consider a datapoint $\mathbf{a}_i$ (row $i$ of the data matrix $\mathbf{X}$). Assuming the data are mean-centered, the projection of $\mathbf{a}_i$ onto the principal components relates the remaining variance to the squared residual by the Pythagorean theorem. Choosing the components to maximize variance is the same as choosing them to minimize the squared residuals.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;generalizations-of-pca-sparse-features-loss-functions&quot;&gt;Generalizations of PCA: sparse features, loss functions&lt;/h3&gt;

&lt;p&gt;Thinking about PCA as minimizing reconstruction error is useful because it draws a connection to statistical regression.&lt;a href=&quot;#f5b&quot; id=&quot;f5t&quot;&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/a&gt; Simple least-squares linear regression has been extended and adapted to a wide variety of statistical problems (&lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_linear_model&quot;&gt;&lt;em&gt;see GLMs&lt;/em&gt;&lt;/a&gt;), and we can leverage this research framework and perspective to come up with more specialized versions of PCA. This general framework has been developed my a number of papers (&lt;a href=&quot;http://papers.nips.cc/paper/2078-a-generalization-of-principal-components-analysis-to-the-exponential-family.pdf&quot;&gt;Collins, 2001&lt;/a&gt;; &lt;a href=&quot;http://papers.nips.cc/paper/2144-generalized2-linear2-models.pdf&quot;&gt;Gordon, 2004&lt;/a&gt;; &lt;a href=&quot;https://courses2.cit.cornell.edu/mru8/doc/udell15_thesis.pdf&quot;&gt;Udell, 2015&lt;/a&gt;). Below is a whirlwind tour:&lt;/p&gt;

&lt;p&gt;We will start with &lt;strong&gt;quadratically regularized PCA&lt;/strong&gt;, which is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Tikhonov_regularization&quot;&gt;ridge regression&lt;/a&gt;. The basic idea is to penalize the squared &lt;a href=&quot;https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm&quot;&gt;Euclidean length (L2 norm)&lt;/a&gt; of the rows of $W$ and $C$ so that they don’t get too large:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underset{W,C}{\text{minimize}}
&amp; &amp; \lVert \mathbf{X} - W C^T \lVert_F^2 ~+~ \gamma \sum_{i=1}^n \lVert \mathbf{w}_i \lVert_2^2 ~+~ \gamma \sum_{j=1}^p \lVert \mathbf{c}_j \lVert_2^2 \\
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&quot;https://courses2.cit.cornell.edu/mru8/doc/udell15_thesis.pdf&quot;&gt;Madeleine Udell’s thesis&lt;/a&gt; shows that the answer to this problem is very similar to classic PCA and can be solved analytically using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;singular value decomposition (SVD)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Interestingly, the rest of the PCA variants listed in this post cannot be analytically solved. In fact, PCA and quadratically-regularized PCA are quite &lt;a href=&quot;https://youtu.be/7clclzk3hrw?t=29m40s&quot;&gt;&lt;em&gt;special cases of nonconvex optimization problems that we can solve exactly&lt;/em&gt;&lt;/a&gt;. In practice, we can still fit the rest of these models using standard techniques like gradient descent. Even better, we can exploit the fact that &lt;strong&gt;&lt;em&gt;these optimization problems are &lt;a href=&quot;http://www2.math.uni-wuppertal.de/~klamroth/publications/gopfkl07.pdf&quot;&gt;biconvex&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;. That is, if we treat $W$ as a fixed constant and optimize over $C$ then the problem is &lt;a href=&quot;https://en.wikipedia.org/wiki/Convex_optimization&quot;&gt;convex&lt;/a&gt;, and vice versa. This suggests the &lt;a href=&quot;http://arxiv.org/pdf/1312.0925v3.pdf&quot;&gt;alternating minimization algorithm&lt;/a&gt; which can work very well in practice. In rough pseudocode:&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Alternating minimization:&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
1      choose initial starting points $W^{(0)}$ and $C^{(0)}$&lt;br /&gt;&lt;br /&gt;
2      $n \leftarrow 0$&lt;br /&gt;&lt;br /&gt;
3      &lt;strong&gt;while&lt;/strong&gt; not converged&lt;br /&gt;&lt;br /&gt;
4           $W^{(n+1)} \leftarrow $ minimize over $W$ while holding $C = C^{(n)}$ constant.&lt;br /&gt;&lt;br /&gt;
5           $C^{(n+1)} \leftarrow $ minimize over $C$ while holding $W = W^{(n+1)}$ constant.&lt;br /&gt;&lt;br /&gt;
6           $n \leftarrow n+1$&lt;br /&gt;&lt;br /&gt;
7      &lt;strong&gt;end while&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;Again, the idea here is that the sub-problems (4) and (5) are easy to optimize because they are convex. It isn’t necessary to minimize the sub-problems to completion, in fact it can work better to take just take alternating gradient steps for each sub-problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sparse PCA.&lt;/strong&gt; If we substitute a &lt;a href=&quot;https://en.wikipedia.org/wiki/Taxicab_geometry&quot;&gt;L1 norm&lt;/a&gt; penalty for the L2 norm regularization we encounter a form of &lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/spc_jcgs.pdf&quot;&gt;sparse PCA&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underset{W,C}{\text{minimize}}
&amp; &amp; \lVert \mathbf{X} - W C^T \lVert_F^2 ~+~ \gamma \sum_{i=1}^n \lVert \mathbf{w}_i \lVert_1 ~+~ \gamma \sum_{j=1}^p \lVert \mathbf{c}_j \lVert_1 \\
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;This problem is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot;&gt;LASSO&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Elastic_net_regularization&quot;&gt;elastic net&lt;/a&gt; regression. Typically, this penalty/regularization choice causes the best $W$ and $C$ to be sparse (i.e. have many zero entries). This can be helpful for interpretation. For example, if we have a gene expression dataset then we may want to limit ourselves so that each component — column of $C$ — only contains a handful of genes, rather than a large combination of all genes (which can be quite large). This is demonstrated in the figure below from &lt;a href=&quot;http://dx.doi.org/10.1137/050645506&quot;&gt;D’Aspremont et al. (2007)&lt;/a&gt;:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Sparse PCA produces similar results to PCA, but with simpler and more interpretable components.. &lt;/b&gt;500 genes were measured for a large number of samples. The factors $f_1$, $f_2$, $f_3$ obtained by traditional PCA each use all 500 genes (&lt;i&gt;left&lt;/i&gt;). The sparse factors $g_1$, $g_2$, and $g_3$ on the right together involve only 14 genes, which can be useful for developing parsimonious hypotheses and future experiments. Both PCA and Sparse PCA separate the three tissue types that were measured; the color of each datapoint corresponds to the tissue type. The separation is slighly larger for PCA, but is less interpretable. Figure reproduced from &lt;a href=&#39;http://dx.doi.org/10.1137/050645506&#39;&gt;D&#39;Aspremont et al. (2007)&lt;/a&gt;, data from Iconix Pharmaceuticals.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca/sparse_pca.png&quot; alt=&quot;500 genes were measured for a large number of samples. The factors $f_1$, $f_2$, $f_3$ obtained by traditional PCA each use all 500 genes (&amp;lt;i&amp;gt;left&amp;lt;/i&amp;gt;). The sparse factors $g_1$, $g_2$, and $g_3$ on the right together involve only 14 genes, which can be useful for developing parsimonious hypotheses and future experiments. Both PCA and Sparse PCA separate the three tissue types that were measured; the color of each datapoint corresponds to the tissue type. The separation is slighly larger for PCA, but is less interpretable. Figure reproduced from &amp;lt;a href=&#39;http://dx.doi.org/10.1137/050645506&#39;&amp;gt;D&#39;Aspremont et al. (2007)&amp;lt;/a&amp;gt;, data from Iconix Pharmaceuticals.&quot; width=&quot;600px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Sparse PCA produces similar results to PCA, but with simpler and more interpretable components.&lt;/b&gt;
	500 genes were measured for a large number of samples. The factors $f_1$, $f_2$, $f_3$ obtained by traditional PCA each use all 500 genes (&lt;i&gt;left&lt;/i&gt;). The sparse factors $g_1$, $g_2$, and $g_3$ on the right together involve only 14 genes, which can be useful for developing parsimonious hypotheses and future experiments. Both PCA and Sparse PCA separate the three tissue types that were measured; the color of each datapoint corresponds to the tissue type. The separation is slighly larger for PCA, but is less interpretable. Figure reproduced from &lt;a href=&quot;http://dx.doi.org/10.1137/050645506&quot;&gt;D&#39;Aspremont et al. (2007)&lt;/a&gt;, data from Iconix Pharmaceuticals.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Non-negative matrix factorization.&lt;/strong&gt; Another popular dimensionality technique is &lt;a href=&quot;http://dx.doi.org/10.1038/44565&quot;&gt;non-negative matrix factorization (NMF)&lt;/a&gt;, which is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-negative_least_squares&quot;&gt;non-negative least-squares&lt;/a&gt; regression. Again, this looks very similar PCA, the only difference being that we constrain/demand each element of $W$ and $C$ to be nonnegative:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underset{W,C}{\text{minimize}}
&amp; &amp; \lVert \mathbf{X} - W C^T \lVert_F^2 \\
&amp; \text{subject to}
&amp; &amp; W_{ik} \geq 0,~ C_{jk} \geq 0
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;This typically only makes sense when your data is also nonnegative, $x_{ij} \geq 0$, since the reconstructed estimate for each datapoint is clearly nonnegative when both $W$ and $C$ are nonnegative. Like sparse PCA, NMF can lead to a more interpretable dimensionality reduction since it forces only additive, not subtractive, combinations of the components. (PCA reconstructs the dataset by both additive and subtractive combinations of components.) NMF is an central part of the calcium imaging analysis pipeline recently published by Liam Paninski’s group &lt;a href=&quot;http://dx.doi.org/10.1016/j.neuron.2015.11.037&quot;&gt;(Pnevmatikakis et al., 2016)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Logistic PCA.&lt;/strong&gt; We can also replace squared error with different loss functions. For example, suppose you sequence the genomes of $n$ patients and check $p$ nucleotide sites for mutations ($x_{ij} = 1$ if patient $i$ has a mutation at site $j$, and $x_{ij} = -1$ if there is not mutation). Because your data is &lt;em&gt;binary&lt;/em&gt; you might use &lt;a href=&quot;http://research.microsoft.com/en-us/um/cambridge/events/aistats2003/proceedings/119.pdf&quot;&gt;logistic PCA&lt;/a&gt;, which is similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;logistic regression&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation}
\begin{aligned}
&amp; \underset{W,C}{\text{minimize}}
&amp; &amp; \sum_{i=1}^n \sum_{j=1}^p  \log ( 1 + \exp(-x_{ij} \cdot \sum_{k=1}^r W_{ik} C_{jk} )) \\
\end{aligned}
\end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;Why use logistic PCA? When we have binary data, modeling the output as a linear combination of factors/components doesn’t make a whole lot of sense: the data are either $x_{ij} = \{+1,-1\}$, but $\mathbf{w}^T_i \mathbf{c}_j$ could be much larger or smaller than these bounds. For a longer explanation, read up on &lt;a href=&quot;http://stats.stackexchange.com/questions/29325/what-is-the-difference-between-linear-regression-and-logistic-regression&quot;&gt;when/why to use logistic regression instead of linear regression&lt;/a&gt; — the reasoning is exactly analogous. It is also informative to compare classic to logistic PCA on simulated binary data, as shown in the plot below:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Logistic PCA can outperform classic PCA on binary data.. &lt;/b&gt;See &lt;a href=&#39;/itsneuronalblog/code/pca/lgc_pca.jl&#39; target=&#39;_blank&#39;&gt;Julia code here&lt;/a&gt; to reproduce this figure.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/pca/logistic_pca.png&quot; alt=&quot;See &amp;lt;a href=&#39;/itsneuronalblog/code/pca/lgc_pca.jl&#39; target=&#39;_blank&#39;&amp;gt;Julia code here&amp;lt;/a&amp;gt; to reproduce this figure.&quot; width=&quot;600px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Logistic PCA can outperform classic PCA on binary data.&lt;/b&gt;
	See &lt;a href=&quot;/itsneuronalblog/code/pca/lgc_pca.jl&quot; target=&quot;_blank&quot;&gt;Julia code here&lt;/a&gt; to reproduce this figure.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;There are many other variations that you can come up with, each of which is tailored to different data types and characteristics:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Robust PCA.&lt;/strong&gt; If you have outliers in your dataset, use the sum of the absolute value of the residuals (L1 loss) or a &lt;a href=&quot;https://en.wikipedia.org/wiki/Huber_loss&quot;&gt;Huber loss&lt;/a&gt; function (&lt;a href=&quot;http://dx.doi.org/10.1109/TPAMI.2008.114&quot;&gt;Kwak, 2008&lt;/a&gt;). There are some alternative formulations of robust PCA, see e.g. &lt;a href=&quot;http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf&quot;&gt;Candes et al. (2009)&lt;/a&gt; and &lt;a href=&quot;http://papers.nips.cc/paper/5430-non-convex-robust-pca.pdf&quot;&gt;Netrapalli et al. (2014)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Poisson PCA and PCA on ordinal data.&lt;/strong&gt; See &lt;a href=&quot;http://qwone.com/~jason/papers/ijcai05-preference.pdf&quot;&gt;Rennie &amp;amp; Srebro (2005)&lt;/a&gt; for some discussion of appropriate loss functions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Zero-Inflated dimensionality reduction.&lt;/strong&gt; Some datasets, such those from single-cell RNAseq, have more zero entries than would be expected under a Poisson noise model. This can arise from technical variability — mRNA is fragile, and lowly expressed genes have less starting material, leading to “dropout” of lowly expressed genes to zero. &lt;a href=&quot;http://dx.doi.org/10.1186/s13059-015-0805-z&quot;&gt;Pierson &amp;amp; Yau (2015)&lt;/a&gt; develop a model to account for this flavor of noise, and their work can be mapped onto the optimization framework described in this post.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All of these methods have the same basic flavor:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We start with a 2D array of data $\mathbf{X}$&lt;/li&gt;
  &lt;li&gt;We define some cost function or objective function that measures the fit (e.g. least-squares or logistic loss)&lt;/li&gt;
  &lt;li&gt;If we want, we can add terms to the cost function regularize the problem (e.g. to encourage sparsity or enforce nonnegativity)&lt;/li&gt;
  &lt;li&gt;We optimize two smaller matrices $W$ and $C^T$ so that their product reconstructs the data as best as possible.&lt;/li&gt;
  &lt;li&gt;The optimization problem is &lt;a href=&quot;http://www2.math.uni-wuppertal.de/~klamroth/publications/gopfkl07.pdf&quot;&gt;biconvex&lt;/a&gt; (unless the regularization terms or constraints aren’t convex) suggesting alternating minimization as a reasonable optimization procedure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the next post we’ll see that this basic procedure extends quite nicely to other data structures. For example, sometimes our data is more naturally represented in a 3D array (i.e. a &lt;strong&gt;&lt;em&gt;tensor&lt;/em&gt;&lt;/strong&gt;), rather than a matrix. We can apply similar optimization schemes to reduce the dimensionality of the data in this case.&lt;/p&gt;

&lt;h3 id=&quot;some-things-you-maybe-didnt-know-about-pca&quot;&gt;Some things you maybe didn’t know about PCA&lt;/h3&gt;

&lt;h4 class=&quot;no_toc&quot; id=&quot;pca-overfits-to-noise-if-p--n-ie-it-is-an-inconsistenthttpsenwikipediaorgwikiconsistentestimator-estimator-of-the-subspace-of-maximal-variance&quot;&gt;PCA overfits to noise if $p &amp;gt; n$ (i.e. it is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Consistent_estimator&quot;&gt;inconsistent&lt;/a&gt; estimator of the subspace of maximal variance).&lt;/h4&gt;

&lt;p&gt;When solving linear systems of equations the number of equations must be greater than then number of unknown variables. In linear regression, this means that we need more observations than unknown variables ($n &amp;gt; p$). We’ve seen that PCA is closely related to regression, and so it should come as no big surprise that PCA runs into problems when $p &amp;gt; n$. Intuitively, each dimension/feature has some noise associated with it, and we need more observations than parameters to reliably tease apart the signal from the noise.&lt;/p&gt;

&lt;p&gt;One way to potentially get around this problem is to use sparse PCA (&lt;a href=&quot;http://dx.doi.org/10.1198/jasa.2009.0121&quot;&gt;Johnston &amp;amp; Lu, 2009&lt;/a&gt;), although this assumes that your dataset is well-represented in a sparse basis. Moreover, you shouldn’t blindly assume that L1 regularization will produce the &lt;em&gt;correct&lt;/em&gt; sparsity pattern (&lt;a href=&quot;http://statweb.stanford.edu/~candes/papers/LassoFDR.pdf&quot;&gt;Su et al., 2015&lt;/a&gt;; &lt;a href=&quot;http://arxiv.org/abs/1601.04650&quot;&gt;Advani &amp;amp; Ganguli, 2016&lt;/a&gt;).&lt;/p&gt;

&lt;h4 class=&quot;no_toc&quot; id=&quot;there-is-a-very-good-and-simple-procedure-to-determine-how-many-principal-components-to-keep&quot;&gt;There is a very good and simple procedure to determine how many principal components to keep&lt;/h4&gt;

&lt;p&gt;A primary motivation behind PCA is to use as few components as possible to &lt;em&gt;reduce&lt;/em&gt; the dimensionality of the data we are working with. Thus, we are often interested in truncating PCA — keep only the top $k$ components and throw away the rest. There are at least two reasons for this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Truncating gives us a sense of how complex the dataset is. If the top two principal components capture a large majority of variance, then the dataset is more-or-less two-dimensional.&lt;a href=&quot;#f6b&quot; id=&quot;f6t&quot;&gt;&lt;sup&gt;[6]&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Truncating denoises the data. The conceptual connection of PCA to regression is again helpful here — PCA is analogous to fitting a smooth curve through noisy data. Similar intuition is given by figure 2 in this blog post, in which a rank-1 approximation gives a smooth, less noisy, representation of the data.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The question then becomes, how do we choose where to truncate? This used to be one of those classic questions with an unsatisfying answer… Basically, eyeball it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dx.doi.org/10.1109/TIT.2014.2323359&quot;&gt;Gavish &amp;amp; Donoho (2014)&lt;/a&gt; present a long overdue result on this problem and their answer is surprisingly simple and concrete. Essentially, the optimal&lt;a href=&quot;#f7b&quot; id=&quot;f7t&quot;&gt;&lt;sup&gt;[7]&lt;/sup&gt;&lt;/a&gt; procedure boils down to estimating the noise in the dataset, $\sigma$, and then throwing away all components whose singular values are below a specified threshold. For a square $n \times n$ matrix, this threshold is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda =  \frac{4\sigma\sqrt{n}}{\sqrt{3}}&lt;/script&gt;

&lt;p&gt;There is a similar threshold for non-square datasets explained in the paper. As with any theoretical study, the result comes with a few assumptions and caveats,&lt;a href=&quot;#f8b&quot; id=&quot;f8t&quot;&gt;&lt;sup&gt;[8]&lt;/sup&gt;&lt;/a&gt; but their work appears robust and useful in practice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Edit:&lt;/em&gt;&lt;/strong&gt; Thanks to Jonathan Pillow for pointing out a Bayesian alternative outlined here: &lt;a href=&quot;http://hd.media.mit.edu/tech-reports/TR-514.pdf&quot;&gt;&lt;em&gt;Minka (2000). Automatic choice of dimensionality for PCA&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 class=&quot;no_toc&quot; id=&quot;pca-becomes-non-trivialhttpepubssiamorgdoiabs101137110820361-to-solve-when-data-entries-are-missing&quot;&gt;PCA becomes &lt;a href=&quot;http://epubs.siam.org/doi/abs/10.1137/110820361&quot;&gt;non-trivial&lt;/a&gt; to solve when data entries are missing&lt;/h4&gt;

&lt;p&gt;After thinking about these topics for a while, I found it pretty incredible that PCA works at all. In the first place, it is pretty special any time you can provably and analytically solve a nonconvex optimization problem.&lt;/p&gt;

&lt;p&gt;The specialness of PCA breaks down even under pretty mild perturbations. &lt;a href=&quot;http://www.jmlr.org/papers/volume11/ilin10a/ilin10a.pdf&quot;&gt;Ilin &amp;amp; Raiko (2010)&lt;/a&gt; discuss a nice illustration of this point. Consider the case where some subset of data entries are not observed $x_{ij} = \text{NA}$. Even if you keep the ordinary PCA objective function, a number of problems arise:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There is no analytical solution because the data covariance matrix is nontrivial to estimate&lt;/li&gt;
  &lt;li&gt;The objective function contains local minima (unlike in classic PCA, where there are only saddle points and one global minimum). Thus, it is difficult to certify that the output of your optimization problem is true solution to the problem.&lt;/li&gt;
  &lt;li&gt;There is no analytical solution even for the bias term, in contrast to classic PCA where the bias equals column-wise mean of the data matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This last one is particularly jarring. It feels so natural to mean-center the data that is easy to forget that this is not always justified. The task of estimating missing data entries is known as &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_completion&quot;&gt;&lt;em&gt;matrix completion&lt;/em&gt;&lt;/a&gt; and is an important problem in the machine learning community (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Netflix_Prize&quot;&gt;&lt;em&gt;Netflix Prize&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;http://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf&quot;&gt;Candes &amp;amp; Recht, 2008&lt;/a&gt;). Alternating minimization is a common approach for solving these problems (e.g., &lt;a href=&quot;http://dx.doi.org/10.1145/2488608.2488693&quot;&gt;Jain et al., 2013&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;tldr&quot;&gt;TL;DR&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;PCA finds low-dimensional projections that maximally preserve variance in the data&lt;/li&gt;
  &lt;li&gt;This is equivalent to finding a projection that minimizes the projection distance in a least-squares sense&lt;/li&gt;
  &lt;li&gt;This second formulation is similar to least-squares regression. Thinking about PCA this way helps because really smart statisticians have spent decades characterizing, generalizing, robustifying regression. Natural extensions of PCA in this framework include:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/spc_jcgs.pdf&quot;&gt;Sparse PCA&lt;/a&gt;, similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Lasso_(statistics)&quot;&gt;LASSO&lt;/a&gt; in regression&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://dx.doi.org/10.1038/44565&quot;&gt;Non-negative matrix factorization&lt;/a&gt;, similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-negative_least_squares&quot;&gt;non-negative least squares&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/um/cambridge/events/aistats2003/proceedings/119.pdf&quot;&gt;Logistic PCA&lt;/a&gt; for binary data, similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;Logistic regression&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;A variety of &lt;a href=&quot;http://epubs.siam.org/doi/pdf/10.1137/07070111X&quot;&gt;tensor decompositions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PCA is not always easy:
    &lt;ul&gt;
      &lt;li&gt;When there are fewer replicates/samples than measured features/variables ($p &amp;gt; n$) PCA is an &lt;a href=&quot;http://dx.doi.org/10.1198/jasa.2009.0121&quot;&gt;inconsistent estimator&lt;/a&gt;. You need to regularize the problem somehow.&lt;/li&gt;
      &lt;li&gt;When there are missing data entries PCA is &lt;a href=&quot;http://dx.doi.org/10.1137/110820361&quot;&gt;provably NP-hard&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How do you choose the number of principal components to keep?
    &lt;ul&gt;
      &lt;li&gt;It isn’t as necessarily hard as you think. &lt;a href=&quot;http://dx.doi.org/10.1109/TIT.2014.2323359&quot;&gt;Gavish &amp;amp; Donoho (2014)&lt;/a&gt; describe an easy procedure to truncate all components below a threshold based on the level of noise in your data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Udell et al. (2015). &lt;a href=&quot;http://arxiv.org/abs/1410.0342&quot;&gt;Generalized Low-Rank Models&lt;/a&gt;. &lt;em&gt;arxiv preprint&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Tipping &amp;amp; Bishop (1999). &lt;a href=&quot;http://dx.doi.org/10.1111/1467-9868.00196&quot;&gt;Probabilistic principal component analysis&lt;/a&gt;. &lt;em&gt;Journal of the Royal Statistical Society: Series B&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Ilin &amp;amp; Raiko (2010). &lt;a href=&quot;http://www.jmlr.org/papers/volume11/ilin10a/ilin10a.pdf&quot;&gt;Practical Approaches to Principal Component Analysis in the Presence of Missing Values&lt;/a&gt;. &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Gordon (2002). &lt;a href=&quot;http://www.cs.cmu.edu/~ggordon/ggllm.pdf&quot;&gt;Generalized&lt;sup&gt;2&lt;/sup&gt; Linear&lt;sup&gt;2&lt;/sup&gt; Models&lt;/a&gt;. &lt;em&gt;NIPS&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Cunningham &amp;amp; Ghahramani (2015) &lt;a href=&quot;http://jmlr.org/papers/volume16/cunningham15a/cunningham15a.pdf&quot;&gt;Linear dimensionality reduction: survey, insights, and generalizations&lt;/a&gt; &lt;em&gt;Journal of Machine Learning Research&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Burges (2009). &lt;a href=&quot;http://dx.doi.org/10.1561/2200000002&quot;&gt;Dimension Reduction: A Guided Tour&lt;/a&gt;. &lt;em&gt;Foundations and Trends in Machine Learning&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=Everything you did and didn&#39;t know about PCA&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/&amp;amp;title=Everything you did and didn&#39;t know about PCA&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/&amp;amp;t=Everything you did and didn&#39;t know about PCA&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;strong&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Follow:
    &lt;/strong&gt;

    &lt;a href=&quot;http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Follow on Feedly&quot; class=&quot;btn-social btn-rss&quot;&gt;
        &lt;i class=&quot;fa fa-rss&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 class=&quot;no_toc&quot; id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f1t&quot; id=&quot;f1b&quot;&gt;&lt;b&gt;[1]&lt;/b&gt;&lt;/a&gt; It is a basic property of the transpose operator that $(\mathbf{X} \mathbf{c})^T = \mathbf{c}^T \mathbf{X}^T$.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f2t&quot; id=&quot;f2b&quot;&gt;&lt;b&gt;[2]&lt;/b&gt;&lt;/a&gt; There are many interesting remarks to make for the aficionados. Note that the covariance matrix $\mathbf{\Sigma} = \mathbf{X}^T \mathbf{X}$ is a symmetric, &lt;a href=&quot;https://en.wikipedia.org/wiki/Positive-definite_matrix&quot;&gt;positive semi-definite matrix&lt;/a&gt;. This means that $\mathbf{z}^T \mathbf{\Sigma} \mathbf{z} \geq 0$ for any vector $\mathbf{z}$, and equivalently that all eigenvalues of $\mathbf{\Sigma}$ are nonnegative. PCA maximizes $\mathbf{w}^T \mathbf{\Sigma} \mathbf{w}$; the &lt;a href=&quot;http://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component&quot;&gt;solution to this problem&lt;/a&gt; is to set $\mathbf{w}$ to the eigenvector of $\mathbf{\Sigma}$ associated with the largest eigenvalue. All &lt;a href=&quot;http://math.stackexchange.com/questions/82467/eigenvectors-of-real-symmetric-matrices-are-orthogonal&quot;&gt;symmetric matrices have orthogonal eigenvectors&lt;/a&gt;, which is why the principal component vectors are always orthogonal. PCA could be achieved by doing an &lt;a href=&quot;https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix&quot;&gt;eigendecomposition&lt;/a&gt; of the covariance matrix. &lt;br /&gt;&lt;br /&gt;Even better, instead of computing $\mathbf{X}^T \mathbf{X}$ and then an eigendecomposition, one can directly compute the &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;singular-value decomposition&lt;/a&gt; on the raw data matrix. SVD works for non-square matrices (unlike eigendecomposition) and produces $\mathbf{X} = \mathbf{U S V}^T$ where $\mathbf{S}$ is a diagonal matrix of &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value&quot;&gt;singular values&lt;/a&gt; and $\mathbf{U}$ and $\mathbf{V}$ are &lt;a href=&quot;https://en.wikipedia.org/wiki/Orthogonal_matrix&quot;&gt;orthogonal matrices&lt;/a&gt;. Since the transpose of an orthogonal matrix is its inverse, and basic properties of the transpose operator: $\mathbf{X}^T \mathbf{X} = \mathbf{V S U}^T \mathbf{U S V}^T = \mathbf{V S S V}^T =  \mathbf{V \Lambda V}^T$, where $\mathbf{\Lambda}$ is just a diagonal matrix of eigenvalues, which are simply the squared singular values in $\mathbf{S}$. Thus, doing the SVD on the raw data directly gives you the eigendecomposition of the covariance matrix.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Check out Appendix A of &lt;a href=&quot;https://courses2.cit.cornell.edu/mru8/doc/udell15_thesis.pdf&quot;&gt;Madeleine Udell’s thesis&lt;/a&gt;, which showcases five equivalent formulations of PCA as optimization problems.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f4t&quot; id=&quot;f4b&quot;&gt;&lt;b&gt;[4]&lt;/b&gt;&lt;/a&gt; Drop it into conversation at parties. You’ll sound smart and not at all obnoxious.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f5t&quot; id=&quot;f5b&quot;&gt;&lt;b&gt;[5]&lt;/b&gt;&lt;/a&gt; In fact, one way of solving PCA is to solve an equivalent &lt;a href=&quot;https://en.wikipedia.org/wiki/Tikhonov_regularization&quot;&gt;ridge regression&lt;/a&gt; problem (&lt;a href=&quot;https://web.stanford.edu/~hastie/Papers/spc_jcgs.pdf&quot;&gt;Zou et al., 2005&lt;/a&gt;). The biggest conceptual difference is that there is no distinction between &lt;em&gt;dependent&lt;/em&gt; and &lt;em&gt;independent&lt;/em&gt; variables — in PCA all the $p$ variables/features are placed on equal footing. 
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f6t&quot; id=&quot;f6b&quot;&gt;&lt;b&gt;[6]&lt;/b&gt;&lt;/a&gt; It is worth mentioning that there are other ways of measuring dimensionality. One elegant way is use the eigenvalues of the covariance matrix ($\lambda_i$) to calculate the &lt;strong&gt;participation ratio:&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;PR = \frac{\left(\sum_i \lambda_i \right)^2}{\sum_i \lambda_i^2},&lt;/script&gt;
which is easy to work with analytically and has a number of nice properties. Suppose we have a $d \times d$ covariance matrix. If all eigenvalues are equal, $\lambda_i = \lambda$, then $PR = d$ indicating that the variance is evenly spread across all dimensions. As a second example, suppose the eigenvalues exponentially decay with rate $1/k$, i.e. $\lambda_i = \lambda \exp(-i / k)$. A small decay rate means that more components are needed to explain a fixed fraction of the variance. Thus, a larger $k$ corresponds to a more complex, higher-dimensional dataset. For this scenario, $PR$ gives a intuitively reasonable measure of dimensionality since $PR \approx 2k$ when $k$ is small relative to $d$ and $k&amp;gt;1$ (the top $2k$ components explain $\sim$86% of the variance).
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f7t&quot; id=&quot;f7b&quot;&gt;&lt;b&gt;[7]&lt;/b&gt;&lt;/a&gt; Optimal in this setting means minimizing the mean squared error in the limit as $n \rightarrow \infty$. This condition is necessary for the math to work out, but their procedure does quite well for finite $n$ in numerical simulations.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f8t&quot; id=&quot;f8b&quot;&gt;&lt;b&gt;[8]&lt;/b&gt;&lt;/a&gt; The result doesn’t hold if: (a) you have missing data, (b) you have a different noise model (e.g. substituting logistic loss for squared error), or (c) if you impose other constraints on the optimization problem (e.g. in nonnegative matrix factorization or sparse PCA).
&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Highlights of NIPS2015</title>
   <link href="http://alexhwilliams.info/2015/12/14/nips/"/>
   <updated>2015-12-14T00:00:00-08:00</updated>
   <id>http://alexhwilliams.info/2015/12/14/nips</id>
   <content type="html">&lt;p&gt;I was warned that NIPS is an overwhelming conference, but I didn’t listen because I’ve gotten used to SfN, which is several times larger. But for what NIPS lacks in size (nearly 4,000 attendees, still no joke) it more than makes up for in it’s energy. It feels like I haven’t talked about anything other than statistics and machine learning for the last 7 days, and I don’t even remember what a good night’s sleep feels like anymore. I’m writing this up on the bus home, physically and emotionally defeated. But my boss told me to consolidate some brief notes from the conference, so here is my attempt.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;high-level-probably-misinformed-thoughts-on-deep-learning&quot;&gt;High-level (probably misinformed) thoughts on deep learning&lt;/h3&gt;

&lt;p&gt;It is pretty much impossible for me to write this post without weighing in on the deep learning craze. From my perspective, it seems like we’ve pretty much solved pattern recognition &lt;a href=&quot;http://rocknrollnerd.github.io/ml/2015/05/27/leopard-sofa.html&quot;&gt;modulo some edge cases&lt;/a&gt;. In fact, we’ve solved it insanely well in certain domains. Take &lt;a href=&quot;http://dx.doi.org/10.1038/nature14236&quot;&gt;DeepMind’s Atari-playing deep net&lt;/a&gt;. Without a doubt, it is an incredible feat of engineering. But does it really “learn” to play the games on a conceptual level?&lt;/p&gt;

&lt;p&gt;Consider what would happen if you flipped a game like space invaders upside down, or even simply switched the red and blue color channels on the pixels.&lt;a href=&quot;#f1b&quot; id=&quot;f1t&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; A human could probably adjust to these changes nearly instantaneously. The deep net would be completely confused and unable to play the game. Sure, you could retrain it, and you might even argue that relearning just requires changes to the early layers of the network. However, a naïve retraining procedure could potentially modify the weights in deeper layers, destroying useful high-level abstractions that the network had already learned.&lt;a href=&quot;#f2b&quot; id=&quot;f2t&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.mit.edu/~mnick/brains-minds-and-machines-2015/&quot;&gt;Brains, Minds, and Machines Symposium&lt;/a&gt; shed great light on these issues. For me, the talks by Josh Tenenbaum and Gary Marcus were particularly insightful. The basic idea I left with (from these talks and other conversations) was that deep learning is very well-suited for pattern recognition and dealing with high-dimensional inputs. But we need to combine this with other frameworks — for example probabilistic inference and simulation — to solve many problems that humans do with ease. Prior to NIPS I had the misconception that the Bayesian/probabilistic viewpoint couldn’t scale to hard problems. What I failed to realize was that &lt;strong&gt;&lt;em&gt;many hard problems aren’t high-dimensional problems&lt;/em&gt;&lt;/strong&gt;. Josh gave the example of inferring the position of a person’s occluded limb in a crowded photo (something deep nets can’t do… yet). Put simply, we don’t have that many limbs and that many possible configurations for them to be in, so having a cognitive engine that simulates all the possibilities is feasible.&lt;/p&gt;

&lt;p&gt;The take-home message was that cognitive science is actually pretty awesome and (potentially) has a lot to offer. Coming from a molecular/cellular neurobiology research background, this was an incredibly refreshing perspective.&lt;/p&gt;

&lt;h3 id=&quot;neuroscience-at-nips&quot;&gt;Neuroscience at NIPS&lt;/h3&gt;

&lt;h4 id=&quot;posters&quot;&gt;Posters&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5767-high-dimensional-neural-spike-train-analysis-with-generalized-count-linear-dynamical-systems&quot;&gt;&lt;strong&gt;High-dimensional neural spike train analysis with generalized count linear dynamical systems&lt;/strong&gt;&lt;/a&gt;.&lt;br /&gt;&lt;em&gt;Yuanjun Gao, Lars Büsing, Krishna V. Shenoy, John P. Cunningham&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Extends previous work (&lt;a href=&quot;http://dx.doi.org/10.1080/09548980701625173&quot;&gt;Kulkarni &amp;amp; Paninski, 2007&lt;/a&gt;; &lt;a href=&quot;http://papers.nips.cc/paper/4995-robust-learning-of-low-dimensional-dynamics-from-large-neural-ensembles&quot;&gt;Pfau et al., 2013&lt;/a&gt;) on extracting low-dimensional dynamics from network recordings. Previous work has assumed Poisson output, but this is constrained by (mean = variance). Real spike counts are often &lt;a href=&quot;https://en.wikipedia.org/wiki/Overdispersion&quot;&gt;over-dispersed&lt;/a&gt;, so something like a &lt;a href=&quot;https://en.wikipedia.org/wiki/Negative_binomial_distribution&quot;&gt;negative binomial distribution&lt;/a&gt; may be more accurate. The authors show how to extend traditional GLMs to accommodate this class of models (and all others in the exponential family). They also apply their method to some interesting data to demonstrate the advantages of this approach.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5962-convolutional-spike-triggered-covariance-analysis-for-neural-subunit-models.pdf&quot;&gt;&lt;strong&gt;Convolutional spike-triggered covariance analysis for neural subunit models&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Anqi Wu, Il Memming Park, Jonathan W. Pillow&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The latest in the quest to fit good phenomenological models of early sensory neurons. The authors examine how to efficiently fit parameters of subunit models — in which a output neuron is activated by a layer of “subunits” with shifted linear filters and individual nonlinearities. Fitting these models is generally hard — see earlier work by &lt;a href=&quot;http://papers.nips.cc/paper/4742-efficient-and-direct-estimation-of-a-neural-subunit-model-for-sensory-coding&quot;&gt;Vintch et al. (2012)&lt;/a&gt;. However, the authors show that it can be easy under certain assumptions.&lt;a href=&quot;#f3b&quot; id=&quot;f3t&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt; They derive an estimator based on the spike-triggered average and covariance; it seems to work well even when the assumptions are violated.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5952-synaptic-sampling-a-bayesian-approach-to-neural-network-plasticity-and-rewiring&quot;&gt;&lt;strong&gt;Synaptic Sampling: A Bayesian Approach to Neural Network Plasticity and Rewiring&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;em&gt;David Kappel, Stefan Habenschuss, Robert Legenstein, Wolfgang Maass&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I really like the motivation of this poster: synapses are highly dynamic biological units that grow, retract, and change in size and strength. Despite this indisputable fact, almost all modeling work considers synaptic weights that are stable, noiseless, and deterministically updated by learning rules. The authors construct and examine a framework where the synaptic weights stochastically explore a probability distribution.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5948-enforcing-balance-allows-local-supervised-learning-in-spiking-recurrent-networks&quot;&gt;&lt;strong&gt;Enforcing balance allows local supervised learning in spiking recurrent networks&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Ralph Bourdoukan, Sophie Denève&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In line with &lt;a href=&quot;http://dx.doi.org/10.1371/journal.pcbi.1003258&quot;&gt;previous work&lt;/a&gt; from Denève’s group, the paper describes how to train spiking neural networks to implement a linear dynamical system. It seems like the key advance is that the rule described here is purely local (and therefore perhaps more biologically plausible).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;shameless-plug&quot;&gt;Shameless Plug&lt;/h4&gt;

&lt;p&gt;Some talented friends from the Columbia NeuroTheory group have started a new company called &lt;a href=&quot;https://cognescent.com/&quot;&gt;&lt;strong&gt;&lt;em&gt;Cognescent&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;. They’re just getting started, but keep an eye on them! They want to expand their team, so get in touch if you are looking for a job.&lt;/p&gt;

&lt;h3 id=&quot;nonconvex-optimization&quot;&gt;Nonconvex Optimization&lt;/h3&gt;

&lt;h4 id=&quot;posters-1&quot;&gt;Posters&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5728-accelerated-proximal-gradient-methods-for-nonconvex-programming&quot;&gt;&lt;strong&gt;Accelerated Proximal Gradient Methods for Nonconvex Programming&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;&lt;em&gt;Huan Li, Zhouchen Lin&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Seminal work by Nesterov produced simple methods for smooth, convex problems that achieved fast (quadratic) convergence using only information the gradient of the objective function (see &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf&quot;&gt;Sutskever et al., 2013&lt;/a&gt; for a review). This work was extended to nonsmooth, convex problems, producing “accelerated” proximal methods (&lt;a href=&quot;http://dx.doi.org/10.1109/TIP.2009.2028250&quot;&gt;Beck &amp;amp; Teboulle, 2009&lt;/a&gt;; &lt;a href=&quot;https://web.stanford.edu/~boyd/papers/prox_algs.html&quot;&gt;Parikh &amp;amp; Boyd, 2014&lt;/a&gt;). This paper by Li and Lin takes it a step further to nonsmooth, nonconvex functions. Like previous work, their algorithm produces iterative updates based on the gradient (subgradient for nonsmooth cases) and an extrapolation term (which is similar, not the same, as momentum). The critical insight is that these extrapolations are potentially quite bad for nonconvex problems, so they extend &lt;a href=&quot;http://dx.doi.org/10.1109/TIP.2009.2028250&quot;&gt;Beck &amp;amp; Teboulle’s&lt;/a&gt; method by monitoring each step and correcting it when it goes off in a bad direction.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;workshop-websitehttpssitesgooglecomsitenips2015nonconvexoptimizationhome&quot;&gt;Workshop &lt;a href=&quot;https://sites.google.com/site/nips2015nonconvexoptimization/home&quot;&gt;&lt;em&gt;(website)&lt;/em&gt;&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://people.csail.mit.edu/hmobahi/&quot;&gt;&lt;strong&gt;Tackling Nonconvex Optimization by Complexity Progression&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
&lt;em&gt;Hossein Mobahi&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The convex envelope of a nonconvex function can be optimized efficiently, and has a unique solution that is equal to the global minimum of the original function. The problem is that it is generally intractable to compute. &lt;a href=&quot;http://people.csail.mit.edu/hmobahi/pubs/gaussian_convenv_2015.pdf&quot;&gt;Mobahi and Fisher (2015)&lt;/a&gt; show that, for certain problems,&lt;a href=&quot;#f4b&quot; id=&quot;f4t&quot;&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/a&gt; a Gaussian smoothing of the nonconvex function can be found in closed form and is the best affine approximation of the convex envelope. You can first solve a &lt;em&gt;very smoothed&lt;/em&gt; version of the nonconvex problem, and then solve progressively less smoothed versions of the problem (i.e. use continuation methods). The basic idea is that solving each smoothed problem gives you a very good warm start on the next, more difficult optimization problem, so you arrive at a good solution. Unfortunately, when the Gaussian smoothing can’t be computed in closed form, it is expensive to compute numerically.&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Check out &lt;a href=&quot;#comment-2411704388&quot;&gt;the discussion&lt;/a&gt; below. Closed form smoothing is possible for common nonlinearities in deep networks! Thanks to Hossein for his comments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There were also a couple of cool posters at this workshop:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Cheng Tang &amp;amp; Claire Monteleoni presented theoretical analysis and guarantees of the $k$-means clustering algorithm. &lt;a href=&quot;/itsneuronalblog/papers/clustering/poster_ncvx_f.pdf&quot;&gt;The poster&lt;/a&gt; (posted with permission) is similar in spirit to my &lt;a href=&quot;/itsneuronalblog/2015/11/18/clustering-is-easy/&quot;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jacob Abernethy, Alex Kulesza, &amp;amp; Matus Telgarsky presented some simple intution/insights into why deep networks are typically more powerful than wide, shallow networks. The case they examine is very simple, but it nevertheless provides nice insight. There is a &lt;a href=&quot;http://arxiv.org/abs/1509.08101&quot;&gt;paper on arxiv&lt;/a&gt; that covers the material on the poster.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;miscellaneous-things-i-thought-were-cool&quot;&gt;Miscellaneous things I thought were cool&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Andrew Gelman gave an excellent, and thoroughly entertaining, talk on how experiments can suffer from the well-known problem of multiple-comparisons, &lt;a href=&quot;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf&quot;&gt;&lt;strong&gt;&lt;em&gt;even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One of the best paper awards, &lt;a href=&quot;http://papers.nips.cc/paper/5762-competitive-distribution-estimation-why-is-good-turing-good&quot;&gt;&lt;em&gt;Competitive Distribution Estimation: Why is Good-Turing Good&lt;/em&gt;&lt;/a&gt;, was pretty interesting to me.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sanjeev Arora announced a &lt;a href=&quot;http://www.offconvex.org/&quot;&gt;new blog on nonconvex optimization&lt;/a&gt; that he will write with a few colleagues.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://memming.wordpress.com/2015/12/09/nips-2015-part-2/&quot;&gt;Il Memming Park’s blog&lt;/a&gt; has some nice notes on the conference.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;women-in-machine-learning&quot;&gt;Women in Machine Learning&lt;/h3&gt;

&lt;p&gt;I’ll end with some very short comments about gender balance. First, as I already noted on Twitter, the &lt;a href=&quot;http://www.wimlworkshop.org/&quot;&gt;Women in Machine Learning&lt;/a&gt; (WiML) poster session was fantastic. I thought it was unfortunate that it wasn’t better advertised — I only ended up there by pure accident, since I found the tutorials kind of dull. Unlike the main poster session, which was swamped with a frustrating number of people&lt;a href=&quot;#f5b&quot; id=&quot;f5t&quot;&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/a&gt;, I got to have a couple of really nice in depth conversations about things very relevant to my interests. (I even found out about &lt;a href=&quot;http://www.genomebiology.com/2015/16/1/241&quot;&gt;a cool way of doing factor analysis on RNA expression datasets&lt;/a&gt;!) It would be nice if the WiML meeting was integrated into the main program — perhaps still as a parallel track, but with more general participation.&lt;/p&gt;

&lt;p&gt;In terms of women in the general meeting, the numbers are pretty abysmal:&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet tw-align-center&quot; lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;There&amp;#39;s a lot of room for improvement at &lt;a href=&quot;https://twitter.com/NipsConference&quot;&gt;@NipsConference&lt;/a&gt;. Cutoff: lots of 0s for women in the &lt;a href=&quot;https://twitter.com/hashtag/NIPS2015?src=hash&quot;&gt;#NIPS2015&lt;/a&gt; Symposia &lt;a href=&quot;https://t.co/5MqP8OomEf&quot;&gt;pic.twitter.com/5MqP8OomEf&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sarah Brown (@BrownSarahM) &lt;a href=&quot;https://twitter.com/BrownSarahM/status/673864059859558401&quot;&gt;December 7, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;I’m optimistic that the insane growth in NIPS attendance will bring greater attention and pressure for the organizers to address this issue. I found &lt;a href=&quot;http://dx.doi.org/10.1371%2Fjournal.pcbi.1003903&quot;&gt;&lt;strong&gt;&lt;em&gt;Ten Simple Rules to Achieve Conference Speaker Gender Balance&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; a thought-provoking read, both about why this is an important topic and what we can do about it.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=Highlights of NIPS2015&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/&amp;amp;title=Highlights of NIPS2015&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/&amp;amp;t=Highlights of NIPS2015&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/12/14/nips/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;strong&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Follow:
    &lt;/strong&gt;

    &lt;a href=&quot;http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Follow on Feedly&quot; class=&quot;btn-social btn-rss&quot;&gt;
        &lt;i class=&quot;fa fa-rss&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f1t&quot; id=&quot;f1b&quot;&gt;&lt;b&gt;[1]&lt;/b&gt;&lt;/a&gt; I didn’t come up with this thought experiment. I stole it from &lt;a href=&quot;https://twitter.com/jhamrick&quot;&gt;@jhamrick&lt;/a&gt; — all credit goes to her.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f2t&quot; id=&quot;f2b&quot;&gt;&lt;b&gt;[2]&lt;/b&gt;&lt;/a&gt; This brings up an interesting research question: is there a general way of identifying layers that should be retrained? Identifying layers that are working fine and should not be retrained? 
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Namely, they assume that (a) the stimulus is Gaussian, (b) that the subunit nonlinearity is a second-order polynomial, and (c) the final nonlinearity is exponential.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f4t&quot; id=&quot;f4b&quot;&gt;&lt;b&gt;[4]&lt;/b&gt;&lt;/a&gt; When the error landscape can be represented by polynomials or Gaussian radial basis functions, then the convolution/smoothing can be solved in closed form.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f5t&quot; id=&quot;f5b&quot;&gt;&lt;b&gt;[5]&lt;/b&gt;&lt;/a&gt; This is another minor criticism I have of the conference. The proportion of posters to people attending is much too small. It became impossible to reach the front of the line and talk with the presenter. Having concurrent sessions/talks is seemingly inevitable given the rapid growth of the conference. The workshops were probably my favorite part for this reason.
&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Clustering is hard, except when it's not</title>
   <link href="http://alexhwilliams.info/2015/11/18/clustering-is-easy/"/>
   <updated>2015-11-18T00:00:00-08:00</updated>
   <id>http://alexhwilliams.info/2015/11/18/clustering-is-easy</id>
   <content type="html">&lt;p&gt;The previous two posts (&lt;a href=&quot;/itsneuronalblog/2015/09/11/clustering1/&quot;&gt;part 1&lt;/a&gt;, &lt;a href=&quot;/itsneuronalblog/2015/10/01/clustering2/&quot;&gt;part 2&lt;/a&gt;) on clustering have been somewhat depressing and pessimistic. However, the reality is that scientists use simple clustering heuristics &lt;em&gt;all the time&lt;/em&gt;, and often find interpretable results. What gives? Is the theoretical hardness of clustering flawed? Or have we just been deluding ourselves? Have we been fooled into believing results that are in some sense fundamentally flawed?&lt;/p&gt;

&lt;p&gt;This post will explore a more optimistic possibility, which has been referred to as the &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter” hypothesis&lt;/em&gt;. Proponents argue that, while we can construct worst-case scenarios that cause algorithms to fail, clustering techniques work very well in practice because real-world datasets often have characteristic structure that more-or-less guarantees the success of these algorithms. Put differently, &lt;a href=&quot;http://arxiv.org/abs/1205.4891&quot;&gt;Daniely et al. (2012)&lt;/a&gt; say that “clustering is easy, otherwise it is pointless” — whenever clustering fails, it is probably because the data in question were not amenable to clustering in the first place.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;notation&quot;&gt;Notation&lt;/h3&gt;

&lt;p&gt;In this post, we are going to view clustering as an optimization problem.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Let $\mathcal{C}$ denote a clustering (or &lt;a href=&quot;https://en.wikipedia.org/wiki/Partition_of_a_set&quot;&gt;partition&lt;/a&gt;) of a dataset into $k$ clusters.&lt;a href=&quot;#f1b&quot; id=&quot;f1t&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Let $F(\mathcal{C})$ be the loss function (a.k.a objective function) that computes a “cost” or “badness” for any clustering.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Our goal is to find the &lt;em&gt;best&lt;/em&gt; or &lt;em&gt;optimal&lt;/em&gt; clustering (i.e. the one with the lowest value of $F$). We call the optimal clustering $C_{opt}$ , and the lowest/best value of the objective function $F_{opt}$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{C}_{\text{opt}} = \arg \min_{\mathcal{C}_i} F(\mathcal{C}_i)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_{\text{opt}} = \min_{\mathcal{C}_i} F(\mathcal{C}_i)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; $k$-means clustering results from choosing $F$ to be the sum-of-squared residuals between each datapoint $\mathbf{x}_j$ and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Centroid&quot;&gt;centroid&lt;/a&gt; ($\bar{\mathbf{x}}$) of the cluster it belongs to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(\mathcal{C}) = \sum_{i=1}^k \sum_{\mathbf{x}_j \in \mathcal{K}_i}  \big\Vert \bar{\mathbf{x}}_i - \mathbf{x}_j \big\Vert^2_2&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; $k$-medians clustering results from choosing $F$ to be the sum of the absolute residuals between each datapoint $\mathbf{x}_j$ and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Medoid&quot;&gt;mediod&lt;/a&gt; ($\tilde{\mathbf{x}}$) of the cluster it belongs to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(\mathcal{C}) = \sum_{i=1}^k \sum_{\mathbf{x}_j \in \mathcal{K}_i} \big \vert \tilde{\mathbf{x}}_i - \mathbf{x}_j \big \vert&lt;/script&gt;

&lt;p&gt;For the purposes of this post, you can assume we’re using either of the above objective functions.&lt;a href=&quot;#f2b&quot; id=&quot;f2t&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt; Throughout this post, we assume that the number of clusters, $k$, is known &lt;em&gt;a priori&lt;/em&gt; — analysis becomes very difficult otherwise.&lt;/p&gt;

&lt;h3 id=&quot;intuition-behind-easy-vs-hard-clustering&quot;&gt;Intuition behind easy vs. hard clustering&lt;/h3&gt;

&lt;p&gt;It is easy to construct datasets where it takes a &lt;em&gt;very long time&lt;/em&gt; to find $\mathcal{C}_{\text{opt}}$. Consider the (schematic) dataset below. The data form an amorphous blob of points that are not easily separated into two clusters.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;A dataset not amenable to clustering.. &lt;/b&gt;Datapoints are shown as open circles, with color representing cluster assignment and &lt;b&gt;x&lt;/b&gt;&#39;s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (&lt;i&gt;left&lt;/i&gt;) and $\mathcal{C}_2$ (&lt;i&gt;middle&lt;/i&gt;) are shown that have a similar loss. There are many local minima in the objective function (&lt;i&gt;right&lt;/i&gt;). &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/c_e_condition_1.png&quot; alt=&quot;Datapoints are shown as open circles, with color representing cluster assignment and &amp;lt;b&amp;gt;x&amp;lt;/b&amp;gt;&#39;s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (&amp;lt;i&amp;gt;left&amp;lt;/i&amp;gt;) and $\mathcal{C}_2$ (&amp;lt;i&amp;gt;middle&amp;lt;/i&amp;gt;) are shown that have a similar loss. There are many local minima in the objective function (&amp;lt;i&amp;gt;right&amp;lt;/i&amp;gt;). &amp;lt;b&amp;gt;&amp;lt;i&amp;gt;Disclaimer: schematic, not real data.&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;&quot; width=&quot;1000px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;A dataset not amenable to clustering.&lt;/b&gt;
	Datapoints are shown as open circles, with color representing cluster assignment and &lt;b&gt;x&lt;/b&gt;&#39;s denoting cluster centers. Two clusterings, $\mathcal{C}_1$ (&lt;i&gt;left&lt;/i&gt;) and $\mathcal{C}_2$ (&lt;i&gt;middle&lt;/i&gt;) are shown that have a similar loss. There are many local minima in the objective function (&lt;i&gt;right&lt;/i&gt;). &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;In the above dataset we can find many clusterings that are nearly equivalent in terms of the loss function (e.g. $\mathcal{C}_1$ and $\mathcal{C}_2$ in the figure). Thus, if we want to be sure to find the &lt;em&gt;very best&lt;/em&gt; clustering, we need to essentially do a brute force search.&lt;a href=&quot;#f3b&quot; id=&quot;f3t&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt; The &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter” hypothesis&lt;/em&gt; (and common sense) would tell us that it is stupid to do a cluster analysis on this dataset — there simply aren’t any clusters to be found!&lt;/p&gt;

&lt;p&gt;Now compare this to a case where there are, in fact, two clearly separated clusters. In this case, there is really only one clustering that passes the “common sense” test. Assuming we pick a reasonable loss function, there should also be a very obvious global solution (unlike the first example):&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;A dataset that is easily clustered.. &lt;/b&gt;As before, two clusterings are shown. The clustering on the &lt;i&gt;left&lt;/i&gt; is much better in terms of the loss function than the clustering shown in the &lt;i&gt;middle&lt;/i&gt;. &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/c_e_condition_good.png&quot; alt=&quot;As before, two clusterings are shown. The clustering on the &amp;lt;i&amp;gt;left&amp;lt;/i&amp;gt; is much better in terms of the loss function than the clustering shown in the &amp;lt;i&amp;gt;middle&amp;lt;/i&amp;gt;. &amp;lt;b&amp;gt;&amp;lt;i&amp;gt;Disclaimer: schematic, not real data.&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;&quot; width=&quot;1000px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;A dataset that is easily clustered.&lt;/b&gt;
	As before, two clusterings are shown. The clustering on the &lt;i&gt;left&lt;/i&gt; is much better in terms of the loss function than the clustering shown in the &lt;i&gt;middle&lt;/i&gt;. &lt;b&gt;&lt;i&gt;Disclaimer: schematic, not real data.&lt;/i&gt;&lt;/b&gt;
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Intuitively, it should be easier to find the solution for this second dataset: there is &lt;em&gt;clear winner&lt;/em&gt; for the clustering, so there is an obvious global minimum, with few local minima. Remember, in the first dataset, there were multiple local minima that were &lt;em&gt;nearly as good&lt;/em&gt; as the global minimum.&lt;/p&gt;

&lt;h3 id=&quot;provably-easy-clustering-situations&quot;&gt;Provably “easy” clustering situations&lt;/h3&gt;

&lt;p&gt;We would like to formalize the intuition outlined in the previous section to develop efficient and accurate clustering algorithms. To do this we introduce the concept of &lt;strong&gt;&lt;em&gt;approximation stability&lt;/em&gt;&lt;/strong&gt;, which characterizes how “nice” the error landscape of the optimization problem is. In the schematic figures, the first difficult-to-cluster example is unstable, while the second easy-to-cluster example is stable. &lt;em&gt;The ultimate punchline is that sufficiently stable clustering problems are provably easy to solve.&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Definition:&lt;/strong&gt; $(c,\epsilon)$-approximation-stability.&lt;/p&gt;

  &lt;p&gt;A clustering problem is said to be $(c,\epsilon)$-stable when all clusterings, $\mathcal{C^\prime}$, that satisfy $F(\mathcal{C}^\prime) \leq c F_{opt}$ also satisfy $d(\mathcal{C}^\prime,\mathcal{C}_{\text{opt}}) &amp;lt; \epsilon$. Here, $0 &amp;lt; \epsilon \ll 1$, and $c &amp;gt; 1$, and $d(\cdot,\cdot)$ measures the fraction of differently assigned datapoints between two clusterings.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The more stable the clustering problem is, the larger $c$ and the smaller $\epsilon$ are allowed to be. For example, if $c = 1.1$ and $\epsilon = 0.02$, then a problem is $(c,\epsilon)$-stable if all clusterings within 10% of the optimal objective value, are no more than 2% different from the optimal clustering.&lt;/p&gt;

&lt;p&gt;As cluster stability increases, two things happen:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The problem becomes easier to solve.&lt;/strong&gt; &lt;a href=&quot;http://dx.doi.org/10.1145/2450142.2450144&quot;&gt;Balcan et al. (2013)&lt;/a&gt; provide several algorithms that are guaranteed to find &lt;em&gt;near-optimal&lt;/em&gt; clusterings if the clusters are large enough and the problem is stable enough.&lt;a href=&quot;#f4b&quot; id=&quot;f4t&quot;&gt;&lt;sup&gt;[4]&lt;/sup&gt;&lt;/a&gt; These algorithms are very efficient,&lt;a href=&quot;#f5b&quot; id=&quot;f5t&quot;&gt;&lt;sup&gt;[5]&lt;/sup&gt;&lt;/a&gt; easy to implement, and similar to classic clustering algorithms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cluster analysis becomes more sensible and interpretable.&lt;/strong&gt; While not immediately obvious, it turns out that approximation stability (as well as similar concepts, like &lt;a href=&quot;http://arxiv.org/abs/1112.0826&quot;&gt;&lt;em&gt;perturbation stability&lt;/em&gt;&lt;/a&gt;) correlates with our intuitive sense of clusterability: when the data contain well-separated and compact clusters, then the clustering optimization problem is likely stable. This is outlined in Lemma 3.1 by &lt;a href=&quot;http://dx.doi.org/10.1145/2450142.2450144&quot;&gt;Balcan et al. (2013)&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In short, research along these lines is &lt;a href=&quot;#caveats&quot;&gt;&lt;em&gt;beginning&lt;/em&gt;&lt;/a&gt; to provide rigorous support for the &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter” hypothesis&lt;/em&gt;. To see the proofs associated with this work in detail check out the course materials for &lt;a href=&quot;http://theory.stanford.edu/~tim/f14/f14.html&quot;&gt;this class on “Beyond Worst-Case Analysis”&lt;/a&gt;. A particularly relevant lecture is embedded below (the others are also online):&lt;/p&gt;

&lt;iframe width=&quot;373&quot; height=&quot;210&quot; style=&quot;margin:20px auto; display:block&quot; src=&quot;https://www.youtube.com/embed/n0T0fyRt0Xo&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h3 id=&quot;caveats&quot;&gt;Caveats&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://cs.uwaterloo.ca/~shai/&quot;&gt;Shai Ben-David&lt;/a&gt; recently published &lt;a href=&quot;http://arxiv.org/abs/1510.05336&quot;&gt;a brief commentary&lt;/a&gt; on the &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter” hypothesis&lt;/em&gt; alongside &lt;a href=&quot;http://arxiv.org/abs/1501.00437&quot;&gt;a more detailed paper&lt;/a&gt;.&lt;a href=&quot;#f6b&quot; id=&quot;f6t&quot;&gt;&lt;sup&gt;[6]&lt;/sup&gt;&lt;/a&gt; He argues that, while the above results (and others) are encouraging, &lt;strong&gt;&lt;em&gt;current theory has only shown clustering to be easy when clusters are very, very obvious&lt;/em&gt;&lt;/strong&gt; in the dataset. For example, Ben-David digs into the specific results of &lt;a href=&quot;http://dx.doi.org/10.1145/2450142.2450144&quot;&gt;Balcan et al. (2013)&lt;/a&gt; and concludes that their (simple, efficient) algorithm indeed produces the correct solution as clustering becomes &lt;em&gt;stable enough&lt;/em&gt;. However, “stable enough” in this case more or less means that &lt;strong&gt;&lt;em&gt;the majority of points sit more than 20 times closer to their true cluster than to any other cluster.&lt;/em&gt;&lt;/strong&gt; This seems like a very strong assumption, which won’t hold for many practical applications.&lt;/p&gt;

&lt;p&gt;There are other caveats to briefly mention.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We have restricted our discussion to center-based clustering frameworks (e.g. $k$-means and $k$-medians). This excludes the possibility of clustering more complicated manifolds. However, I’m not sure how much this matters. It is easy to dream up toy, nonlinear datasets (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction&quot;&gt;the Swiss Roll&lt;/a&gt;) that cause center-based clustering to fail. Are real-world datasets this pathological? &lt;a href=&quot;https://en.wikipedia.org/wiki/Consensus_clustering&quot;&gt;Ensemble clustering&lt;/a&gt; provides a nice way to cluster non-linear manifolds with center-based techniques. Thus, to address this concern, it would be interesting to extend the theoretical results covered in this post to ensemble-based algorithms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Throughout this post we have assumed that the number of clusters is known beforehand. Estimating the number of clusters ($k$) is a well-known and generally unsolved problem.&lt;a href=&quot;#f7b&quot; id=&quot;f7t&quot;&gt;&lt;sup&gt;[7]&lt;/sup&gt;&lt;/a&gt; In practice, we typically run clustering algorithms for various choices of $k$, and compare results in a somewhat &lt;em&gt;ad hoc&lt;/em&gt; manner. For clustering to truly be “easy”, we need simple, consistent, and accurate methods for estimating $k$. While there is some work on this issue (e.g., &lt;a href=&quot;http://statweb.stanford.edu/~gwalther/gap&quot;&gt;Tibshirani et al., 2001&lt;/a&gt;), most of it is constrained to the case of “well-separated” clusters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusions-and-related-work&quot;&gt;Conclusions and related work&lt;/h3&gt;

&lt;p&gt;A theoretical understanding of clustering algorithms is desperately needed, and despite substantial caveats, it seems that we are beginning to make progress. I find the theoretical analysis in this area to be quite interesting and worthy of further work. However, it may be overly optimistic to conclude the &lt;em&gt;“Clustering is only difficult when it does not matter”&lt;/em&gt;. Given current results, it is probably safer to conclude that &lt;em&gt;“Clustering is difficult, except when it isn’t”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The essential flavor of this work is part of a &lt;a href=&quot;http://sunju.org/research/nonconvex/&quot;&gt;growing literature&lt;/a&gt; on finding provably accurate and efficient algorithms to solve problems that were traditionally thought to be difficult (often NP-Hard) to solve. A well-known example in the machine learning community is &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-negative_matrix_factorization&quot;&gt;nonnegative matrix factorization (NMF)&lt;/a&gt; under the “separability condition.” While NMF is NP-hard in general, work by &lt;a href=&quot;http://arxiv.org/abs/1111.0952&quot;&gt;Arora et al. (2012)&lt;/a&gt; showed that it could be solved in polynomial time under certain assumptions (which were typically satisfied or nearly satisfied, in practice). &lt;a href=&quot;http://arxiv.org/abs/1310.7529&quot;&gt;Further&lt;/a&gt; &lt;a href=&quot;http://arxiv.org/abs/1208.1237&quot;&gt;work&lt;/a&gt; by &lt;a href=&quot;https://scholar.google.be/citations?user=pVIJV7wAAAAJ&quot;&gt;Nicolas Gilles&lt;/a&gt; on this problem is worthy of special mention.&lt;/p&gt;

&lt;p&gt;While all of this may seem a bit tangential to the topic of clustering, it really isn’t. One of the reasons NMF is useful is that it produces a sparse representation of a dataset, which can be thought of as an approximate clustering, or &lt;em&gt;soft clustering&lt;/em&gt; of a dataset &lt;a href=&quot;https://smartech.gatech.edu/bitstream/handle/1853/20058/GT-CSE-08-01.pdf?sequence=1&quot;&gt;(Park &amp;amp; Kim, 2008)&lt;/a&gt;. In other words, the very recent and very exciting work on provable NMF algorithms raises the tantalizing possibility that these ideas will soon provide deep insight into clustering.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=Clustering is hard, except when it&#39;s not&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;amp;title=Clustering is hard, except when it&#39;s not&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&amp;amp;t=Clustering is hard, except when it&#39;s not&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;strong&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Follow:
    &lt;/strong&gt;

    &lt;a href=&quot;http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Follow on Feedly&quot; class=&quot;btn-social btn-rss&quot;&gt;
        &lt;i class=&quot;fa fa-rss&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f1t&quot; id=&quot;f1b&quot;&gt;&lt;b&gt;[1]&lt;/b&gt;&lt;/a&gt; Explicitly, $\mathcal{C} = \{ \mathcal{K}_1,\mathcal{K}_2,…,\mathcal{K}_k\}$, where each $\mathcal{K}_i$ is a set of datapoints, $\mathcal{K}_i = \{\mathbf{x}_1^{(i)},\mathbf{x}_2^{(i)},…\}$, where $\mathbf{x}_j^{(i)}$, is a datapoint (a vector) in cluster $i$, indexed by $j$.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f2t&quot; id=&quot;f2b&quot;&gt;&lt;b&gt;[2]&lt;/b&gt;&lt;/a&gt; It is instructive to prove that the arithmetic mean (i.e. centroid) of a set of points &lt;a href=&quot;http://math.stackexchange.com/questions/967138/formal-proof-that-mean-minimize-squared-error-function&quot;&gt;minimizes the sum-of-squared residuals&lt;/a&gt;. Similarly, the median &lt;a href=&quot;http://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations&quot;&gt;minimizes the sum-of-absolute residuals&lt;/a&gt;.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Speaking loosely, the necessity of this brute-force search makes finding the solution to the k-means optimization problem NP-hard &lt;a href=&quot;https://dx.doi.org/10.1007%2Fs10994-009-5103-0&quot;&gt;(Aloise et al., 2009)&lt;/a&gt;. Note that there are simple and efficient algorithms that find local minima, given an initial guess. However, solving the problem (i.e. finding and certifying that you’ve found the global minimum) is NP-hard in the worst-case.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f4t&quot; id=&quot;f4b&quot;&gt;&lt;b&gt;[4]&lt;/b&gt;&lt;/a&gt; What, exactly, does it mean for a clustering problem to be “stable enough”? This is a very critical question, that is revisited in the &lt;a href=&quot;#caveats&quot;&gt;caveats section&lt;/a&gt;.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f5t&quot; id=&quot;f5b&quot;&gt;&lt;b&gt;[5]&lt;/b&gt;&lt;/a&gt; They run in &lt;a href=&quot;https://en.wikipedia.org/wiki/P_(complexity)&quot;&gt;polynomial time&lt;/a&gt;. Again, clustering problems are NP-hard in the worst case; they (probably) take exponential time to solve (assuming P $\neq$ NP).
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f6t&quot; id=&quot;f6b&quot;&gt;&lt;b&gt;[6]&lt;/b&gt;&lt;/a&gt; I borrowed his phrase — the &lt;em&gt;“Clustering is Only Difficult When It Does Not Matter”&lt;/em&gt; hypothesis — for this post, although it is also a title of a &lt;a href=&quot;http://arxiv.org/abs/1205.4891&quot;&gt;older paper&lt;/a&gt; from a different group.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f7t&quot; id=&quot;f7b&quot;&gt;&lt;b&gt;[7]&lt;/b&gt;&lt;/a&gt; I discussed this point to some extent in &lt;a href=&quot;/itsneuronalblog/2015/09/11/clustering1/&quot;&gt;a previous post&lt;/a&gt;
&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Is clustering mathematically impossible?</title>
   <link href="http://alexhwilliams.info/2015/10/01/clustering2/"/>
   <updated>2015-10-01T00:00:00-07:00</updated>
   <id>http://alexhwilliams.info/2015/10/01/clustering2</id>
   <content type="html">&lt;p&gt;In the &lt;a href=&quot;http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&quot;&gt;previous post&lt;/a&gt;, we saw intuitive reasons why clustering is a hard,&lt;a href=&quot;#f1b&quot; id=&quot;f1t&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; and maybe even &lt;em&gt;ill-defined&lt;/em&gt;, problem. In practice, we are often stuck using heuristics that can sometimes perform quite badly when their assumptions are violated (see &lt;a href=&quot;https://en.wikipedia.org/wiki/No_free_lunch_theorem&quot;&gt;&lt;em&gt;No free lunch theorem&lt;/em&gt;&lt;/a&gt;). Is there a mathematical way of expressing all of these difficulties? This post will cover some theoretical results of &lt;a href=&quot;/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf&quot;&gt;Kleinberg (2002)&lt;/a&gt; related to this question.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Notation.&lt;/em&gt;&lt;/strong&gt; Suppose we have a set of $N$ datapoints $x^{(1)}, x^{(2)}, …, x^{(N)}$. A &lt;em&gt;clustering function&lt;/em&gt; produces a &lt;a href=&quot;https://en.wikipedia.org/wiki/Partition_of_a_set&quot;&gt;&lt;em&gt;partition&lt;/em&gt;&lt;/a&gt; (i.e. a set of clusters), based on the pairwise distances between datapoints. The distance between two points $x^{(i)}$ and $x^{(j)}$ is given by $d(x^{(i)},x^{(j)})$, where $d$ is the &lt;em&gt;distance function&lt;/em&gt;. We could choose different ways to measure distance,&lt;a href=&quot;#f2b&quot; id=&quot;f2t&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt; for simplicity you can imagine we are using &lt;a href=&quot;https://en.wikipedia.org/wiki/Euclidean_distance&quot;&gt;Euclidean distance&lt;/a&gt;, $\sqrt{ (x^{(i)}-x^{(j)}) \cdot (x^{(i)}-x^{(j)})}$.&lt;/p&gt;

&lt;h3 id=&quot;an-axiomatic-approach-to-clustering&quot;&gt;An axiomatic approach to clustering&lt;/h3&gt;

&lt;p&gt;There are many possible clustering functions we could come up with. Some are stupid — randomly split the data into two groups — and others are useful in practice. We would like to precisely define what it means for a clustering function to be “useful in practice.”&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf&quot;&gt;Kleinberg (2002)&lt;/a&gt; proposed that the ideal clustering function would achieve three properties: &lt;a href=&quot;/itsneuronalblog/2015/10/01/clustering2/#scale-invariance&quot;&gt;&lt;em&gt;scale-invariance&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;/itsneuronalblog/2015/10/01/clustering2/#consistency&quot;&gt;&lt;em&gt;consistency&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;/itsneuronalblog/2015/10/01/clustering2/#richness&quot;&gt;&lt;em&gt;richness&lt;/em&gt;&lt;/a&gt;. The idea is that these principles should align with your intuitive notion of what a “good clustering function” is:&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;strong&gt;1. Scale-invariance:&lt;/strong&gt; An ideal clustering function does not change its result when the data are scaled equally in all directions.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Scale-invariance.. &lt;/b&gt;For any scalar $\alpha &gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-scale-invariance.png&quot; alt=&quot;For any scalar $\alpha &amp;gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Scale-invariance.&lt;/b&gt;
	For any scalar $\alpha &amp;gt; 0$ the clustering function $f$ produces same result when the distances, $d$, between all datapoints are multiplied: $f(d) = f(\alpha \cdot d)$.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;2. Consistency:&lt;/strong&gt; If we stretch the data so that the distances between clusters increases and/or the distances within clusters decreases, then the clustering shouldn’t change.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Consistency.. &lt;/b&gt;Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the &lt;i&gt;same&lt;/i&gt; cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to &lt;i&gt;different&lt;/i&gt; clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn&#39;t change: $f(d) = f(d^\prime)$&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-consistency.png&quot; alt=&quot;Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the &amp;lt;i&amp;gt;same&amp;lt;/i&amp;gt; cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to &amp;lt;i&amp;gt;different&amp;lt;/i&amp;gt; clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn&#39;t change: $f(d) = f(d^\prime)$&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Consistency.&lt;/b&gt;
	Let $d$ and $d^\prime$ be two distance functions. The clustering function produces a partition of points for the first distance function, $d$. If, for every pair $(i,j)$ belonging to the &lt;i&gt;same&lt;/i&gt; cluster, $d(i,j) \geq d^\prime(i,j)$, and for every pair belonging to &lt;i&gt;different&lt;/i&gt; clusters, $d(i,j) \leq d^\prime(i,j)$ then the clustering result shouldn&#39;t change: $f(d) = f(d^\prime)$
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;3. Richness:&lt;/strong&gt; Suppose a dataset contains $N$ points, but we are not told anything about the distances between points. An ideal clustering function would be flexible enough to produce all possible partition/clusterings of this set. This means that the it automatically determines both the number and proportions of clusters in the dataset. This is shown schemetically below for a set of six datapoints:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Richness.. &lt;/b&gt;For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-richness.png&quot; alt=&quot;For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Richness.&lt;/b&gt;
	For a clustering function, $f$, richness implies that $\text{Range}(f)$ is equal to all possible partitions of a set of length $N$.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;kleinbergs-impossibility-theorem&quot;&gt;Kleinberg’s &lt;em&gt;Impossibility Theorem&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf&quot;&gt;Kleinberg’s paper&lt;/a&gt; is a bait-and-switch though. &lt;strong&gt;&lt;em&gt;It turns out that no clustering function can satisfy all three axioms!&lt;/em&gt;&lt;/strong&gt; &lt;a href=&quot;#f3b&quot; id=&quot;f3t&quot;&gt;&lt;sup&gt;[3]&lt;/sup&gt;&lt;/a&gt; The proof in Kleinberg’s paper is a little terse — A simpler proof is given in &lt;a href=&quot;http://www.cs.fsu.edu/~ackerman/thesisPhD.pdf&quot;&gt;Margareta Ackerman’s thesis&lt;/a&gt;, specifically Theorem 21. The intuition provided there is diagrammed below.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Intuition behind impossibility.. &lt;/b&gt;A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/impossibility-intuition.png&quot; alt=&quot;A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.&quot; width=&quot;350px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Intuition behind impossibility.&lt;/b&gt;
	A consequence of the richness axiom is that we can define two different distance functions, $d_1$ (top left) and $d_2$ (bottom left), that respectively put all the data points into individual clusters and into some other clustering. Then we can define a third distance function $d_3$ (top and bottom right) that simply scales $d_2$ so that the minimum distance between points in $d_3$ space is larger than the maximum distance in $d_1$ space. Then, we arrive at a contradiction, since by consistency the clustering should be the same for the $d_1 \rightarrow d_3$ transformation, but also the same for the $d_2 \rightarrow d_3$ transformation.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;clustering-functions-that-satisfy-two-of-the-three-axioms&quot;&gt;Clustering functions that satisfy two of the three axioms&lt;/h3&gt;

&lt;p&gt;The above explanation may still be a bit difficult to digest. Another perspective for understanding the impossibility theory is to examine clustering functions that come close to satisfying the three axioms.&lt;/p&gt;

&lt;p&gt;Kleinberg mentions three variants of &lt;a href=&quot;https://en.wikipedia.org/wiki/Single-linkage_clustering&quot;&gt;single-linkage clustering&lt;/a&gt; as an illustration. Single-linkage clustering starts by assigning each point to its own cluster, and then repeatedly fusing together the nearest clusters (where &lt;em&gt;nearest&lt;/em&gt; is measured by our specified distance function). To complete the clustering function we need a &lt;em&gt;stopping condition&lt;/em&gt; — something that tells us when to terminate and return the current set of clusters as our solution. Kleinberg outlines three different stopping conditions, each of which violates one of his three axioms, while satisfying the other two.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. $k$-cluster stopping condition:&lt;/strong&gt; Stop fusing clusters once we have $k$ clusters (where $k$ is some number provided beforehand, similar to the &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means algorithm&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This clearly violates the &lt;em&gt;richness&lt;/em&gt; axiom. For example, if we choose $k=3$, then we could never return a result with 2 clusters, 4 clusters, etc. However, it satisfies &lt;em&gt;scale-invariance&lt;/em&gt; and &lt;em&gt;consistency&lt;/em&gt;. To check this, notice that the transformations in the above diagrams above do not change which $k$ clusters are nearest to each other. It is only once we start merging and dividing clusters that we get into trouble.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;$k$-cluster stopping does not satisfy richness. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/k-stopping-violation.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;$k$-cluster stopping does not satisfy richness&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;2. Distance-$r$ stopping condition:&lt;/strong&gt; Stop when the nearest two clusters are farther than a pre-defined distance $r$.&lt;/p&gt;

&lt;p&gt;This satisfies &lt;em&gt;richness&lt;/em&gt; — we can place $N$ points to end up in $N$ clusters by having the minimum distance between any two points to be greater than $r$, we can place $N$ points to end up in one cluster by having the maximum distance be less than $r$, and we can generate all partitions between these extremes.&lt;/p&gt;

&lt;p&gt;It also satisfies &lt;em&gt;consistency&lt;/em&gt;. Shrinking the distances between points in a cluster keeps the maximum distance less than $r$ (our criterion for defining a cluster in the first place). Expanding the distances between points in different clusters keeps the minimum distance greater than $r$. Thus, the clusters remain the same.&lt;/p&gt;

&lt;p&gt;However, &lt;em&gt;scale-invariance&lt;/em&gt; is violated. If we multiply the data by a large enough number, then the $N$ points will be assigned $N$ different clusters (all points are more than distance $r$ from each other). If we multiply the data by a number close to zero, everything ends up in the same cluster.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;distance-$r$ stopping does not satisfy scale-invariance. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/distance-r-violation.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;distance-$r$ stopping does not satisfy scale-invariance&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;3. Scale-$\epsilon$ stopping condition:&lt;/strong&gt; Stop when the nearest two clusters are farther than a fraction of the maximum distance between two points. This is like the distance-$r$ stopping condition, except we choose $r = \epsilon \cdot \Delta$, where $\Delta$ is the maximum distance between any two data points and $\epsilon$ is a number between 0 and 1.&lt;/p&gt;

&lt;p&gt;By adapting $r$ to the scale of the data, this procedure now satisfies &lt;em&gt;scale-invariance&lt;/em&gt; in addition to &lt;em&gt;richness&lt;/em&gt;. However, it &lt;strong&gt;does not&lt;/strong&gt; satisfy &lt;em&gt;consistency&lt;/em&gt;. To see this, consider the following transformation of data, in which one cluster (the green one) is pulled much further away from the other two clusters. This increases the maximum distance between data points, leading us to merge the blue and red clusters into one:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;scale-$\epsilon$ stopping does not satisfy consistency. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-consistency-violation.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;scale-$\epsilon$ stopping does not satisfy consistency&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;sidestepping-impossibility-and-subsequent-work&quot;&gt;Sidestepping impossibility and subsequent work&lt;/h3&gt;

&lt;p&gt;Kleinberg’s analysis outlines what we &lt;strong&gt;should not expect&lt;/strong&gt; clustering algorithms to do for us. It is good not to have unrealistic expectations. But can we circumvent his impossibility theorem, and are his axioms even really desirable?&lt;/p&gt;

&lt;p&gt;The consistency axiom is particularly suspect as illustrated below:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Is consistency a desirable axiom?. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering-consistency-problem.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Is consistency a desirable axiom?&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The problem is that our intuitive sense of clustering would probably lead us to merge the two clusters in the lower left corner. This criticism is taken up in &lt;a href=&quot;http://www.cs.fsu.edu/~ackerman/thesisPhD.pdf&quot;&gt;Margareta Ackerman’s thesis&lt;/a&gt;, which I hope to summarize in a future blog post.&lt;/p&gt;

&lt;p&gt;Many clustering algorithms also ignore the &lt;em&gt;richness&lt;/em&gt; axiom by specifying the number of clusters beforehand. For example, we can run $k$-means multiple times with different choices of $k$, allowing us to re-interpret the same dataset at different levels of granularity. &lt;a href=&quot;http://stanford.edu/~rezab/papers/slunique.pdf&quot;&gt;Zadeh &amp;amp; Ben-David (2009)&lt;/a&gt; study a relaxation of the richness axiom, which they call $k$-richness — a desirable clustering function should produce all possible $k$-partitions of a datset (rather than &lt;strong&gt;all&lt;/strong&gt; partitions).&lt;/p&gt;

&lt;p&gt;Overall, Kleinberg’s axiomatic approach provides an interesting perspective on clustering, but his analysis serves more as a starting point, rather than a definitive theoretical characterization of clustering.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=Is clustering mathematically impossible?&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;amp;title=Is clustering mathematically impossible?&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&amp;amp;t=Is clustering mathematically impossible?&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;strong&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Follow:
    &lt;/strong&gt;

    &lt;a href=&quot;http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Follow on Feedly&quot; class=&quot;btn-social btn-rss&quot;&gt;
        &lt;i class=&quot;fa fa-rss&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f1t&quot; id=&quot;f1b&quot;&gt;&lt;b&gt;[1]&lt;/b&gt;&lt;/a&gt; I am using loose language when I say clustering is a “hard problem.” Similar to the &lt;a href=&quot;http://localhost:4000/itsneuronalblog/2015/09/11/clustering1/&quot;&gt;previous post&lt;/a&gt;, we will be concerned with why clustering is hard on a conceptual/theoretical level. But it is also worth pointing out that clustering is hard on a computational level — it takes a long time to compute a provably optimal solution. For example, &lt;em&gt;k&lt;/em&gt;-means is provably NP-hard for even k=2 clusters &lt;a href=&quot;https://dx.doi.org/10.1007%2Fs10994-009-5103-0&quot;&gt;(Aloise et al., 2009)&lt;/a&gt;. This is because cluster assignment is a discrete variable (a point &lt;em&gt;either&lt;/em&gt; belongs to a cluster or does not); in many cases, discrete optimization problems are more difficult to solve than continuous problems because we can compute the derivatives of the objective function and thus take advantage of gradient-based methods. (However this &lt;a href=&quot;http://cstheory.stackexchange.com/questions/31054/is-it-a-rule-that-discrete-problems-are-np-hard-and-continuous-problems-are-not&quot;&gt;doesn’t entirely account for&lt;/a&gt; the hardness.)
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f2t&quot; id=&quot;f2b&quot;&gt;&lt;b&gt;[2]&lt;/b&gt;&lt;/a&gt; Kleinberg (2002) only requires that the distance be nonnegative and symmetric, $d(x_i,x_j) = d(x_j,x_i)$, and not necessarily satisfy the &lt;a href=&quot;https://en.wikipedia.org/wiki/Triangle_inequality&quot;&gt;triangle inequality&lt;/a&gt;. According to Wikipedia these are called &lt;a href=&quot;https://en.wikipedia.org/wiki/Metric_(mathematics)#Semimetrics&quot;&gt;&lt;em&gt;semimetrics&lt;/em&gt;&lt;/a&gt;. There are many other exotic distance functions that fit within this space. For example, we can choose other &lt;a href=&quot;https://en.wikipedia.org/wiki/Norm_(mathematics)&quot;&gt;vector norms&lt;/a&gt; $d(x,y) = ||x -y||$ or information theoretic quantities like &lt;a href=&quot;https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&quot;&gt;&lt;em&gt;Jensen-Shannon divergence&lt;/em&gt;&lt;/a&gt;.
&lt;/p&gt;
&lt;p class=&quot;footnotes&quot;&gt;
&lt;a href=&quot;#f3t&quot; id=&quot;f3b&quot;&gt;&lt;b&gt;[3]&lt;/b&gt;&lt;/a&gt; Interesting side note: the title of Kleinberg’s paper — &lt;em&gt;An Impossibility Theorem for Clustering&lt;/em&gt; — is an homage to &lt;a href=&quot;https://en.wikipedia.org/wiki/Arrow%27s_impossibility_theorem&quot;&gt;&lt;em&gt;Kenneth Arrow’s impossibility theorem&lt;/em&gt;&lt;/a&gt;, which roughly states that there is no “fair” voting system in which voters rank three or more choices. As in Kleinberg’s approach, “fairness” is defined by three axioms, which cannot be simultaneously satisfied.
&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>What is clustering and why is it hard?</title>
   <link href="http://alexhwilliams.info/2015/09/11/clustering1/"/>
   <updated>2015-09-11T00:00:00-07:00</updated>
   <id>http://alexhwilliams.info/2015/09/11/clustering1</id>
   <content type="html">&lt;p&gt;I’ve been working on some clustering techniques to &lt;a href=&quot;http://alexhwilliams.info/pubs/DoE_2015_DNA_meth.compressed.pdf&quot;&gt;identify cell types from DNA methylation data&lt;/a&gt;. When you dive into the literature on clustering, two things becomes immediately apparent: first, clustering is fundamental to many scientific questions, and second, there is &lt;a href=&quot;http://stanford.edu/~rezab/papers/slunique.pdf&quot;&gt;“distressingly little general theory”&lt;/a&gt; on how it works or how to apply it to your particular data.&lt;/p&gt;

&lt;p&gt;This was surprising to me. I imagine that most biologists and neuroscientists come across &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;k-means clustering&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;hierarchical clustering&lt;/a&gt;, and similar techniques all the time in papers related to their work. Given how commonplace these techniques are, one would think that we have a solid handle on how they work and what can go wrong.&lt;/p&gt;

&lt;p&gt;This will be the first post in a short series on clustering techniques. I will try to explain why clustering is hard from a high-level, intuitive perspective. The next post will cover some more technical theoretical results. I’ll focus on &lt;a href=&quot;http://web.stanford.edu/~rezab/classes/cme305/W15/Notes/Kleinberg%20-%20impossibility%20theorem.pdf&quot;&gt;Jon Kleinberg’s paper&lt;/a&gt; which precisely defines an ideal clustering function, but then proves that &lt;strong&gt;&lt;em&gt;no such function exists&lt;/em&gt;&lt;/strong&gt; and that there are inevitable tradeoffs that must be made. The final few posts will cover other theoretical work and some current projects of mine.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;what-is-clustering&quot;&gt;What is clustering?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Clustering&quot;&gt;&lt;em&gt;Clustering&lt;/em&gt;&lt;/a&gt; is difficult because it is an &lt;em&gt;unsupervised learning&lt;/em&gt; problem: we are given a dataset and are asked to infer structure within it (in this case, the latent clusters/categories in the data). The problem is that there isn’t necessarily a “correct” or ground truth solution that we can refer to if we want to check our answers. This is in contrast to &lt;a href=&quot;https://en.wikipedia.org/wiki/Statistical_classification&quot;&gt;&lt;em&gt;classification problems&lt;/em&gt;&lt;/a&gt;, where we do know the ground truth. Deep artificial neural networks are very good at classification (&lt;a href=&quot;http://bits.blogs.nytimes.com/2014/08/18/computer-eyesight-gets-a-lot-more-accurate/&quot;&gt;&lt;em&gt;NYT article&lt;/em&gt;&lt;/a&gt;; &lt;a href=&quot;http://www.image-net.org/papers/imagenet_cvpr09.pdf&quot;&gt;Deng et al. 2009&lt;/a&gt;), but clustering is still a very open problem.&lt;/p&gt;

&lt;p&gt;For example, it is a classification problem to predict whether or not a patient has a common disease based on a list of symptoms. In this case, we can draw upon past clinical records to make this judgment, and we can gather further data (e.g. a blood test) to confirm our prediction. In other words, we assume there is a self-evident ground truth (the patient either has or does not have disease X) that can be observed.&lt;/p&gt;

&lt;p&gt;For clustering, we lack this critical information. For example, suppose you are given a large number of beetles and told to group them into clusters based on their appearance. Assuming that you aren’t an entomologist, this will involve some judgment calls and guesswork.&lt;sup&gt;[1]&lt;/sup&gt; If you and a friend sort the same 100 beetles into 5 groups, you will likely come up with slightly different answers. And — here’s the important part — there isn’t really a way to determine which one of you is “right”.&lt;/p&gt;

&lt;h3 id=&quot;approaches-for-clustering&quot;&gt;Approaches for clustering&lt;/h3&gt;

&lt;p&gt;There is a lot of material written on this already, so rather than re-hash what’s out there I will just point you to the best resources.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;K-means clustering&lt;/strong&gt;  (&lt;a href=&quot;https://en.wikipedia.org/wiki/K-means_clustering&quot;&gt;&lt;em&gt;Wikipedia&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;http://www.bytemuse.com/post/k-means-clustering-visualization/&quot;&gt;Visualization by @ChrisPolis&lt;/a&gt;, &lt;a href=&quot;http://tech.nitoyon.com/en/blog/2013/11/07/k-means/&quot;&gt;Visualization by TECH-NI blog&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Hierarchical clustering&lt;/strong&gt; works by starting with each datapoint in its own cluster and fusing the nearest clusters together repeatedly (&lt;a href=&quot;https://en.wikipedia.org/wiki/Hierarchical_clustering&quot;&gt;&lt;em&gt;Wikipedia&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://youtu.be/XJ3194AmH40&quot;&gt;&lt;em&gt;Youtube #1&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://youtu.be/VMyXc3SiEqs&quot;&gt;&lt;em&gt;Youtube #2&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Single-linkage_clustering&quot;&gt;&lt;strong&gt;Single-linkage clustering&lt;/strong&gt;&lt;/a&gt; is a particularly popular and well-characterized form of hierarchical clustering. Briefly, single-linkage begins by initializing each point as its own cluster, and then repeatedly combining the two closest clusters (as measured by their closest points of approach) until the desired number of clusters is achieved.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Bayesian methods&lt;/strong&gt; include &lt;a href=&quot;http://ifas.jku.at/gruen/BayesMix/bayesmix-intro.pdf&quot;&gt;finite mixture models&lt;/a&gt; and &lt;a href=&quot;http://www.kyb.tue.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2299.pdf&quot;&gt;infinite mixture models&lt;/a&gt;.&lt;sup&gt;[2]&lt;/sup&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The important thing to realize is that all of these approaches are &lt;a href=&quot;http://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf&quot;&gt;very computationally difficult&lt;/a&gt; to solve exactly for large datasets (more on this in my next post). As a result, we often resort to optimization heuristics that may or may not produce reasonable results. And, as we will soon see, even if the results are “reasonable” from the algorithm’s perspective, they might not align with our intuition, prior knowledge, or desired outcome.&lt;/p&gt;

&lt;h3 id=&quot;it-is-difficult-to-determine-the-number-of-clusters-in-a-dataset&quot;&gt;It is difficult to determine the number of clusters in a dataset&lt;/h3&gt;

&lt;p&gt;This has to be the most widely understood problem with clustering. In fact, there is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set&quot;&gt;&lt;em&gt;entire Wikipedia article&lt;/em&gt;&lt;/a&gt; devoted to it. If you think about the problem for long enough, you will come to the inescapable conclusion is that there is no “true” number of clusters (though some numbers &lt;strong&gt;&lt;em&gt;feel&lt;/em&gt;&lt;/strong&gt; better than others), and that the same dataset is appropriately viewed at various levels of granularity depending on analysis goals.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering_ambiguity.png&quot; alt=&quot;&quot; width=&quot;400px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;While this problem cannot be “solved” definitively, there are some nice ways of dealing with it. Hierarchical clustering approaches provide cluster assignments for all possible number of clusters, allowing the analyst or reader to view the data across different levels of granularity. There are also Bayesian approaches such as &lt;a href=&quot;http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/&quot;&gt;Dirichlet Process Mixture Models&lt;/a&gt; that adaptively estimate the number of clusters based on a hyperparameter which tunes dispersion. A number of recent papers have focused on &lt;em&gt;convex clustering&lt;/em&gt; techniques that fuse cluster centroids together in a continuous manner along a regularization path; this exposes a hierarchical structure for a clustering approach (roughly) similar to k-means.&lt;sup&gt;[3]&lt;/sup&gt; Of course, there are many other papers out there on this subject.&lt;sup&gt;[4]&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;it-is-difficult-to-cluster-outliers-even-if-they-form-a-common-group&quot;&gt;It is difficult to cluster outliers (even if they form a common group)&lt;/h3&gt;

&lt;p&gt;I recommend you read David Robinson’s excellent post on &lt;a href=&quot;http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means&quot;&gt;the shortcomings of k-means clustering&lt;/a&gt;. The following example he provides is particularly compelling:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Raw Data.. &lt;/b&gt;Three spherical clusters with variable numbers of elements/points.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering_01.png&quot; alt=&quot;Three spherical clusters with variable numbers of elements/points.&quot; width=&quot;400px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Raw Data.&lt;/b&gt;
	Three spherical clusters with variable numbers of elements/points.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The human eye can pretty easily separate these data into three groups, but the k-means algorithm fails pretty hard:&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;. &lt;/b&gt;&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/clustering_02.png&quot; alt=&quot;&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;&lt;/b&gt;
	
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Rather than assigning the points in the upper left corner to their own cluster, the algorithm breaks the largest cluster (in the upper right) into two clusters. In other words it tolerates a few large errors (upper left) in order to decrease the errors where data is particularly dense (upper right). This likely doesn’t align with our analysis, but it is &lt;em&gt;completely reasonable&lt;/em&gt; from the perspective of the algorithm. And again, &lt;strong&gt;&lt;em&gt;there isn’t a ground truth to show that the algorithm is “wrong” per se.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;it-is-difficult-to-cluster-non-spherical-overlapping-data&quot;&gt;It is difficult to cluster non-spherical, overlapping data&lt;/h3&gt;

&lt;p&gt;A final, related problem arises from the shape of the data clusters. Every clustering algorithm makes structural assumptions about the dataset that need to be considered. For example, k-means works by minimizing the total sum-of-squared distance to the cluster centroids. This can produce undesirable results when the clusters are elongated in certain directions — particularly when the between-cluster distance is smaller than the maximum within-cluster distance. Single-linkage clustering, in contrast, can perform well in these cases, since points are clustered together based on their nearest neighbor, which facilitates clustering along ‘paths’ in the dataset.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;Datasets where single-linkage outperforms k-means.. &lt;/b&gt;If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C).&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/kmeans_fail.png&quot; alt=&quot;If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C).&quot; width=&quot;500px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;Datasets where single-linkage outperforms k-means.&lt;/b&gt;
	If your dataset contains long paths, then single-linkage clustering (panels B and D) will typically perform better than k-means (panels A and C).
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;However, there is &lt;a href=&quot;https://en.wikipedia.org/wiki/No_free_lunch_theorem&quot;&gt;no free lunch&lt;/a&gt;.&lt;sup&gt;[5]&lt;/sup&gt; Single-linkage clustering is more sensitive to noise, because each clustering assignment is based on a single pair of datapoints (the pair with minimal distance). This can cause paths to form between overlapping clouds of points. In contrast, k-means uses a more global calculation — minimizing the distance to the nearest centroid summed over all points. As a result, k-means typically does a better job of identifying partially overlapping clusters.&lt;/p&gt;

&lt;!--
Credit goes to http://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll
 --&gt;
&lt;table class=&quot;image&quot;&gt;
&lt;!-- &lt;caption align=&quot;bottom&quot;&gt;&lt;b&gt;A dataset where k-means outperforms single-linkage.. &lt;/b&gt;Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances.&lt;/caption&gt; --&gt;
&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/itsneuronalblog/img/clustering/linkage_fail.png&quot; alt=&quot;Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances.&quot; width=&quot;550px&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;p style=&quot;text-align:center; padding:0 50px; font-size:15px;&quot;&gt;
	&lt;b&gt;A dataset where k-means outperforms single-linkage.&lt;/b&gt;
	Single-linkage clustering tends to erroneously fuse together overlapping groups of points (red dots); small groups of outliers (black dots) are clustered together based on their small pairwise distances.
&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The above figures were schematically reproduced from &lt;a href=&quot;http://www.stat.cmu.edu/~cshalizi/350/lectures/08/lecture-08.pdf&quot;&gt;these lecture notes&lt;/a&gt; from a statistics course at Carnegie Mellon.&lt;/p&gt;

&lt;h3 id=&quot;are-there-solutions-these-problems&quot;&gt;Are there solutions these problems?&lt;/h3&gt;

&lt;p&gt;This post was meant to highlight the inherent difficulty of clustering rather than propose solutions to these issues. It may therefore come off as a bit pessimistic. There are many heuristics that can help overcome the above issues, but I think it is important to emphasize that these are &lt;strong&gt;&lt;em&gt;only heuristics&lt;/em&gt;&lt;/strong&gt;, not guarantees. While many of biologists treat k-means as an “off the shelf” clustering algorithm, we need to be at least a little careful when we do this.&lt;/p&gt;

&lt;p&gt;One of the more interesting heuristics worth reading up on is called &lt;a href=&quot;http://dx.doi.org/10.1109/ICPR.2002.1047450&quot;&gt;ensemble clustering&lt;/a&gt;. The basic idea is to average the outcomes of several clustering techniques or from the same technique fit from different random initializations. Each clustering fit may suffer from instability, but the average behavior of the ensemble of models will tend to be more desirable. This general trick is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_averaging&quot;&gt;&lt;em&gt;ensemble averaging&lt;/em&gt;&lt;/a&gt; and has been successfully applied to a number of machine learning problems.&lt;sup&gt;[6]&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;This post only provides a quick outline of the typical issues that arise for clustering problems. The details of the algorithms have been purposefully omitted, although a deep understanding of these issues likely requires a closer look at these specifics. &lt;a href=&quot;http://www.cse.msu.edu/biometrics/Publications/Clustering/JainClustering_PRL10.pdf&quot;&gt;Jain (2010)&lt;/a&gt; provides a more comprehensive review that is worth reading.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;share-page&quot;&gt;

    &lt;strong&gt;
    Share:
    &lt;/strong&gt;
    
    &lt;a href=&quot;https://twitter.com/intent/tweet?text=What is clustering and why is it hard?&amp;amp;url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;amp;via=ItsNeuronal&amp;amp;related=ItsNeuronal&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Twitter&quot; class=&quot;btn-social btn-twitter&quot;&gt;
        &lt;i class=&quot;fa fa-twitter&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;http://www.reddit.com/submit?url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;amp;title=What is clustering and why is it hard?&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Reddit&quot; class=&quot;btn-social btn-reddit&quot;&gt;
        &lt;i class=&quot;fa fa-reddit&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://facebook.com/sharer.php?u=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&amp;amp;t=What is clustering and why is it hard?&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Facebook&quot; class=&quot;btn-social btn-facebook&quot;&gt;
        &lt;i class=&quot;fa fa-facebook-official&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;a href=&quot;https://plus.google.com/share?url=http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Share on Google+&quot; class=&quot;btn-social btn-google&quot;&gt;
        &lt;i class=&quot;fa fa-google-plus&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

    &lt;strong&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Follow:
    &lt;/strong&gt;

    &lt;a href=&quot;http://cloud.feedly.com/#subscription%2Ffeed%2Fhttp%3A%2F%2Falexhwilliams.info%2Fitsneuronalblog%2Ffeed.xml&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;Follow on Feedly&quot; class=&quot;btn-social btn-rss&quot;&gt;
        &lt;i class=&quot;fa fa-rss&quot;&gt;&lt;/i&gt;
    &lt;/a&gt;

&lt;/div&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;p&gt;&lt;span class=&quot;footnotes&quot;&gt;
&lt;strong&gt;[1]&lt;/strong&gt; It will probably involve guesswork even if you are.&lt;br /&gt;
&lt;strong&gt;[2]&lt;/strong&gt; Highlighting the difficulty of clustering, &lt;a href=&quot;https://normaldeviate.wordpress.com/2012/08/04/mixture-models-the-twilight-zone-of-statistics/&quot;&gt;Larry Wasserman&lt;/a&gt; has joked that “mixtures, like tequila, are inherently evil and should be avoided at all costs.” &lt;a href=&quot;http://andrewgelman.com/2012/08/15/how-to-think-about-mixture-models/&quot;&gt;Andrew Gelman&lt;/a&gt; is slightly less pessimistic.&lt;br /&gt;
&lt;strong&gt;[3]&lt;/strong&gt; Convex clustering performs continuous clustering, similar to how &lt;a href=&quot;http://statweb.stanford.edu/~tibs/lasso/lasso.pdf&quot;&gt;LASSO&lt;/a&gt; performs continuous variable selection.&lt;br /&gt;
&lt;strong&gt;[4]&lt;/strong&gt; See &lt;a href=&quot;http://dx.doi.org/10.1111/1467-9868.00293&quot;&gt;Tibshirani et al. (2001)&lt;/a&gt;, &lt;a href=&quot;http://dx.doi.org/10.1186/gb-2002-3-7-research0036&quot;&gt;Dudoit &amp;amp; Fridlyand (2002)&lt;/a&gt;, &lt;a href=&quot;http://dx.doi.org/10.1109/34.990138&quot;&gt;Figueiredo &amp;amp; Jain (2002)&lt;/a&gt;, &lt;a href=&quot;http://dx.doi.org/10.1111/j.1541-0420.2007.00784.x&quot;&gt;Yan &amp;amp; Ye (2007)&lt;/a&gt;, &lt;a href=&quot;http://www.cs.berkeley.edu/~jordan/papers/kulis-jordan-icml12.pdf&quot;&gt;Kulis &amp;amp; Jordan (2012)&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;[5]&lt;/strong&gt; The &lt;a href=&quot;http://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf&quot;&gt;“no free lunch” theorem&lt;/a&gt; roughly states that whenever an algorithm performs well on a certain class of problems it is because it makes &lt;em&gt;good assumptions&lt;/em&gt; about those problems; however, you can always construct new problems that violate these assumptions, leading to worse performance. Interestingly, this basic idea pops up in other contexts. For example, certain feedback control systems can be engineered so that they are robust to particular perturbations, but such engineering renders them &lt;strong&gt;&lt;em&gt;more sensitive&lt;/em&gt;&lt;/strong&gt; to other forms of perturbations (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Bode%27s_sensitivity_integral&quot;&gt;“waterbed effect.”&lt;/a&gt;)&lt;br /&gt;
&lt;strong&gt;[6]&lt;/strong&gt; See &lt;a href=&quot;https://en.wikipedia.org/wiki/Bootstrap_aggregating&quot;&gt;&lt;em&gt;bootstrap aggregation&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;&gt;&lt;em&gt;random forests&lt;/em&gt;&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot;&gt;&lt;em&gt;ensemble learning&lt;/em&gt;&lt;/a&gt;. Seminal work on this topic was done by &lt;a href=&quot;https://en.wikipedia.org/wiki/Leo_Breiman&quot;&gt;Leo Breiman &lt;/a&gt; — his papers are lucid, fascinating, and accessible, and I particularly recommend his 1996 article &lt;a href=&quot;http://dx.doi.org/10.1007/BF00058655&quot;&gt;“Bagging Predictors”&lt;/a&gt;. This is also covered in any modern textbook, such as &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;.
&lt;/span&gt;&lt;/p&gt;
</content>
 </entry>
 

</feed>
